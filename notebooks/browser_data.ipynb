{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc342201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d08376f1-f503-4c62-8a81-7c3db70f617c\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"d08376f1-f503-4c62-8a81-7c3db70f617c\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d08376f1-f503-4c62-8a81-7c3db70f617c\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 3.5.4\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.134-952ae203dbbe\n",
      "LOGGING: writing to /storage/zoghbi/home/u235147/VarPredBrowser/notebooks/hail-20260110-1722-0.2.134-952ae203dbbe.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"c01852b6-b42a-4894-a9fc-4bc3430e7e8a\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"c01852b6-b42a-4894-a9fc-4bc3430e7e8a\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"c01852b6-b42a-4894-a9fc-4bc3430e7e8a\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hail as hl\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "cpus = 128\n",
    "memory = int(3600*cpus/256)\n",
    "\n",
    "\n",
    "tmpdir = '/local/tmp'\n",
    "\n",
    "config = {\n",
    "    'spark.driver.memory': f'{memory}g',  #Set to total memory\n",
    "    'spark.executor.memory': f'{memory}g',\n",
    "    'spark.local.dir': tmpdir,\n",
    "    'spark.ui.enabled': 'false'\n",
    "}\n",
    "\n",
    "hl.init(spark_conf=config, master=f'local[{cpus}]', tmp_dir=tmpdir, local_tmpdir=tmpdir)\n",
    "\n",
    "hl.plot.output_notebook()\n",
    "%matplotlib inline\n",
    "\n",
    "my_bucket = '/storage/zoghbi/home/u235147/merged_vars'\n",
    "\n",
    "rg38 = hl.get_reference('GRCh38')\n",
    "\n",
    "#!wget -P {my_bucket} https://storage.googleapis.com/hail-common/references/Homo_sapiens_assembly38.fasta.fai\n",
    "#!wget -P {my_bucket} https://storage.googleapis.com/hail-common/references/Homo_sapiens_assembly38.fasta.gz\n",
    "\n",
    "rg38.add_sequence(f'{my_bucket}/Homo_sapiens_assembly38.fasta.gz',\n",
    "                  f'{my_bucket}/Homo_sapiens_assembly38.fasta.fai')\n",
    "\n",
    "#!wget -P {my_bucket} https://storage.googleapis.com/hail-common/references/grch37_to_grch38.over.chain.gz\n",
    "# rg37 = hl.get_reference('GRCh37') \n",
    "# rg37.add_liftover(f'{my_bucket}/grch37_to_grch38.over.chain.gz', rg38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b9bcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/storage/zoghbi/home/u235147/miniforge3/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": "#CLINVAR DATA (Add P/LP missense variants to locus-level data, group by locus, count variants and include clinrevstat metadata as a comma separated list)\nclinvar_file = 'clinvar_20251013.vcf.gz'\n#!wget -P {my_bucket} https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/weekly/{clinvar_file}\nrecode = {str(i): f\"chr{i}\" for i in list(range(1, 23)) + ['X', 'Y']}\nrecode['MT'] = 'chrM'\nclinvar_data = hl.import_vcf(f'{my_bucket}/{clinvar_file}', reference_genome='GRCh38', contig_recoding=recode, skip_invalid_loci=True, force_bgz=True).rows()\n\nclinvar_data = clinvar_data.filter(hl.len(clinvar_data.alleles) > 1)\n\nclinvar_by_locus = clinvar_data.group_by('locus').aggregate(\n    clinvar_variants = hl.agg.collect(hl.struct(\n        ref = clinvar_data.alleles[0],\n        alt = clinvar_data.alleles[1],\n        significance = hl.delimit(clinvar_data.info.CLNSIG, ','),\n        status = hl.delimit(clinvar_data.info.CLNREVSTAT, ','),\n        mol_csq = hl.if_else(\n            hl.is_defined(clinvar_data.info.MC) & (hl.len(clinvar_data.info.MC) > 0),\n            clinvar_data.info.MC[0].split('|')[1],\n            hl.missing(hl.tstr)\n        ),\n        variation_id = clinvar_data.rsid\n    ))\n  )\n\nclinvar_by_locus = clinvar_by_locus.checkpoint(f'{tmpdir}/clinvar_by_locus.ht', overwrite=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f2ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# TRAINING LABELS: (Include as locus level information, with a count for each label (unlabelled, labelled, unlabelled_high_qual, labelled_high_qual))\n",
    "train_data = hl.import_table('/local/Missense_Predictor_copy/_rgc_train_vars.csv', delimiter=',')\n",
    "train_data = train_data.key_by(\n",
    "    locus = hl.locus('chr' + train_data.ID_38.split('-')[0], hl.int(train_data.ID_38.split('-')[1]), reference_genome='GRCh38')\n",
    ")\n",
    "train_data = train_data.group_by('locus').aggregate(\n",
    "    train_counts = hl.struct(\n",
    "        unlabelled = hl.agg.count_where(train_data.label == 'unlabelled'),\n",
    "        labelled = hl.agg.count_where(train_data.label == 'labelled'),\n",
    "        unlabelled_high_qual = hl.agg.count_where((train_data.label == 'unlabelled') & (train_data.High_Qual == 'True')),\n",
    "        labelled_high_qual = hl.agg.count_where((train_data.label == 'labelled') & (train_data.High_Qual == 'True'))\n",
    "    )\n",
    ")\n",
    "train_data = train_data.checkpoint(f'{tmpdir}/train_data.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54ee540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:================================================>  (1121 + 54) / 1175]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbNSFP scores table: 27778551 rows, 10 columns\n"
     ]
    }
   ],
   "source": [
    "# DBNSFP SCORES - Collapse to max value per (locus, transcript)\n",
    "# Only include core dbNSFP scores; RGC/gnomAD constraint metrics come from their tables\n",
    "# Percentiles are calculated in Polars post-processing\n",
    "\n",
    "ht = hl.read_table('/local/Missense_Predictor_copy/Data/dbnsfp/All_missense_with_impute_mane_select_final_with_perc_with_impute_con_perc.ht')\n",
    "\n",
    "# Core dbNSFP score columns (no percentiles - those are calculated in Polars)\n",
    "score_cols = [\n",
    "    'AlphaMissense_am_pathogenicity',\n",
    "    'ESM1b_score',\n",
    "    'RGC_MTR.MTR',\n",
    "    'Non_Neuro_CCR.resid_pctile',\n",
    "    'AlphaSync.plddt',\n",
    "    'AlphaSync.plddt10',\n",
    "    'AlphaSync.relasa',\n",
    "    'AlphaSync.relasa10',\n",
    "]\n",
    "\n",
    "# Create locus column\n",
    "ht = ht.annotate(\n",
    "    locus = hl.locus(ht['locus.contig'], ht['locus.position'], reference_genome='GRCh38')\n",
    ")\n",
    "\n",
    "# Build selection dictionary with sanitized column names (replace . with _)\n",
    "all_cols = list(ht.row)\n",
    "select_dict = {}\n",
    "for col in score_cols:\n",
    "    if col in all_cols:\n",
    "        select_dict[col.replace('.', '_')] = ht[col]\n",
    "\n",
    "ht_selected = ht.select(\n",
    "    'locus',\n",
    "    'alleles', \n",
    "    'Ensembl_transcriptid',\n",
    "    **select_dict\n",
    ")\n",
    "\n",
    "# Group by locus and transcript, take max of each numeric column\n",
    "agg_dict = {f'max_{col}': hl.agg.max(ht_selected[col]) for col in select_dict.keys()}\n",
    "dbnsfp_ht = ht_selected.group_by('locus', 'Ensembl_transcriptid').aggregate(**agg_dict)\n",
    "\n",
    "dbnsfp_ht = dbnsfp_ht.checkpoint(f'{tmpdir}/dbnsfp_scores.ht', overwrite=True)\n",
    "print(f\"dbNSFP scores table: {dbnsfp_ht.count()} rows, {len(list(dbnsfp_ht.row))} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7iyom5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===================================================>(1174 + 1) / 1175]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbNSFP stacked table row count: 27778551\n"
     ]
    }
   ],
   "source": [
    "# DBNSFP STACKED SCORES - Collect variant-level scores with named struct fields\n",
    "# This complements the max aggregation above by preserving individual variant scores\n",
    "# REFACTORED: Using hl.struct() instead of hl.tuple() for named field access\n",
    "\n",
    "# Load the raw dbNSFP data\n",
    "ht_stacked = hl.read_table('/local/Missense_Predictor_copy/Data/dbnsfp/All_missense_with_impute_mane_select_final_with_perc_with_impute_con_perc.ht')\n",
    "\n",
    "# Create locus column\n",
    "ht_stacked = ht_stacked.annotate(\n",
    "    locus = hl.locus(ht_stacked['locus.contig'], ht_stacked['locus.position'], reference_genome='GRCh38')\n",
    ")\n",
    "\n",
    "# Select only the columns needed for stacked display\n",
    "ht_stacked = ht_stacked.select(\n",
    "    'locus',\n",
    "    'alleles',\n",
    "    'Ensembl_transcriptid',\n",
    "    'AlphaMissense_am_pathogenicity',\n",
    "    'AlphaMissense_am_pathogenicity_exome_perc',\n",
    "    'ESM1b_score',\n",
    "    'ESM1b_score_exome_perc'\n",
    ")\n",
    "\n",
    "# Group by (locus, transcript) and collect structs with named fields: {alt, score, percentile}\n",
    "# REFACTORED: Changed from hl.tuple() to hl.struct() for better schema and access patterns\n",
    "dbnsfp_stacked_ht = ht_stacked.group_by('locus', 'Ensembl_transcriptid').aggregate(\n",
    "    dbnsfp_stacked = hl.struct(\n",
    "        AlphaMissense = hl.agg.collect(hl.struct(\n",
    "            alt=ht_stacked.alleles[1],  # Alt allele\n",
    "            score=ht_stacked.AlphaMissense_am_pathogenicity,\n",
    "            percentile=ht_stacked.AlphaMissense_am_pathogenicity_exome_perc\n",
    "        )),\n",
    "        ESM1b = hl.agg.collect(hl.struct(\n",
    "            alt=ht_stacked.alleles[1],  # Alt allele\n",
    "            score=ht_stacked.ESM1b_score,\n",
    "            percentile=ht_stacked.ESM1b_score_exome_perc\n",
    "        ))\n",
    "    )\n",
    ")\n",
    "\n",
    "dbnsfp_stacked_ht = dbnsfp_stacked_ht.checkpoint(f'{tmpdir}/dbnsfp_stacked.ht', overwrite=True)\n",
    "print(f\"dbNSFP stacked table row count: {dbnsfp_stacked_ht.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9d7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MANE Select genes: 19338\n",
      "Genes with domains: 7411\n",
      "Genes without domains: 11927\n",
      "\n",
      "First 10 examples of genes without domains:\n",
      "1. A1BG\n",
      "   Transcript: ENST00000263100.8\n",
      "   RefSeq: NM_130786.4\n",
      "\n",
      "2. A1CF\n",
      "   Transcript: ENST00000373997.8\n",
      "   RefSeq: NM_014576.4\n",
      "\n",
      "3. A2ML1\n",
      "   Transcript: ENST00000299698.12\n",
      "   RefSeq: NM_144670.6\n",
      "\n",
      "4. AAAS\n",
      "   Transcript: ENST00000209873.9\n",
      "   RefSeq: NM_015665.6\n",
      "\n",
      "5. AACS\n",
      "   Transcript: ENST00000316519.11\n",
      "   RefSeq: NM_023928.5\n",
      "\n",
      "6. AADACL2\n",
      "   Transcript: ENST00000356517.4\n",
      "   RefSeq: NM_207365.4\n",
      "\n",
      "7. AADAT\n",
      "   Transcript: ENST00000337664.9\n",
      "   RefSeq: NM_016228.4\n",
      "\n",
      "8. AAGAB\n",
      "   Transcript: ENST00000261880.10\n",
      "   RefSeq: NM_024666.5\n",
      "\n",
      "9. AAK1\n",
      "   Transcript: ENST00000409085.9\n",
      "   RefSeq: NM_014911.5\n",
      "\n",
      "10. AAMDC\n",
      "   Transcript: ENST00000393427.7\n",
      "   RefSeq: NM_024684.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import polars\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load MANE Select data\n",
    "mane_path = Path(f'{my_bucket}/data/raw/MANE.GRCh38.summary.txt.gz')\n",
    "df_mane = pd.read_csv(mane_path, sep='\\t', compression='gzip')\n",
    "df_mane = df_mane[df_mane['MANE_status'] == 'MANE Select'].copy()\n",
    "\n",
    "# Load output with domains\n",
    "output_path = Path(f'{my_bucket}/output/mane_domain_track_v2.parquet')\n",
    "df_output = pd.read_parquet(output_path)\n",
    "\n",
    "# Get all genes in MANE\n",
    "all_mane_genes = set(df_mane['symbol'].dropna().unique())\n",
    "\n",
    "# Get genes with domains\n",
    "genes_with_domains = set(df_output['gene_symbol'].dropna().unique())\n",
    "\n",
    "# Find genes without domains\n",
    "genes_without_domains = all_mane_genes - genes_with_domains\n",
    "\n",
    "print(f\"Total MANE Select genes: {len(all_mane_genes)}\")\n",
    "print(f\"Genes with domains: {len(genes_with_domains)}\")\n",
    "print(f\"Genes without domains: {len(genes_without_domains)}\")\n",
    "print(\"\\nFirst 10 examples of genes without domains:\")\n",
    "for i, gene in enumerate(sorted(genes_without_domains)[:10], 1):\n",
    "    # Get transcript info for this gene\n",
    "    gene_info = df_mane[df_mane['symbol'] == gene].iloc[0]\n",
    "    print(f\"{i}. {gene}\")\n",
    "    print(f\"   Transcript: {gene_info.get('Ensembl_nuc', 'N/A')}\")\n",
    "    print(f\"   RefSeq: {gene_info.get('RefSeq_nuc', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7411 transcript-to-UniProt mappings\n",
      "Loaded representative domains for 16255 proteins\n",
      "Created 20913 domain records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=====================================================>  (91 + 5) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain aggregation table row count: 7217\n"
     ]
    }
   ],
   "source": [
    "# DOMAIN INFORMATION (Load from interpro_representative_domains.json)\n",
    "# Uses API-generated representative domains from fetch_interpro_domains.py\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "# Get Spark session from Hail\n",
    "spark_session = hl.utils.java.Env.spark_session()\n",
    "\n",
    "# Load transcript -> UniProt mapping from existing parquet\n",
    "mapping_df = pl.read_parquet(f'{my_bucket}/output/mane_domain_track_v2.parquet').select([\n",
    "    'transcript_id_ensembl', 'protein_id_uniprot'\n",
    "]).unique()\n",
    "print(f\"Loaded {len(mapping_df)} transcript-to-UniProt mappings\")\n",
    "\n",
    "# Load representative domains from JSON cache\n",
    "with open('/storage/zoghbi/home/u235147/VarPredBrowser/data/cache/interpro_representative_domains.json', 'r') as f:\n",
    "    rep_domains = json.load(f)\n",
    "print(f\"Loaded representative domains for {len(rep_domains)} proteins\")\n",
    "\n",
    "# Build domain records with transcript_id\n",
    "domain_records = []\n",
    "for row in mapping_df.iter_rows(named=True):\n",
    "    transcript_id = row['transcript_id_ensembl'].split('.')[0]  # Strip version\n",
    "    uniprot_id = row['protein_id_uniprot']\n",
    "    \n",
    "    if uniprot_id in rep_domains:\n",
    "        for d in rep_domains[uniprot_id]:\n",
    "            domain_records.append({\n",
    "                'transcript_id': transcript_id,\n",
    "                'domain_id': d.get('interpro_id') or d.get('member_db_id', ''),\n",
    "                'domain_name': d.get('domain_name', ''),\n",
    "                'domain_type': d.get('domain_type', ''),\n",
    "                'source_db': d.get('source_db', ''),\n",
    "                'start_aa': d.get('start_aa', 0),\n",
    "                'end_aa': d.get('end_aa', 0),\n",
    "            })\n",
    "\n",
    "print(f\"Created {len(domain_records)} domain records\")\n",
    "\n",
    "# Convert to Polars DataFrame and then to Spark DataFrame for Hail\n",
    "domain_df = pl.DataFrame(domain_records)\n",
    "spark_df = spark_session.createDataFrame(domain_df.to_pandas())\n",
    "\n",
    "# Convert to Hail table\n",
    "domain_ht = hl.Table.from_spark(spark_df)\n",
    "\n",
    "# Key by transcript_id and aggregate domains into an array\n",
    "domain_ht = domain_ht.key_by('transcript_id')\n",
    "domain_agg = domain_ht.group_by('transcript_id').aggregate(\n",
    "    domains = hl.agg.collect(hl.struct(\n",
    "        domain_id = domain_ht.domain_id,\n",
    "        domain_name = domain_ht.domain_name,\n",
    "        domain_type = domain_ht.domain_type,\n",
    "        source_db = domain_ht.source_db,\n",
    "        start_aa = domain_ht.start_aa,\n",
    "        end_aa = domain_ht.end_aa\n",
    "    ))\n",
    ")\n",
    "\n",
    "domain_agg = domain_agg.checkpoint(f'{tmpdir}/domains.ht', overwrite=True)\n",
    "print(f\"Domain aggregation table row count: {domain_agg.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knphdflxbh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>(198 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "# CONSTRAINT PREDICTIONS (Load predictions and group by locus into structs with named fields)\n",
    "# Source: /local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht\n",
    "# REFACTORED: Using hl.struct() instead of hl.tuple() for named field access\n",
    "\n",
    "preds_ht = hl.read_table('/local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht')\n",
    "\n",
    "# Create locus from ID_38 (format: chr-pos-ref-alt)\n",
    "preds_ht = preds_ht.annotate(\n",
    "    locus = hl.locus('chr' + preds_ht.ID_38.split('-')[0], hl.int(preds_ht.ID_38.split('-')[1]), reference_genome='GRCh38')\n",
    ")\n",
    "\n",
    "# Select the prediction columns we need\n",
    "preds_ht = preds_ht.select(\n",
    "    'locus',\n",
    "    'alleles',\n",
    "    'Ensembl_transcriptid',\n",
    "    'Constraint_200_otu_No_RGC_aapos_AON4_pred',\n",
    "    'Constraint_200_otu_No_RGC_aapos_AON4_n_pred',\n",
    "    'Core_200_otu_No_RGC_aapos_AON4_pred',\n",
    "    'Core_200_otu_No_RGC_aapos_AON4_n_pred',\n",
    "    'Complete_200_otu_No_RGC_aapos_AON4_pred',\n",
    "    'Complete_200_otu_No_RGC_aapos_AON4_n_pred'\n",
    ")\n",
    "\n",
    "preds_ht = preds_ht.annotate(\n",
    "    Const_Core_diff_200_otu_No_RGC_aapos_AON4_pred = preds_ht.Constraint_200_otu_No_RGC_aapos_AON4_pred - preds_ht.Core_200_otu_No_RGC_aapos_AON4_pred\n",
    ")\n",
    "\n",
    "# Group by locus and collect structs with named fields: {alt, pred, n_pred} for each model\n",
    "# REFACTORED: Changed from hl.tuple() to hl.struct() for better schema and access patterns\n",
    "preds_ht = preds_ht.order_by('Ensembl_transcriptid', 'locus', 'alleles')\n",
    "preds_agg = preds_ht.group_by('Ensembl_transcriptid', 'locus').aggregate(\n",
    "    preds = hl.struct(\n",
    "        Constraint = hl.agg.collect(hl.struct(\n",
    "            alt=preds_ht.alleles[1],\n",
    "            pred=preds_ht.Constraint_200_otu_No_RGC_aapos_AON4_pred,\n",
    "            n_pred=preds_ht.Constraint_200_otu_No_RGC_aapos_AON4_n_pred\n",
    "        )),\n",
    "        Core = hl.agg.collect(hl.struct(\n",
    "            alt=preds_ht.alleles[1],\n",
    "            pred=preds_ht.Core_200_otu_No_RGC_aapos_AON4_pred,\n",
    "            n_pred=preds_ht.Core_200_otu_No_RGC_aapos_AON4_n_pred\n",
    "        )),\n",
    "        Complete = hl.agg.collect(hl.struct(\n",
    "            alt=preds_ht.alleles[1],\n",
    "            pred=preds_ht.Complete_200_otu_No_RGC_aapos_AON4_pred,\n",
    "            n_pred=preds_ht.Complete_200_otu_No_RGC_aapos_AON4_n_pred\n",
    "        ))\n",
    "    )\n",
    ")\n",
    "\n",
    "preds_agg = preds_agg.checkpoint(f'{tmpdir}/preds.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "du9ru4eihwo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rgc_scaled.ht for variant consequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>(224 + 2) / 226]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consequence table row count: 33271388\n",
      "\n",
      "Consequence category distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=====================================================>(224 + 2) / 226]\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">csq_category</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">n</div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">str</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">int64</td></tr>\n",
       "</thead><tbody><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;missense&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">72752726</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;plof&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">4119703</td></tr>\n",
       "<tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">&quot;synonymous&quot;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">22941731</td></tr>\n",
       "</tbody></table>"
      ],
      "text/plain": [
       "+--------------+----------+\n",
       "| csq_category |        n |\n",
       "+--------------+----------+\n",
       "| str          |    int64 |\n",
       "+--------------+----------+\n",
       "| \"missense\"   | 72752726 |\n",
       "| \"plof\"       |  4119703 |\n",
       "| \"synonymous\" | 22941731 |\n",
       "+--------------+----------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VARIANT CONSEQUENCES (Extract from rgc_scaled.ht and group by locus)\n",
    "# Source: /storage/zoghbi/home/u235147/merged_vars/rgc_scaled.ht\n",
    "# Field: most_deleterious_consequence_cds\n",
    "# REFACTORED: Using hl.struct() instead of hl.tuple() for named field access\n",
    "\n",
    "# Define consequence categorization function\n",
    "def categorize_consequence(csq):\n",
    "    \"\"\"\n",
    "    Map VEP consequence terms to simplified categories for filtering.\n",
    "    Categories: plof, missense, synonymous, other\n",
    "    \"\"\"\n",
    "    plof = {'frameshift_variant', 'stop_gained', 'splice_donor_variant',\n",
    "            'splice_acceptor_variant', 'start_lost', 'stop_lost'}\n",
    "    missense = {'missense_variant', 'inframe_insertion', 'inframe_deletion',\n",
    "                'protein_altering_variant'}\n",
    "    synonymous = {'synonymous_variant'}\n",
    "    \n",
    "    return (hl.case()\n",
    "        .when(hl.set(plof).contains(csq), 'plof')\n",
    "        .when(hl.set(missense).contains(csq), 'missense')\n",
    "        .when(hl.set(synonymous).contains(csq), 'synonymous')\n",
    "        .default('other'))\n",
    "\n",
    "# Load rgc_scaled.ht which contains consequence annotations\n",
    "print(\"Loading rgc_scaled.ht for variant consequences...\")\n",
    "rgc_scaled = hl.read_table(f'{my_bucket}/rgc_scaled.ht')\n",
    "\n",
    "# Select only the fields we need: locus, alleles, region (for transcript), and consequence\n",
    "# Filter to coding variants (where consequence is defined)\n",
    "csq_ht = rgc_scaled.filter(hl.is_defined(rgc_scaled.most_deleterious_consequence_cds))\n",
    "csq_ht = csq_ht.select(\n",
    "    'region',  # Contains transcript-position info\n",
    "    csq_raw = csq_ht.most_deleterious_consequence_cds\n",
    ")\n",
    "\n",
    "# Add categorized consequence\n",
    "csq_ht = csq_ht.annotate(\n",
    "    csq_category = categorize_consequence(csq_ht.csq_raw)\n",
    ")\n",
    "\n",
    "# Extract transcript ID from region (format: ENST00000123456-100)\n",
    "csq_ht = csq_ht.annotate(\n",
    "    transcript_id = csq_ht.region.split('-')[0]\n",
    ")\n",
    "\n",
    "# Group by locus and transcript, collect structs with named fields: {alt, csq}\n",
    "# REFACTORED: Changed from hl.tuple() to hl.struct() for better schema and access patterns\n",
    "csq_agg = csq_ht.group_by('transcript_id', 'locus').aggregate(\n",
    "    variant_consequences = hl.agg.collect(hl.struct(\n",
    "        alt=csq_ht.alleles[1],  # Alternate allele\n",
    "        csq=csq_ht.csq_category\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Checkpoint for efficiency\n",
    "csq_agg = csq_agg.checkpoint(f'{tmpdir}/tmp/variant_consequences.ht', overwrite=True)\n",
    "print(f\"Consequence table row count: {csq_agg.count()}\")\n",
    "\n",
    "# Show distribution of consequence categories\n",
    "print(\"\\nConsequence category distribution:\")\n",
    "csq_ht.group_by('csq_category').aggregate(n=hl.agg.count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gj9p51i0fen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 96) / 96]\r"
     ]
    }
   ],
   "source": [
    "# VEP AA_POS (Add accurate amino acid position from VEP annotations)\n",
    "# VEP protein_start encodes correct position accounting for strand direction\n",
    "\n",
    "# Load VEP table\n",
    "vep_ht = hl.read_table('/storage/zoghbi/home/u235147/merged_vars/vep.ht')\n",
    "\n",
    "vep_ht = vep_ht.annotate(\n",
    "    transcript_consequences = vep_ht.vep.transcript_consequences,\n",
    "    transcript_id = vep_ht.regions.split('-')[0]\n",
    ")\n",
    "\n",
    "vep_ht = vep_ht.annotate(\n",
    "    transcript_consequences = vep_ht.transcript_consequences.filter(lambda x: x.transcript_id.split('\\\\.')[0] == vep_ht.transcript_id)\n",
    ")\n",
    "\n",
    "\n",
    "# Explode transcript_consequences to get one row per transcript\n",
    "vep_exploded = vep_ht.explode(vep_ht.transcript_consequences)\n",
    "\n",
    "# Key by (locus, transcript_id) for joining wth base table\n",
    "vep_exploded = vep_exploded.key_by(\n",
    "    locus = hl.locus(vep_exploded.locus.contig, vep_exploded.locus.position, reference_genome='GRCh38'),\n",
    "    transcript_id = vep_exploded.transcript_id\n",
    ")\n",
    "\n",
    "\n",
    "# Select relevant fields and key by (locus, alleles, transcript_id)\n",
    "vep_aa_pos = vep_exploded.select(\n",
    "    aa_pos_vep = vep_exploded.transcript_consequences.protein_start\n",
    ")\n",
    "\n",
    "\n",
    "# Checkpoint for performance\n",
    "vep_aa_pos = vep_aa_pos.checkpoint(f'{tmpdir}/vep_aa_pos.ht', overwrite=True)\n",
    "print(f\"VEP aa_pos table row count: {vep_aa_pos.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bjy9l46g0i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading allele frequency tables...\n",
      "Tables loaded and keyed by (locus, alleles)\n",
      "\n",
      "=== RGC Table Schema ===\n",
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'rsid': str \n",
      "    'qual': float64 \n",
      "    'filters': set<str> \n",
      "    'info': struct {\n",
      "        AFR_AC: float64, \n",
      "        AFR_AF: float64, \n",
      "        AFR_AN: float64, \n",
      "        ALL_AC: int32, \n",
      "        ALL_AF: float64, \n",
      "        ALL_AN: int32, \n",
      "        AMI_AC: int32, \n",
      "        AMI_AF: float64, \n",
      "        AMI_AN: int32, \n",
      "        ASH_AC: float64, \n",
      "        ASH_AF: float64, \n",
      "        ASH_AN: float64, \n",
      "        BI_AC: float64, \n",
      "        BI_AF: float64, \n",
      "        BI_AN: float64, \n",
      "        C_EUR_AC: float64, \n",
      "        C_EUR_AF: float64, \n",
      "        C_EUR_AN: float64, \n",
      "        EAS_AC: float64, \n",
      "        EAS_AF: float64, \n",
      "        EAS_AN: float64, \n",
      "        EUR_AC: float64, \n",
      "        EUR_AF: float64, \n",
      "        EUR_AN: float64, \n",
      "        E_AFR_AC: float64, \n",
      "        E_AFR_AF: float64, \n",
      "        E_AFR_AN: float64, \n",
      "        E_ASIA_AC: float64, \n",
      "        E_ASIA_AF: float64, \n",
      "        E_ASIA_AN: float64, \n",
      "        FIN_AC: float64, \n",
      "        FIN_AF: float64, \n",
      "        FIN_AN: float64, \n",
      "        GBR_AC: float64, \n",
      "        GBR_AF: float64, \n",
      "        GBR_AN: float64, \n",
      "        GHA_AC: float64, \n",
      "        GHA_AF: float64, \n",
      "        GHA_AN: float64, \n",
      "        IAM_AC: float64, \n",
      "        IAM_AF: float64, \n",
      "        IAM_AN: float64, \n",
      "        IND_AC: float64, \n",
      "        IND_AF: float64, \n",
      "        IND_AN: float64, \n",
      "        IRE_AC: float64, \n",
      "        IRE_AF: float64, \n",
      "        IRE_AN: float64, \n",
      "        ITA_AC: float64, \n",
      "        ITA_AF: float64, \n",
      "        ITA_AN: float64, \n",
      "        MEA_AC: float64, \n",
      "        MEA_AF: float64, \n",
      "        MEA_AN: float64, \n",
      "        MEX_AC: float64, \n",
      "        MEX_AF: float64, \n",
      "        MEX_AN: float64, \n",
      "        NIN_AC: float64, \n",
      "        NIN_AF: float64, \n",
      "        NIN_AN: float64, \n",
      "        N_EUR_AC: float64, \n",
      "        N_EUR_AF: float64, \n",
      "        N_EUR_AN: float64, \n",
      "        ON_TARGET: bool, \n",
      "        PAK_AC: float64, \n",
      "        PAK_AF: float64, \n",
      "        PAK_AN: float64, \n",
      "        SAS_AC: float64, \n",
      "        SAS_AF: float64, \n",
      "        SAS_AN: float64, \n",
      "        SE_ASIA_AC: float64, \n",
      "        SE_ASIA_AF: float64, \n",
      "        SE_ASIA_AN: float64, \n",
      "        SPA_AC: float64, \n",
      "        SPA_AF: float64, \n",
      "        SPA_AN: float64, \n",
      "        S_AFR_AC: float64, \n",
      "        S_AFR_AF: float64, \n",
      "        S_AFR_AN: float64, \n",
      "        S_EUR_AC: float64, \n",
      "        S_EUR_AF: float64, \n",
      "        S_EUR_AN: float64, \n",
      "        W_AFR_AC: float64, \n",
      "        W_AFR_AF: float64, \n",
      "        W_AFR_AN: float64\n",
      "    } \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n",
      "\n",
      "=== gnomAD Exomes Table Schema ===\n",
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'gnomadV4_exomes': struct {\n",
      "        AC: int32, \n",
      "        AN: int32, \n",
      "        AC_XX: int32, \n",
      "        AN_XX: int32, \n",
      "        AC_XY: int32, \n",
      "        AN_XY: int32, \n",
      "        filters: set<str>, \n",
      "        AC_raw: int32, \n",
      "        lcr: bool, \n",
      "        non_par: bool, \n",
      "        segdup: bool\n",
      "    } \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n",
      "\n",
      "=== gnomAD Genomes Table Schema ===\n",
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh38> \n",
      "    'alleles': array<str> \n",
      "    'gnomadV4_genomes': struct {\n",
      "        AC: int32, \n",
      "        AC_XX: int32, \n",
      "        AC_XY: int32, \n",
      "        AC_raw: int32, \n",
      "        AN: int32, \n",
      "        AN_XX: int32, \n",
      "        AN_XY: int32, \n",
      "        AN_raw: int32, \n",
      "        filters: set<str>\n",
      "    } \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ALLELE FREQUENCIES (Section 3 of browser-data-refactor.md)\n",
    "# Collect AF/AC/AN per variant for each cohort with filter status\n",
    "# Data sources:\n",
    "# - RGC: /storage/zoghbi/data/sharing/hail_tables/no_anno/rgc.ht\n",
    "# - gnomAD Exomes: gnomadV4exomes_snvs_sex.ht\n",
    "# - gnomAD Genomes: gnomadV4genomes_snvs.ht\n",
    "\n",
    "# Load data sources\n",
    "print(\"Loading allele frequency tables...\")\n",
    "rgc_ht = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/no_anno/rgc.ht')\n",
    "gnomad_exomes_ht = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV4_exomes/gnomadV4exomes_snvs_sex.ht')\n",
    "gnomad_genomes_ht = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV4_genomes/gnomadV4genomes_snvs.ht')\n",
    "\n",
    "# Key tables by locus, alleles for joining\n",
    "rgc_ht = rgc_ht.key_by('locus', 'alleles')\n",
    "gnomad_exomes_ht = gnomad_exomes_ht.key_by('locus', 'alleles')\n",
    "gnomad_genomes_ht = gnomad_genomes_ht.key_by('locus', 'alleles')\n",
    "\n",
    "print(\"Tables loaded and keyed by (locus, alleles)\")\n",
    "\n",
    "# Describe tables for reference\n",
    "print(\"\\n=== RGC Table Schema ===\")\n",
    "rgc_ht.describe()\n",
    "print(\"\\n=== gnomAD Exomes Table Schema ===\")\n",
    "gnomad_exomes_ht.describe()\n",
    "print(\"\\n=== gnomAD Genomes Table Schema ===\")\n",
    "gnomad_genomes_ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f3449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base table key fields (auto-included): ['locus', 'region']\n",
      "Keeping 122 columns from base table (+ key fields):\n",
      "  - Core: 5\n",
      "  - RGC oe/vir: 105\n",
      "  - RGC count/obs: 12\n",
      "Found 471 gnomAD constraint columns (oe/vir)\n",
      "Aggregating allele frequencies by locus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================================> (39 + 1) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added VEP aa_pos annotation\n",
      "Added 6 cross-norm percentile columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:======================================================> (39 + 1) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged table row count: 33387608\n"
     ]
    }
   ],
   "source": "#This is the base table to which everything else is merged\n# Load base table - key is ['locus', 'region']\nbase_ht = hl.read_table('/storage/zoghbi/home/u235147/merged_vars/tmp/constraint_metrics_by_locus_rgc_glaf.ht')\n\n# Check key fields - these are automatically included and CANNOT be in select()\nkey_fields = list(base_ht.key)\nprint(f\"Base table key fields (auto-included): {key_fields}\")\n\n# Core row columns - EXCLUDING key fields (locus, region)\n# transcript_id is a row field, not a key field\ncore_cols = ['HGNC', 'chrom', 'pos', 'aa_pos', 'transcript_id']\n\n# RGC oe/vir columns (excluding any key fields)\nrgc_oe_vir_cols = [col for col in base_ht.row if ('oe' in col or 'vir' in col) and col not in key_fields]\n\n# RGC count/obs columns (excluding any key fields) - EXCLUDING max_af columns (deprecated)\nrgc_count_cols = [col for col in base_ht.row\n                  if col.startswith('rgc_')\n                  and ('_count' in col or '_obs_' in col or '_prob_mu' in col)\n                  and '_max_af' not in col  # Exclude deprecated max_af columns\n                  and col not in key_fields]\n\ncols_to_keep = core_cols + rgc_oe_vir_cols + rgc_count_cols\nprint(f\"Keeping {len(cols_to_keep)} columns from base table (+ key fields):\")\nprint(f\"  - Core: {len(core_cols)}\")\nprint(f\"  - RGC oe/vir: {len(rgc_oe_vir_cols)}\")\nprint(f\"  - RGC count/obs: {len(rgc_count_cols)}\")\n\nbase_ht = base_ht.select(*cols_to_keep)\nbase_ht = base_ht.key_by('locus', 'transcript_id')\n\n# Load join target tables\ngnomadV4_exomes_coverage = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV4_exomes_coverage_struct.ht')\ngnomadV4_genomes_coverage = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV3_coverage_struct.ht')\nphylop_ht = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/phyloPscores_hg38_final.ht')\n\n# GNOMAD CONSTRAINT METRICS (Section 6 of browser-data-refactor.md)\n# NOTE: This table has no key, must key by locus before joining\ngnomad_constraint_ht = hl.read_table(f'{my_bucket}/constraint_metrics_by_locus.ht')\ngnomad_constraint_ht = gnomad_constraint_ht.key_by('locus')  # Key by locus for joining\n\n# Dynamically get gnomAD constraint columns containing 'oe' or 'vir'\n# NOTE: Source table already has gnomad_ prefix, so we copy columns as-is\ngnomad_constraint_cols = [col for col in gnomad_constraint_ht.row if 'oe' in col or 'vir' in col]\nprint(f\"Found {len(gnomad_constraint_cols)} gnomAD constraint columns (oe/vir)\")\n\n# OPTIMIZED JOINS: Join once to get struct, then extract fields\nbase_ht = base_ht.annotate(\n    _exome_cov = gnomadV4_exomes_coverage[base_ht.locus],\n    _genome_cov = gnomadV4_genomes_coverage[base_ht.locus],\n    _phylop = phylop_ht[base_ht.locus],\n    _gnomad_constraint = gnomad_constraint_ht[base_ht.locus],\n)\n\n# GNOMAD COVERAGE - All thresholds (Task 2 of browser-data-refactor.md)\n# Extract 12 total coverage columns: 6 thresholds x 2 cohorts\ncoverage_thresholds = [10, 15, 20, 25, 30, 50]\nbase_ht = base_ht.annotate(\n    # Exome coverage (gnomAD v4) - 6 thresholds\n    **{f'gnomad_exomes_over_{t}': base_ht._exome_cov.gnomADV4_coverage[f'over_{t}'] for t in coverage_thresholds},\n    # Genome coverage (gnomAD v3) - 6 thresholds  \n    **{f'gnomad_genomes_over_{t}': hl.float64(base_ht._genome_cov.gnomADV3_coverage[f'over_{t}']) for t in coverage_thresholds},\n    # PhyloP conservation scores (Task 6)\n    phylop_scores_447way = base_ht._phylop.phylop_scores['447way'],\n    phylop_scores_100way = base_ht._phylop.phylop_scores['100way'],\n)\n\n# Dynamically annotate gnomAD constraint columns (oe/vir)\n# Source table already has gnomad_ prefix, so copy columns as-is (no additional prefix)\nbase_ht = base_ht.annotate(**{\n    col: base_ht._gnomad_constraint[col] for col in gnomad_constraint_cols\n})\n\n# Drop temporary join structs\nbase_ht = base_ht.drop('_exome_cov', '_genome_cov', '_phylop', '_gnomad_constraint')\n\n# ALLELE FREQUENCIES (Task 4 of browser-data-refactor.md)\n# Base table is locus-keyed, AF tables are (locus, alleles)-keyed\n# Must aggregate variant-level data by locus into array of structs\nprint(\"Aggregating allele frequencies by locus...\")\n\n# Group RGC by locus and collect variant-level data\nrgc_by_locus = rgc_ht.group_by('locus').aggregate(\n    rgc_variants = hl.agg.collect(hl.struct(\n        alt = rgc_ht.alleles[1],\n        af = rgc_ht.info.ALL_AF,\n        ac = rgc_ht.info.ALL_AC,\n        an = rgc_ht.info.ALL_AN,\n        filters = rgc_ht.filters\n    ))\n)\n\n# Group gnomAD exomes by locus\ngnomad_exomes_by_locus = gnomad_exomes_ht.group_by('locus').aggregate(\n    gnomad_exomes_variants = hl.agg.collect(hl.struct(\n        alt = gnomad_exomes_ht.alleles[1],\n        ac = gnomad_exomes_ht.gnomadV4_exomes.AC,\n        an = gnomad_exomes_ht.gnomadV4_exomes.AN,\n        af = hl.if_else(\n            gnomad_exomes_ht.gnomadV4_exomes.AN > 0,\n            gnomad_exomes_ht.gnomadV4_exomes.AC / gnomad_exomes_ht.gnomadV4_exomes.AN,\n            hl.missing(hl.tfloat64)\n        ),\n        filters = gnomad_exomes_ht.gnomadV4_exomes.filters\n    ))\n)\n\n# Group gnomAD genomes by locus\ngnomad_genomes_by_locus = gnomad_genomes_ht.group_by('locus').aggregate(\n    gnomad_genomes_variants = hl.agg.collect(hl.struct(\n        alt = gnomad_genomes_ht.alleles[1],\n        ac = gnomad_genomes_ht.gnomadV4_genomes.AC,\n        an = gnomad_genomes_ht.gnomadV4_genomes.AN,\n        af = hl.if_else(\n            gnomad_genomes_ht.gnomadV4_genomes.AN > 0,\n            gnomad_genomes_ht.gnomadV4_genomes.AC / gnomad_genomes_ht.gnomadV4_genomes.AN,\n            hl.missing(hl.tfloat64)\n        ),\n        filters = gnomad_genomes_ht.gnomadV4_genomes.filters\n    ))\n)\n\n# Join aggregated AF data to base table\nbase_ht = base_ht.annotate(\n    rgc_variants = rgc_by_locus[base_ht.locus].rgc_variants,\n    gnomad_exomes_variants = gnomad_exomes_by_locus[base_ht.locus].gnomad_exomes_variants,\n    gnomad_genomes_variants = gnomad_genomes_by_locus[base_ht.locus].gnomad_genomes_variants,\n)\n\nbase_ht = base_ht.checkpoint(f'{tmpdir}/base_with_coverage_and_af.ht', overwrite=True)\n\n\n# Load preprocessed tables\nclinvar_ht = hl.read_table(f'{tmpdir}/clinvar_by_locus.ht')\ntrain_ht = hl.read_table(f'{tmpdir}/train_data.ht')\ndbnsfp_ht = hl.read_table(f'{tmpdir}/dbnsfp_scores.ht')\ndbnsfp_stacked_ht = hl.read_table(f'{tmpdir}/dbnsfp_stacked.ht')\ndomain_ht = hl.read_table(f'{tmpdir}/domains.ht')\npreds_ht = hl.read_table(f'{tmpdir}/preds.ht')\n\n# Merge ClinVar variants array (Task 3 - new format, replaces old clinvar struct)\nclinvar_ht = clinvar_ht.key_by('locus')\nmerged = base_ht.annotate(\n    clinvar_variants = clinvar_ht[base_ht.locus].clinvar_variants\n)\n\n# Merge Training Labels (by locus) - FLATTEN to top level\ntrain_ht = train_ht.key_by('locus')\n_train = train_ht[merged.locus]\nmerged = merged.annotate(\n    train_unlabelled = _train.train_counts.unlabelled,\n    train_labelled = _train.train_counts.labelled,\n    train_unlabelled_high_qual = _train.train_counts.unlabelled_high_qual,\n    train_labelled_high_qual = _train.train_counts.labelled_high_qual,\n)\n\n# Merge dbNSFP scores (by locus, transcript) - FLATTEN to top level\n# Get all dbnsfp column names dynamically\ndbnsfp_ht = dbnsfp_ht.key_by('locus', 'Ensembl_transcriptid')\ndbnsfp_cols = [col for col in dbnsfp_ht.row if col not in ['locus', 'Ensembl_transcriptid']]\n_dbnsfp = dbnsfp_ht[merged.locus, merged.transcript_id]\nmerged = merged.annotate(**{\n    col: _dbnsfp[col] for col in dbnsfp_cols\n})\n\n# Merge dbNSFP stacked scores (by locus, transcript) - FLATTEN to top level\ndbnsfp_stacked_ht = dbnsfp_stacked_ht.key_by('locus', 'Ensembl_transcriptid')\nmerged = merged.annotate(\n    AlphaMissense_stacked = dbnsfp_stacked_ht[merged.locus, merged.transcript_id].dbnsfp_stacked.AlphaMissense,\n    ESM1b_stacked = dbnsfp_stacked_ht[merged.locus, merged.transcript_id].dbnsfp_stacked.ESM1b\n)\n\nmerged = merged.checkpoint(f'{tmpdir}/merged_with_dbnsfp.ht', overwrite=True)\n\n# Merge VEP aa_pos FIRST (needed for domain filtering below)\ntry:\n    vep_aa_pos_ht = hl.read_table(f'{tmpdir}/vep_aa_pos.ht')\n    merged = merged.annotate(\n        aa_pos_vep = vep_aa_pos_ht[merged.locus, merged.transcript_id].aa_pos_vep\n    )\n    print(\"Added VEP aa_pos annotation\")\nexcept Exception as e:\n    print(f\"VEP aa_pos not available yet: {e}\")\n\n# Merge Domain information (by transcript) - FILTER BY AMINO ACID POSITION\n# Only include domains where this position's aa_pos_vep falls within [start_aa, end_aa]\ndomain_ht = domain_ht.key_by('transcript_id')\n_all_domains = domain_ht[merged.transcript_id].domains\nmerged = merged.annotate(\n    domains = hl.if_else(\n        hl.is_defined(_all_domains) & hl.is_defined(merged.aa_pos_vep),\n        _all_domains.filter(lambda d: \n            (merged.aa_pos_vep >= d.start_aa) & (merged.aa_pos_vep <= d.end_aa)\n        ),\n        hl.missing(_all_domains.dtype)\n    )\n)\n\n# Merge Constraint Predictions (by locus, transcript) - FLATTEN to top level\npreds_ht = preds_ht.key_by('Ensembl_transcriptid', 'locus')\n_preds = preds_ht[merged.transcript_id, merged.locus].preds\nmerged = merged.annotate(\n    Constraint = _preds.Constraint,\n    Core = _preds.Core,\n    Complete = _preds.Complete,\n)\n\n# Merge Variant Consequences (by locus, transcript)\ncsq_ht = hl.read_table(f'{tmpdir}/variant_consequences.ht')\ncsq_ht = csq_ht.key_by('transcript_id', 'locus')\nmerged = merged.annotate(\n    variant_consequences = csq_ht[merged.transcript_id, merged.locus].variant_consequences\n)\n\n# VEP aa_pos already merged above (before domain filtering)\n\n# Checkpoint the merged table\nmerged = merged.checkpoint(f'/storage/zoghbi/home/u235147/VarPredBrowser/notebooks/merged_browser_data.ht', overwrite=True)\nprint(f\"Merged table row count: {merged.count()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Export merged table to parquet\n",
    "merged_ht = hl.read_table(f'/storage/zoghbi/home/u235147/VarPredBrowser/notebooks/merged_browser_data.ht')\n",
    "\n",
    "\n",
    "output_folder = f'{my_bucket}/rgc_browser_data_merged_tmp'\n",
    "output_file = f'{my_bucket}/rgc_browser_data_merged.parquet'\n",
    "\n",
    "# Repartition to single partition and export to parquet folder\n",
    "merged_repartitioned = merged_ht.repartition(1)\n",
    "spark_df = merged_repartitioned.to_spark()\n",
    "spark_df.write.mode('overwrite').parquet(output_folder)\n",
    "\n",
    "print(f\"Exported to folder: {output_folder}\")\n",
    "\n",
    "# Consolidate Spark output folder to single parquet file\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Find the part file in the output folder\n",
    "part_files = [f for f in os.listdir(output_folder) if f.startswith('part-') and f.endswith('.parquet')]\n",
    "\n",
    "part_file_path = os.path.join(output_folder, part_files[0])\n",
    "# Remove existing output file if it exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "# Move part file to final location\n",
    "shutil.move(part_file_path, output_file)\n",
    "# Clean up the temp folder\n",
    "shutil.rmtree(output_folder)\n",
    "print(f\"Consolidated to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_percentiles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate exome-wide and cross-normalized percentiles using Polars\n",
    "# This runs AFTER parquet export and BEFORE preprocessing\n",
    "# Uses rank(method='average') for proper tie handling\n",
    "\n",
    "input_file = f'{my_bucket}/rgc_browser_data_merged.parquet'\n",
    "output_file = f'{my_bucket}/rgc_browser_data_merged_with_perc.parquet'\n",
    "\n",
    "!python /storage/zoghbi/home/u235147/VarPredBrowser/scripts/calculate_percentiles.py \\\n",
    "    --input {input_file} \\\n",
    "    --output {output_file}\n",
    "\n",
    "# Update the input for preprocessing to use the percentile-augmented file\n",
    "print(f\"\\nPercentile calculation complete. Use {output_file} for preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa731110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VARPRED BROWSER - DATA PREPROCESSING\n",
      "Generating axis tables for all filter modes\n",
      "============================================================\n",
      "\n",
      "Loading data from /storage/zoghbi/home/u235147/merged_vars/rgc_browser_data_merged.parquet...\n",
      "Loaded 33,387,608 total positions\n",
      "  Columns: 287\n",
      "\n",
      "Detected columns:\n",
      "  Chromosome: chrom\n",
      "  Position: pos\n",
      "  Gene: HGNC\n",
      "\n",
      "Chromosome distribution (top 10):\n",
      "  chr1: 3,414,807\n",
      "  chr2: 2,477,732\n",
      "  chr19: 2,149,560\n",
      "  chr11: 1,980,678\n",
      "  chr3: 1,921,014\n",
      "  chr17: 1,918,143\n",
      "  chr12: 1,732,842\n",
      "  chr6: 1,690,167\n",
      "  chr7: 1,565,442\n",
      "  chr5: 1,562,490\n",
      "\n",
      "============================================================\n",
      "Processing filter: all_sites\n",
      "  All positions where any variant is possible\n",
      "============================================================\n",
      "\n",
      "Applying filter...\n",
      "\u2713 Kept 33,387,608 positions\n",
      "Sorting by chromosome and position...\n",
      "Generating compressed coordinates...\n",
      "\n",
      "Column breakdown:\n",
      "  - Core (idx, chrom, pos, gene): 4\n",
      "  - RGC raw metrics: 142\n",
      "  - ClinVar: 4\n",
      "  - Training labels: 4\n",
      "  - dbNSFP scores: 9\n",
      "  - gnomAD coverage: 2\n",
      "  - phyloP: 0\n",
      "  - gnomAD constraint: 0\n",
      "  - Constraint predictions: 3\n",
      "  - Stacked scores: 0\n",
      "  - Variant consequences: 0\n",
      "  - Percentiles: 111\n",
      "  - Domains: 1\n",
      "  Total columns: 282\n",
      "\n",
      "Saving axis table...\n",
      "\u2713 Saved: /storage/zoghbi/home/u235147/VarPredBrowser/data/all_sites.parquet\n",
      "  Size: 16492.74 MB\n",
      "\n",
      "Generating gene index...\n",
      "\u2713 Saved: /storage/zoghbi/home/u235147/VarPredBrowser/data/gene_index_all_sites.parquet\n",
      "  Genes: 19,024\n",
      "\n",
      "Per-chromosome breakdown:\n",
      "  chr1  :  3,414,807 positions\n",
      "  chr2  :  2,477,732 positions\n",
      "  chr3  :  1,921,014 positions\n",
      "  chr4  :  1,352,616 positions\n",
      "  chr5  :  1,562,490 positions\n",
      "  chr6  :  1,690,167 positions\n",
      "  chr7  :  1,565,442 positions\n",
      "  chr8  :  1,143,267 positions\n",
      "  chr9  :  1,359,315 positions\n",
      "  chr10 :  1,295,571 positions\n",
      "  chr11 :  1,980,678 positions\n",
      "  chr12 :  1,732,842 positions\n",
      "  chr13 :    614,844 positions\n",
      "  chr14 :  1,051,530 positions\n",
      "  chr15 :  1,165,698 positions\n",
      "  chr16 :  1,388,352 positions\n",
      "  chr17 :  1,918,143 positions\n",
      "  chr18 :    524,208 positions\n",
      "  chr19 :  2,149,560 positions\n",
      "  chr20 :    787,176 positions\n",
      "  chr21 :    317,811 positions\n",
      "  chr22 :    685,752 positions\n",
      "  chrX  :  1,288,593 positions\n",
      "\n",
      "============================================================\n",
      "Processing filter: missense_only\n",
      "  Positions where missense variants are possible\n",
      "============================================================\n",
      "\n",
      "Applying filter...\n",
      "\u2713 Kept 27,720,524 positions\n",
      "Sorting by chromosome and position...\n",
      "Generating compressed coordinates...\n",
      "\n",
      "Column breakdown:\n",
      "  - Core (idx, chrom, pos, gene): 4\n",
      "  - RGC raw metrics: 142\n",
      "  - ClinVar: 4\n",
      "  - Training labels: 4\n",
      "  - dbNSFP scores: 9\n",
      "  - gnomAD coverage: 2\n",
      "  - phyloP: 0\n",
      "  - gnomAD constraint: 0\n",
      "  - Constraint predictions: 3\n",
      "  - Stacked scores: 0\n",
      "  - Variant consequences: 0\n",
      "  - Percentiles: 111\n",
      "  - Domains: 1\n",
      "  Total columns: 282\n",
      "\n",
      "Saving axis table...\n",
      "\u2713 Saved: /storage/zoghbi/home/u235147/VarPredBrowser/data/missense_only.parquet\n",
      "  Size: 14533.80 MB\n",
      "\n",
      "Generating gene index...\n",
      "\u2713 Saved: /storage/zoghbi/home/u235147/VarPredBrowser/data/gene_index_missense_only.parquet\n",
      "  Genes: 19,022\n",
      "\n",
      "Per-chromosome breakdown:\n",
      "  chr1  :  2,833,702 positions\n",
      "  chr2  :  2,072,012 positions\n",
      "  chr3  :  1,601,531 positions\n",
      "  chr4  :  1,135,738 positions\n",
      "  chr5  :  1,302,820 positions\n",
      "  chr6  :  1,406,285 positions\n",
      "  chr7  :  1,299,379 positions\n",
      "  chr8  :    951,338 positions\n",
      "  chr9  :  1,127,843 positions\n",
      "  chr10 :  1,080,374 positions\n",
      "  chr11 :  1,632,655 positions\n",
      "  chr12 :  1,441,811 positions\n",
      "  chr13 :    516,092 positions\n",
      "  chr14 :    875,286 positions\n",
      "  chr15 :    973,712 positions\n",
      "  chr16 :  1,139,557 positions\n",
      "  chr17 :  1,581,124 positions\n",
      "  chr18 :    439,570 positions\n",
      "  chr19 :  1,761,940 positions\n",
      "  chr20 :    649,164 positions\n",
      "  chr21 :    263,829 positions\n",
      "  chr22 :    562,496 positions\n",
      "  chrX  :  1,072,266 positions\n",
      "\n",
      "============================================================\n",
      "Processing filter: synonymous_only\n",
      "  Positions where synonymous variants are possible\n",
      "============================================================\n",
      "\n",
      "Applying filter...\n",
      "\u2713 Kept 11,577,608 positions\n",
      "Sorting by chromosome and position...\n",
      "Generating compressed coordinates...\n",
      "\n",
      "Column breakdown:\n",
      "  - Core (idx, chrom, pos, gene): 4\n",
      "  - RGC raw metrics: 142\n",
      "  - ClinVar: 4\n",
      "  - Training labels: 4\n",
      "  - dbNSFP scores: 9\n",
      "  - gnomAD coverage: 2\n",
      "  - phyloP: 0\n",
      "  - gnomAD constraint: 0\n",
      "  - Constraint predictions: 3\n",
      "  - Stacked scores: 0\n",
      "  - Variant consequences: 0\n",
      "  - Percentiles: 111\n",
      "  - Domains: 1\n",
      "  Total columns: 282\n",
      "\n",
      "Saving axis table...\n",
      "\u2713 Saved: /storage/zoghbi/home/u235147/VarPredBrowser/data/synonymous_only.parquet\n",
      "  Size: 6305.48 MB\n",
      "\n",
      "Generating gene index...\n",
      "\u2713 Saved: /storage/zoghbi/home/u235147/VarPredBrowser/data/gene_index_synonymous_only.parquet\n",
      "  Genes: 19,022\n",
      "\n",
      "Per-chromosome breakdown:\n",
      "  chr1  :  1,185,099 positions\n",
      "  chr2  :    855,272 positions\n",
      "  chr3  :    663,596 positions\n",
      "  chr4  :    468,423 positions\n",
      "  chr5  :    542,619 positions\n",
      "  chr6  :    585,712 positions\n",
      "  chr7  :    541,956 positions\n",
      "  chr8  :    397,101 positions\n",
      "  chr9  :    471,761 positions\n",
      "  chr10 :    448,179 positions\n",
      "  chr11 :    687,800 positions\n",
      "  chr12 :    598,838 positions\n",
      "  chr13 :    213,118 positions\n",
      "  chr14 :    366,397 positions\n",
      "  chr15 :    404,447 positions\n",
      "  chr16 :    483,780 positions\n",
      "  chr17 :    666,216 positions\n",
      "  chr18 :    181,090 positions\n",
      "  chr19 :    749,901 positions\n",
      "  chr20 :    272,752 positions\n",
      "  chr21 :    110,027 positions\n",
      "  chr22 :    238,394 positions\n",
      "  chrX  :    445,130 positions\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "\n",
      "Generated 3 axis tables:\n",
      "\n",
      "  all_sites:\n",
      "    - Positions: 33,387,608\n",
      "    - Columns: 282\n",
      "    - Genes: 19,024\n",
      "\n",
      "  missense_only:\n",
      "    - Positions: 27,720,524\n",
      "    - Columns: 282\n",
      "    - Genes: 19,022\n",
      "\n",
      "  synonymous_only:\n",
      "    - Positions: 11,577,608\n",
      "    - Columns: 282\n",
      "    - Genes: 19,022\n",
      "\n",
      "Output directory: /storage/zoghbi/home/u235147/VarPredBrowser/data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python /storage/zoghbi/home/u235147/VarPredBrowser/scripts/preprocess_browser_data.py \\\n",
    "      --input /storage/zoghbi/home/u235147/merged_vars/rgc_browser_data_merged_with_perc.parquet \\\n",
    "      --output /storage/zoghbi/home/u235147/VarPredBrowser/data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}