{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc342201",
   "metadata": {},
   "outputs": [],
   "source": "import hail as hl\nfrom scipy.optimize import minimize\nimport numpy as np\n\ncpus = 96\nmemory = int(3600*cpus/256)\n\n\ntmpdir = '/local/tmp'\n\nconfig = {\n    'spark.driver.memory': f'{memory}g',  #Set to total memory\n    'spark.executor.memory': f'{memory}g',\n    'spark.local.dir': tmpdir,\n    'spark.ui.enabled': 'false'\n}\n\nhl.init(spark_conf=config, master=f'local[{cpus}]', tmp_dir=tmpdir, local_tmpdir=tmpdir)\n\nhl.plot.output_notebook()\n%matplotlib inline\n\nmy_bucket = '/storage/zoghbi/home/u235147/merged_vars'\n\nrg38 = hl.get_reference('GRCh38')\n\n#!wget -P {my_bucket} https://storage.googleapis.com/hail-common/references/Homo_sapiens_assembly38.fasta.fai\n#!wget -P {my_bucket} https://storage.googleapis.com/hail-common/references/Homo_sapiens_assembly38.fasta.gz\n\nrg38.add_sequence(f'{my_bucket}/Homo_sapiens_assembly38.fasta.gz',\n                  f'{my_bucket}/Homo_sapiens_assembly38.fasta.fai')\n\n#!wget -P {my_bucket} https://storage.googleapis.com/hail-common/references/grch37_to_grch38.over.chain.gz\n# rg37 = hl.get_reference('GRCh37') \n# rg37.add_liftover(f'{my_bucket}/grch37_to_grch38.over.chain.gz', rg38)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9bcfa",
   "metadata": {},
   "outputs": [],
   "source": "#CLINVAR DATA (Add P/LP missense variants to locus-level data, group by locus, count variants and include clinrevstat metadata as a comma separated list)\nclinvar_file = 'clinvar_20251013.vcf.gz'\n#!wget -P {my_bucket} https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/weekly/{clinvar_file}\nrecode = {str(i): f\"chr{i}\" for i in list(range(1, 23)) + ['X', 'Y']}\nrecode['MT'] = 'chrM'\nclinvar_data = hl.import_vcf(f'{my_bucket}/{clinvar_file}', reference_genome='GRCh38', contig_recoding=recode, skip_invalid_loci=True, force_bgz=True).rows()\nclinvar_data = clinvar_data.key_by('locus', 'alleles')\nclinvar_data = clinvar_data.select(\n    clinvar_status = clinvar_data.info.CLNREVSTAT,\n    clinvar_label = clinvar_data.info.CLNSIG,\n    clinvar_var_type = clinvar_data.info.MC.map(lambda x: x.split('|')[1])\n)\n\n# Group by locus - aggregate to count variants and combine metadata as comma-separated lists\nclinvar_by_locus = clinvar_data.group_by('locus').aggregate(\n    clinvar_count = hl.agg.count(),\n    clinvar_status_list = hl.agg.collect(hl.delimit(clinvar_data.clinvar_status, ',')),\n    clinvar_label_list = hl.agg.collect(hl.delimit(clinvar_data.clinvar_label, ',')),\n    clinvar_var_type_list = hl.agg.collect(hl.delimit(clinvar_data.clinvar_var_type, ','))\n)\n\nclinvar_by_locus = clinvar_by_locus.checkpoint(f'{my_bucket}/tmp/clinvar_by_locus.ht', overwrite=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70f2ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# TRAINING LABELS: (Include as locus level information, with a count for each label (unlabelled, labelled, unlabelled_high_qual, labelled_high_qual))\n",
    "train_data = hl.import_table('/local/Missense_Predictor_copy/_rgc_train_vars.csv', delimiter=',')\n",
    "train_data = train_data.key_by(\n",
    "    locus = hl.locus('chr' + train_data.ID_38.split('-')[0], hl.int(train_data.ID_38.split('-')[1]), reference_genome='GRCh38')\n",
    ")\n",
    "train_data = train_data.group_by('locus').aggregate(\n",
    "    train_counts = hl.struct(\n",
    "        unlabelled = hl.agg.count_where(train_data.label == 'unlabelled'),\n",
    "        labelled = hl.agg.count_where(train_data.label == 'labelled'),\n",
    "        unlabelled_high_qual = hl.agg.count_where((train_data.label == 'unlabelled') & (train_data.High_Qual == 'True')),\n",
    "        labelled_high_qual = hl.agg.count_where((train_data.label == 'labelled') & (train_data.High_Qual == 'True'))\n",
    "    )\n",
    ")\n",
    "train_data = train_data.checkpoint(f'{my_bucket}/tmp/train_data.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54ee540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/storage/zoghbi/home/u235147/miniforge3/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 2:====================================================>(1174 + 1) / 1175]\r"
     ]
    }
   ],
   "source": [
    "#Locus level data (Collapse to max value for that locus and transcript, rename the columns to reflect this)\n",
    "ht = hl.read_table('/local/Missense_Predictor_copy/Data/dbnsfp/All_missense_with_impute_mane_select_final_with_perc_with_impute_con_perc.ht')\n",
    "\n",
    "# Find OE/VIR exome_perc columns\n",
    "all_cols = list(ht.row)\n",
    "oe_vir_cols = [col for col in all_cols if ('_oe_' in col or '_vir_' in col) and '_exome_perc' in col]\n",
    "\n",
    "# Score columns to include\n",
    "score_cols = ['AlphaMissense_am_pathogenicity', 'RGC_MTR.MTR', 'RGC_MTR.MTRpercentile_exome', \n",
    "              'Non_Neuro_CCR.resid_pctile', 'ESM1b_score', 'AlphaSync.plddt', 'AlphaSync.plddt10',\n",
    "              'AlphaSync.relasa', 'AlphaSync.relasa10', 'AlphaMissense_am_pathogenicity_exome_perc', \n",
    "              'ESM1b_score_exome_perc', 'AlphaSync.plddt_exome_perc', 'AlphaSync.plddt10_exome_perc',\n",
    "              'AlphaSync.relasa_exome_perc', 'AlphaSync.relasa10_exome_perc']\n",
    "\n",
    "# Create locus column\n",
    "ht = ht.annotate(\n",
    "    locus = hl.locus(ht['locus.contig'], ht['locus.position'], reference_genome='GRCh38')\n",
    ")\n",
    "\n",
    "# Build selection dictionary with sanitized column names (replace . with _)\n",
    "select_dict = {}\n",
    "for col in score_cols:\n",
    "    if col in all_cols:\n",
    "        select_dict[col.replace('.', '_')] = ht[col]\n",
    "for col in oe_vir_cols:\n",
    "    select_dict[col] = ht[col]\n",
    "\n",
    "ht_selected = ht.select(\n",
    "    'locus',\n",
    "    'alleles', \n",
    "    'Ensembl_transcriptid',\n",
    "    **select_dict\n",
    ")\n",
    "\n",
    "# Group by locus and transcript, take max of each numeric column\n",
    "agg_dict = {}\n",
    "for col in select_dict.keys():\n",
    "    agg_dict[f'max_{col}'] = hl.agg.max(ht_selected[col])\n",
    "\n",
    "dbnsfp_ht = ht_selected.group_by('locus', 'Ensembl_transcriptid').aggregate(**agg_dict)\n",
    "\n",
    "dbnsfp_ht = dbnsfp_ht.checkpoint(f'{my_bucket}/tmp/dbnsfp_scores.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd7iyom5dcf",
   "source": "# DBNSFP STACKED SCORES - Collect (allele, score, percentile) tuples for stacked display\n# This complements the max aggregation above by preserving individual variant scores\n\n# Load the raw dbNSFP data\nht_stacked = hl.read_table('/local/Missense_Predictor_copy/Data/dbnsfp/All_missense_with_impute_mane_select_final_with_perc_with_impute_con_perc.ht')\n\n# Create locus column\nht_stacked = ht_stacked.annotate(\n    locus = hl.locus(ht_stacked['locus.contig'], ht_stacked['locus.position'], reference_genome='GRCh38')\n)\n\n# Select only the columns needed for stacked display\nht_stacked = ht_stacked.select(\n    'locus',\n    'alleles',\n    'Ensembl_transcriptid',\n    'AlphaMissense_am_pathogenicity',\n    'AlphaMissense_am_pathogenicity_exome_perc',\n    'ESM1b_score',\n    'ESM1b_score_exome_perc'\n)\n\n# Group by (locus, transcript) and collect tuples: (allele, score, percentile)\n# Structure mirrors preds_agg for consistency\ndbnsfp_stacked_ht = ht_stacked.group_by('locus', 'Ensembl_transcriptid').aggregate(\n    dbnsfp_stacked = hl.struct(\n        AlphaMissense = hl.agg.collect(hl.tuple([\n            ht_stacked.alleles[1],  # Alt allele\n            ht_stacked.AlphaMissense_am_pathogenicity,\n            ht_stacked.AlphaMissense_am_pathogenicity_exome_perc\n        ])),\n        ESM1b = hl.agg.collect(hl.tuple([\n            ht_stacked.alleles[1],  # Alt allele\n            ht_stacked.ESM1b_score,\n            ht_stacked.ESM1b_score_exome_perc\n        ]))\n    )\n)\n\ndbnsfp_stacked_ht = dbnsfp_stacked_ht.checkpoint(f'{my_bucket}/tmp/dbnsfp_stacked.ht', overwrite=True)\nprint(f\"dbNSFP stacked table row count: {dbnsfp_stacked_ht.count()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467326",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_block = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MANE Select Domain Annotation Track Pipeline - Version 2\n",
    "=========================================================\n",
    "\n",
    "Simplified approach using UniProt REST API to fetch InterPro domains.\n",
    "\n",
    "Output schema:\n",
    "- transcript_id_ensembl: MANE Select transcript (ENST...)\n",
    "- protein_id_uniprot: UniProt accession\n",
    "- domain_id_interpro: InterPro ID\n",
    "- domain_name: Domain name\n",
    "- domain_start_aa: Amino acid start (1-based)\n",
    "- domain_end_aa: Amino acid end (1-based)\n",
    "- source_db: Source database (Pfam, etc.)\n",
    "\n",
    "Author: Bioinformatics Pipeline Assistant\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('mane_domain_pipeline_v2.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Pipeline configuration.\"\"\"\n",
    "    \n",
    "    data_dir: Path = field(default_factory=lambda: Path(\"data\"))\n",
    "    raw_dir: Path = field(default_factory=lambda: Path(\"data/raw\"))\n",
    "    cache_dir: Path = field(default_factory=lambda: Path(\"data/cache\"))\n",
    "    output_dir: Path = field(default_factory=lambda: Path(\"output\"))\n",
    "    \n",
    "    # MANE Select\n",
    "    mane_summary_url: str = \"https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/current/MANE.GRCh38.v1.4.summary.txt.gz\"\n",
    "    \n",
    "    # UniProt API\n",
    "    uniprot_batch_size: int = 100\n",
    "    uniprot_api_delay: float = 0.1  # Seconds between API calls\n",
    "    \n",
    "    # Output\n",
    "    output_filename: str = \"mane_domain_track_v2\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        for d in [self.data_dir, self.raw_dir, self.cache_dir, self.output_dir]:\n",
    "            d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Download Utilities\n",
    "# =============================================================================\n",
    "\n",
    "def download_file(url: str, output_path: Path, description: str = None) -> Path:\n",
    "    \"\"\"Download a file using aria2c or requests.\"\"\"\n",
    "    if output_path.exists():\n",
    "        logger.info(f\"File already exists: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    logger.info(f\"Downloading: {url}\")\n",
    "    \n",
    "    aria2c_path = shutil.which('aria2c')\n",
    "    if aria2c_path:\n",
    "        try:\n",
    "            cmd = [\n",
    "                'aria2c', '--max-connection-per-server=8', '--split=8',\n",
    "                '--min-split-size=1M', '--file-allocation=none',\n",
    "                '--continue=true', '--auto-file-renaming=false',\n",
    "                '-d', str(output_path.parent), '-o', output_path.name, url\n",
    "            ]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                logger.info(f\"Downloaded: {output_path}\")\n",
    "                return output_path\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"aria2c failed: {e}, falling back to requests\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        with tqdm(total=total_size, unit='iB', unit_scale=True, desc=description) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MANE Select Parsing\n",
    "# =============================================================================\n",
    "\n",
    "def parse_mane_summary(summary_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse MANE summary to get transcript-protein mappings.\"\"\"\n",
    "    logger.info(f\"Parsing MANE summary: {summary_path}\")\n",
    "    \n",
    "    df = pd.read_csv(summary_path, sep='\\t', compression='gzip')\n",
    "    \n",
    "    # Filter to MANE Select only\n",
    "    if 'MANE_status' in df.columns:\n",
    "        df = df[df['MANE_status'] == 'MANE Select'].copy()\n",
    "    \n",
    "    logger.info(f\"Found {len(df)} MANE Select entries\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UniProt Mapping\n",
    "# =============================================================================\n",
    "\n",
    "def map_ensembl_to_uniprot(\n",
    "    ensembl_protein_ids: List[str],\n",
    "    id_mapping_path: Path\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Map Ensembl protein IDs to UniProt accessions.\"\"\"\n",
    "    logger.info(\"Mapping Ensembl proteins to UniProt\")\n",
    "    \n",
    "    # Build lookup set (strip versions)\n",
    "    ensp_set = set()\n",
    "    for ensp in ensembl_protein_ids:\n",
    "        if pd.notna(ensp):\n",
    "            ensp_set.add(str(ensp))\n",
    "            ensp_set.add(str(ensp).split('.')[0])\n",
    "    \n",
    "    logger.info(f\"Looking up {len(ensp_set)} Ensembl protein IDs\")\n",
    "    \n",
    "    # Parse ID mapping file\n",
    "    ensp_to_uniprot = {}\n",
    "    reviewed_accs = set()\n",
    "    \n",
    "    with gzip.open(id_mapping_path, 'rt') as f:\n",
    "        for line in tqdm(f, desc=\"Parsing ID mapping\"):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "            \n",
    "            uniprot_acc, id_type, id_val = parts\n",
    "            \n",
    "            if id_type == 'Ensembl_PRO':\n",
    "                id_base = id_val.split('.')[0]\n",
    "                if id_val in ensp_set or id_base in ensp_set:\n",
    "                    # Store both versioned and unversioned\n",
    "                    if id_val not in ensp_to_uniprot:\n",
    "                        ensp_to_uniprot[id_val] = []\n",
    "                    ensp_to_uniprot[id_val].append(uniprot_acc)\n",
    "                    \n",
    "                    if id_base not in ensp_to_uniprot:\n",
    "                        ensp_to_uniprot[id_base] = []\n",
    "                    if uniprot_acc not in ensp_to_uniprot[id_base]:\n",
    "                        ensp_to_uniprot[id_base].append(uniprot_acc)\n",
    "            \n",
    "            elif id_type == 'UniProtKB-ID' and '_HUMAN' in id_val:\n",
    "                reviewed_accs.add(uniprot_acc)\n",
    "    \n",
    "    # Select best UniProt accession for each ENSP\n",
    "    final_mapping = {}\n",
    "    for ensp, accs in ensp_to_uniprot.items():\n",
    "        if len(accs) == 1:\n",
    "            final_mapping[ensp] = accs[0]\n",
    "        else:\n",
    "            # Prefer reviewed\n",
    "            reviewed = [a for a in accs if a in reviewed_accs]\n",
    "            if reviewed:\n",
    "                final_mapping[ensp] = reviewed[0]\n",
    "            else:\n",
    "                final_mapping[ensp] = accs[0]\n",
    "    \n",
    "    logger.info(f\"Mapped {len(final_mapping)} Ensembl proteins to UniProt\")\n",
    "    return final_mapping\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UniProt API Domain Fetching\n",
    "# =============================================================================\n",
    "\n",
    "def is_family_domain(domain: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a domain is Family-level (should be filtered out).\n",
    "    \n",
    "    Filters out:\n",
    "    - Domains with domain_type == 'Family' or 'family' (case-insensitive)\n",
    "    \"\"\"\n",
    "    domain_type = str(domain.get('domain_type', '')).lower()\n",
    "    \n",
    "    # Filter Family-level domains (case-insensitive)\n",
    "    if domain_type == 'family':\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def fetch_uniprot_domains_batch(\n",
    "    uniprot_accessions: List[str],\n",
    "    config: PipelineConfig\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Fetch InterPro domain annotations from UniProt REST API.\n",
    "    \n",
    "    Uses the UniProt API to get domain features for each protein.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Fetching domains for {len(uniprot_accessions)} proteins via UniProt API\")\n",
    "    \n",
    "    domains = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(uniprot_accessions), config.uniprot_batch_size), \n",
    "                  desc=\"Fetching from UniProt\"):\n",
    "        batch = uniprot_accessions[i:i + config.uniprot_batch_size]\n",
    "        \n",
    "        # Build query for batch\n",
    "        acc_query = ' OR '.join([f'accession:{acc}' for acc in batch])\n",
    "        \n",
    "        url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "        params = {\n",
    "            'query': acc_query,\n",
    "            'format': 'json',\n",
    "            'fields': 'accession,xref_interpro',\n",
    "            'size': len(batch)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for entry in data.get('results', []):\n",
    "                acc = entry.get('primaryAccession', '')\n",
    "                \n",
    "                # Extract InterPro cross-references\n",
    "                xrefs = entry.get('uniProtKBCrossReferences', [])\n",
    "                interpro_refs = [x for x in xrefs if x.get('database') == 'InterPro']\n",
    "                \n",
    "                if interpro_refs:\n",
    "                    domains[acc] = []\n",
    "                    for ref in interpro_refs:\n",
    "                        ipr_id = ref.get('id', '')\n",
    "                        props = {p.get('key'): p.get('value') for p in ref.get('properties', [])}\n",
    "                        \n",
    "                        domains[acc].append({\n",
    "                            'interpro_id': ipr_id,\n",
    "                            'domain_name': props.get('EntryName', ''),\n",
    "                        })\n",
    "            \n",
    "            time.sleep(config.uniprot_api_delay)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"API error for batch starting at {i}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Found domains for {len(domains)} proteins\")\n",
    "    return domains\n",
    "\n",
    "\n",
    "def fetch_interpro_domains_direct(\n",
    "    uniprot_accessions: List[str],\n",
    "    config: PipelineConfig,\n",
    "    cache_path: Optional[Path] = None\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Fetch InterPro domains using InterPro API directly.\n",
    "    \n",
    "    This gives us the actual domain coordinates.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Fetching domains for {len(uniprot_accessions)} proteins via InterPro API\")\n",
    "    \n",
    "    # Check cache\n",
    "    if cache_path and cache_path.exists():\n",
    "        logger.info(f\"Loading cached domains from {cache_path}\")\n",
    "        with open(cache_path, 'r') as f:\n",
    "            cached = json.load(f)\n",
    "        # Drop Family-level domains (Family type and PANTHER)\n",
    "        cached = {\n",
    "            acc: [d for d in feats if not is_family_domain(d)]\n",
    "            for acc, feats in cached.items()\n",
    "            if any(not is_family_domain(d) for d in feats)\n",
    "        }\n",
    "        logger.info(f\"Filtered out Family-level domains from cache\")\n",
    "        return cached\n",
    "    \n",
    "    domains = {}\n",
    "    \n",
    "    for acc in tqdm(uniprot_accessions, desc=\"Fetching from InterPro\"):\n",
    "        # Use InterPro API to get protein matches\n",
    "        url = f\"https://www.ebi.ac.uk/interpro/api/entry/interpro/protein/uniprot/{acc}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers={'Accept': 'application/json'})\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                domains[acc] = []\n",
    "                for result in data.get('results', []):\n",
    "                    metadata = result.get('metadata', {})\n",
    "                    ipr_id = metadata.get('accession', '')\n",
    "                    ipr_name = metadata.get('name', '')\n",
    "                    ipr_type = metadata.get('type', '')\n",
    "                    source_db = metadata.get('source_database', '')\n",
    "                    \n",
    "                    # Get protein locations\n",
    "                    proteins = result.get('proteins', [])\n",
    "                    for protein in proteins:\n",
    "                        for location in protein.get('entry_protein_locations', []):\n",
    "                            for fragment in location.get('fragments', []):\n",
    "                                domains[acc].append({\n",
    "                                    'interpro_id': ipr_id,\n",
    "                                    'domain_name': ipr_name,\n",
    "                                    'domain_type': ipr_type,\n",
    "                                    'source_db': source_db,\n",
    "                                    'start_aa': fragment.get('start', 0),\n",
    "                                    'end_aa': fragment.get('end', 0)\n",
    "                                })\n",
    "            \n",
    "            time.sleep(config.uniprot_api_delay)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.debug(f\"Error fetching {acc}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Found domains for {len(domains)} proteins\")\n",
    "    \n",
    "    # Filter out Family-level domains (Family type and PANTHER)\n",
    "    filtered_domains = {\n",
    "        acc: [d for d in feats if not is_family_domain(d)]\n",
    "        for acc, feats in domains.items()\n",
    "        if any(not is_family_domain(d) for d in feats)\n",
    "    }\n",
    "    \n",
    "    # Cache results\n",
    "    if cache_path:\n",
    "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump(filtered_domains, f)\n",
    "        logger.info(f\"Cached filtered domains to {cache_path} (Family-level domains excluded)\")\n",
    "    \n",
    "    logger.info(f\"Filtered out Family-level domains (Family type and PANTHER)\")\n",
    "    return filtered_domains\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(config: PipelineConfig):\n",
    "    \"\"\"Run the pipeline.\"\"\"\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"MANE Select Domain Annotation Track Pipeline v2\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Download MANE summary\n",
    "    logger.info(\"\\n[Step 1] Downloading MANE Select data...\")\n",
    "    mane_summary_path = download_file(\n",
    "        config.mane_summary_url,\n",
    "        config.raw_dir / \"MANE.GRCh38.summary.txt.gz\",\n",
    "        \"MANE Summary\"\n",
    "    )\n",
    "    \n",
    "    # Step 2: Parse MANE summary\n",
    "    logger.info(\"\\n[Step 2] Parsing MANE Select...\")\n",
    "    mane_df = parse_mane_summary(mane_summary_path)\n",
    "    \n",
    "    # Step 3: Map to UniProt\n",
    "    logger.info(\"\\n[Step 3] Mapping to UniProt...\")\n",
    "    \n",
    "    # Download ID mapping if needed\n",
    "    id_mapping_url = \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/by_organism/HUMAN_9606_idmapping.dat.gz\"\n",
    "    id_mapping_path = download_file(\n",
    "        id_mapping_url,\n",
    "        config.raw_dir / \"HUMAN_9606_idmapping.dat.gz\",\n",
    "        \"UniProt ID Mapping\"\n",
    "    )\n",
    "    \n",
    "    # Get Ensembl protein IDs from MANE\n",
    "    ensembl_proteins = mane_df['Ensembl_prot'].dropna().tolist()\n",
    "    \n",
    "    # Map to UniProt\n",
    "    ensp_to_uniprot = map_ensembl_to_uniprot(ensembl_proteins, id_mapping_path)\n",
    "    \n",
    "    # Add UniProt accession to MANE dataframe\n",
    "    mane_df['UniProt_acc'] = mane_df['Ensembl_prot'].apply(\n",
    "        lambda x: ensp_to_uniprot.get(str(x), ensp_to_uniprot.get(str(x).split('.')[0], '')) \n",
    "        if pd.notna(x) else ''\n",
    "    )\n",
    "    \n",
    "    # Get unique UniProt accessions\n",
    "    uniprot_accessions = mane_df['UniProt_acc'].dropna()\n",
    "    uniprot_accessions = [a for a in uniprot_accessions if a != '']\n",
    "    uniprot_accessions = list(set(uniprot_accessions))\n",
    "    \n",
    "    logger.info(f\"Found {len(uniprot_accessions)} unique UniProt accessions\")\n",
    "    \n",
    "    # Step 4: Fetch InterPro domains\n",
    "    logger.info(\"\\n[Step 4] Fetching InterPro domain annotations...\")\n",
    "    \n",
    "    cache_path = config.cache_dir / \"interpro_domains_cache.json\"\n",
    "    interpro_domains = fetch_interpro_domains_direct(\n",
    "        uniprot_accessions, config, cache_path\n",
    "    )\n",
    "    \n",
    "    # Step 5: Build output table\n",
    "    logger.info(\"\\n[Step 5] Building output table...\")\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for _, row in tqdm(mane_df.iterrows(), total=len(mane_df), desc=\"Building records\"):\n",
    "        enst = row.get('Ensembl_nuc', '')\n",
    "        ensp = row.get('Ensembl_prot', '')\n",
    "        uniprot = row.get('UniProt_acc', '')\n",
    "        gene_symbol = row.get('symbol', '')\n",
    "        \n",
    "        if not uniprot or uniprot not in interpro_domains:\n",
    "            continue\n",
    "        \n",
    "        for domain in interpro_domains[uniprot]:\n",
    "            records.append({\n",
    "                'transcript_id_ensembl': enst,\n",
    "                'protein_id_ensembl': ensp,\n",
    "                'protein_id_uniprot': uniprot,\n",
    "                'gene_symbol': gene_symbol,\n",
    "                'domain_id_interpro': domain.get('interpro_id', ''),\n",
    "                'domain_name': domain.get('domain_name', ''),\n",
    "                'domain_type': domain.get('domain_type', ''),\n",
    "                'source_db': domain.get('source_db', ''),\n",
    "                'domain_start_aa': domain.get('start_aa', 0),\n",
    "                'domain_end_aa': domain.get('end_aa', 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    logger.info(f\"Created {len(df)} domain annotation records\")\n",
    "    \n",
    "    # Step 6: Write output\n",
    "    logger.info(\"\\n[Step 6] Writing Parquet output...\")\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        # Sort by transcript\n",
    "        df = df.sort_values(['transcript_id_ensembl', 'domain_start_aa'])\n",
    "        \n",
    "        # Write Parquet\n",
    "        output_path = config.output_dir / f\"{config.output_filename}.parquet\"\n",
    "        df.to_parquet(output_path, index=False, compression='snappy')\n",
    "        logger.info(f\"Wrote {len(df)} records to: {output_path}\")\n",
    "        \n",
    "        # Also write TSV for easy inspection\n",
    "        tsv_path = config.output_dir / f\"{config.output_filename}.tsv\"\n",
    "        df.to_csv(tsv_path, sep='\\t', index=False)\n",
    "        logger.info(f\"Wrote TSV to: {tsv_path}\")\n",
    "    else:\n",
    "        logger.warning(\"No domain records to write!\")\n",
    "    \n",
    "    # Summary\n",
    "    logger.info(\"\\n\" + \"=\" * 70)\n",
    "    logger.info(\"Pipeline Complete - Summary\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"MANE Select transcripts: {len(mane_df)}\")\n",
    "    logger.info(f\"Transcripts with UniProt: {len(mane_df[mane_df['UniProt_acc'] != ''])}\")\n",
    "    logger.info(f\"UniProt proteins with domains: {len(interpro_domains)}\")\n",
    "    logger.info(f\"Total domain annotations: {len(df)}\")\n",
    "    if len(df) > 0:\n",
    "        logger.info(f\"Unique InterPro entries: {df['domain_id_interpro'].nunique()}\")\n",
    "        logger.info(f\"Genes with domains: {df['gene_symbol'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"MANE Domain Track Pipeline v2\")\n",
    "    parser.add_argument('--data-dir', type=Path, default=Path('data'))\n",
    "    parser.add_argument('--output-dir', type=Path, default=Path('output'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config = PipelineConfig(\n",
    "        data_dir=args.data_dir,\n",
    "        raw_dir=args.data_dir / 'raw',\n",
    "        cache_dir=args.data_dir / 'cache',\n",
    "        output_dir=args.output_dir\n",
    "    )\n",
    "    \n",
    "    run_pipeline(config)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"mane_domain_pipeline_v2.py\", \"w\") as f:\n",
    "    f.write(code_block)\n",
    "\n",
    "!python mane_domain_pipeline_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafdd21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>domain_type</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;Coiled_coil&quot;</td><td>85</td></tr><tr><td>&quot;Disordered&quot;</td><td>30</td></tr><tr><td>&quot;Homologous_superfamily&quot;</td><td>12747</td></tr><tr><td>&quot;Domain&quot;</td><td>13902</td></tr><tr><td>&quot;Conserved_site&quot;</td><td>1849</td></tr><tr><td>&quot;Repeat&quot;</td><td>901</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 2)\n",
       "┌────────────────────────┬───────┐\n",
       "│ domain_type            ┆ count │\n",
       "│ ---                    ┆ ---   │\n",
       "│ str                    ┆ u32   │\n",
       "╞════════════════════════╪═══════╡\n",
       "│ Coiled_coil            ┆ 85    │\n",
       "│ Disordered             ┆ 30    │\n",
       "│ Homologous_superfamily ┆ 12747 │\n",
       "│ Domain                 ┆ 13902 │\n",
       "│ Conserved_site         ┆ 1849  │\n",
       "│ Repeat                 ┆ 901   │\n",
       "└────────────────────────┴───────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_parquet('/storage/zoghbi/home/u235147/merged_vars/output/mane_domain_track_v2.parquet')\n",
    "df['domain_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d7703",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom pathlib import Path\n\n# Load MANE Select data\nmane_path = Path(f'{my_bucket}/data/raw/MANE.GRCh38.summary.txt.gz')\ndf_mane = pd.read_csv(mane_path, sep='\\t', compression='gzip')\ndf_mane = df_mane[df_mane['MANE_status'] == 'MANE Select'].copy()\n\n# Load output with domains\noutput_path = Path(f'{my_bucket}/output/mane_domain_track_v2.parquet')\ndf_output = pd.read_parquet(output_path)\n\n# Get all genes in MANE\nall_mane_genes = set(df_mane['symbol'].dropna().unique())\n\n# Get genes with domains\ngenes_with_domains = set(df_output['gene_symbol'].dropna().unique())\n\n# Find genes without domains\ngenes_without_domains = all_mane_genes - genes_with_domains\n\nprint(f\"Total MANE Select genes: {len(all_mane_genes)}\")\nprint(f\"Genes with domains: {len(genes_with_domains)}\")\nprint(f\"Genes without domains: {len(genes_without_domains)}\")\nprint(\"\\nFirst 10 examples of genes without domains:\")\nfor i, gene in enumerate(sorted(genes_without_domains)[:10], 1):\n    # Get transcript info for this gene\n    gene_info = df_mane[df_mane['symbol'] == gene].iloc[0]\n    print(f\"{i}. {gene}\")\n    print(f\"   Transcript: {gene_info.get('Ensembl_nuc', 'N/A')}\")\n    print(f\"   RefSeq: {gene_info.get('RefSeq_nuc', 'N/A')}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82c8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "#Domain information (Load from pre-generated parquet and prepare for merging by transcript)\n",
    "# The domain parquet was generated by the MANE Domain Track Pipeline\n",
    "spark_session = hl.utils.java.Env.spark_session()\n",
    "\n",
    "# Read domain parquet\n",
    "domain_df = spark_session.read.parquet(f'{my_bucket}/output/mane_domain_track_v2.parquet')\n",
    "\n",
    "# Convert to Hail table\n",
    "domain_ht = hl.Table.from_spark(domain_df)\n",
    "\n",
    "# Strip version from transcript_id for joining\n",
    "domain_ht = domain_ht.annotate(\n",
    "    transcript_id = domain_ht.transcript_id_ensembl.split('\\\\.')[0]\n",
    ")\n",
    "\n",
    "# Key by transcript_id and aggregate domains into an array (multiple domains per transcript)\n",
    "domain_ht = domain_ht.key_by('transcript_id')\n",
    "domain_agg = domain_ht.group_by('transcript_id').aggregate(\n",
    "    domains = hl.agg.collect(hl.struct(\n",
    "        domain_id = domain_ht.domain_id_interpro,\n",
    "        domain_name = domain_ht.domain_name,\n",
    "        domain_type = domain_ht.domain_type,\n",
    "        source_db = domain_ht.source_db,\n",
    "        start_aa = domain_ht.domain_start_aa,\n",
    "        end_aa = domain_ht.domain_end_aa\n",
    "    ))\n",
    ")\n",
    "\n",
    "domain_agg = domain_agg.checkpoint(f'{my_bucket}/tmp/domains.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d025bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'ID_38': str \n",
      "    'genename': str \n",
      "    'alleles': array<str> \n",
      "    'Ensembl_transcriptid': str \n",
      "    'aapos': int32 \n",
      "    'aaref': str \n",
      "    'Uniprot_acc': str \n",
      "    'aaalt': str \n",
      "    'plddt_lt_60': bool \n",
      "    'Constraint_1000_General_pred': float32 \n",
      "    'Constraint_1000_General_pred_std': float32 \n",
      "    'Constraint_1000_General_label': float32 \n",
      "    'Constraint_1000_General_n_pred': int32 \n",
      "    'Core_1000_General_pred': float32 \n",
      "    'Core_1000_General_pred_std': float32 \n",
      "    'Core_1000_General_label': float32 \n",
      "    'Core_1000_General_n_pred': int32 \n",
      "    'Complete_1000_General_pred': float32 \n",
      "    'Complete_1000_General_pred_std': float32 \n",
      "    'Complete_1000_General_label': float32 \n",
      "    'Complete_1000_General_n_pred': int32 \n",
      "    'Constraint_200_rgc_zero_General_pred': float32 \n",
      "    'Constraint_200_rgc_zero_General_pred_std': float32 \n",
      "    'Constraint_200_rgc_zero_General_label': float32 \n",
      "    'Constraint_200_rgc_zero_General_n_pred': int32 \n",
      "    'Core_200_rgc_zero_General_pred': float32 \n",
      "    'Core_200_rgc_zero_General_pred_std': float32 \n",
      "    'Core_200_rgc_zero_General_label': float32 \n",
      "    'Core_200_rgc_zero_General_n_pred': int32 \n",
      "    'Complete_200_rgc_zero_General_pred': float32 \n",
      "    'Complete_200_rgc_zero_General_pred_std': float32 \n",
      "    'Complete_200_rgc_zero_General_label': float32 \n",
      "    'Complete_200_rgc_zero_General_n_pred': int32 \n",
      "    'Complete_200_obs_weighted_General_pred': float32 \n",
      "    'Complete_200_obs_weighted_General_pred_std': float32 \n",
      "    'Complete_200_obs_weighted_General_label': float32 \n",
      "    'Complete_200_obs_weighted_General_n_pred': int32 \n",
      "    'Constraint_200_obs_weighted_General_pred': float32 \n",
      "    'Constraint_200_obs_weighted_General_pred_std': float32 \n",
      "    'Constraint_200_obs_weighted_General_label': float32 \n",
      "    'Constraint_200_obs_weighted_General_n_pred': int32 \n",
      "    'Complete_200_site_weighted_General_pred': float32 \n",
      "    'Complete_200_site_weighted_General_pred_std': float32 \n",
      "    'Complete_200_site_weighted_General_label': float32 \n",
      "    'Complete_200_site_weighted_General_n_pred': int32 \n",
      "    'Constraint_200_site_weighted_General_pred': float32 \n",
      "    'Constraint_200_site_weighted_General_pred_std': float32 \n",
      "    'Constraint_200_site_weighted_General_label': float32 \n",
      "    'Constraint_200_site_weighted_General_n_pred': int32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_pred': float32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_pred_std': float32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_label': float32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_n_pred': int32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_pred': float32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_pred_std': float32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_label': float32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_n_pred': int32 \n",
      "    'locus': locus<GRCh38> \n",
      "    'REVEL_score': str \n",
      "    'REVEL_rankscore': str \n",
      "    'MPC_score': str \n",
      "    'MPC_rankscore': str \n",
      "    'ESM1b_converted_rankscore': str \n",
      "    'AllofUs_ALL_AF': str \n",
      "    'RegeneronME_ALL_AF': str \n",
      "    'gnomAD4.1_joint_AF': str \n",
      "    'ESM1b_score': float64 \n",
      "    'iGEMME_score': float64 \n",
      "    'CPT.CPT1_score': float64 \n",
      "    'AlphaMissense_am_pathogenicity': float64 \n",
      "    'PopEve_Score': float64 \n",
      "    'PopEve_pop_adjusted_Score': float64 \n",
      "    'PopEve_pop_adjusted_ESM1v_Score': float64 \n",
      "----------------------------------------\n",
      "Key: []\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preds_ht = hl.read_table('/local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht')\n",
    "preds_ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "knphdflxbh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred====> (94 + 2) / 96]\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/storage/zoghbi/home/u235147/miniforge3/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 4:========================================================>(95 + 1) / 96]\r"
     ]
    }
   ],
   "source": [
    "# CONSTRAINT PREDICTIONS (Load predictions and group by locus into tuples of (pred, n_pred))\n",
    "# Source: /local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht\n",
    "preds_ht = hl.read_table('/local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht')\n",
    "\n",
    "# Create locus from ID_38 (format: chr-pos-ref-alt)\n",
    "preds_ht = preds_ht.annotate(\n",
    "    locus = hl.locus('chr' + preds_ht.ID_38.split('-')[0], hl.int(preds_ht.ID_38.split('-')[1]), reference_genome='GRCh38')\n",
    ")\n",
    "\n",
    "# Select the prediction columns we need\n",
    "preds_ht = preds_ht.select(\n",
    "    'locus',\n",
    "    'alleles',\n",
    "    'Ensembl_transcriptid',\n",
    "    'Constraint_1000_General_pred',\n",
    "    'Constraint_1000_General_n_pred',\n",
    "    'Core_1000_General_pred',\n",
    "    'Core_1000_General_n_pred',\n",
    "    'Complete_1000_General_pred',\n",
    "    'Complete_1000_General_n_pred'\n",
    ")\n",
    "\n",
    "preds_ht = preds_ht.annotate(\n",
    "    Const_Core_diff_1000_General_pred = preds_ht.Constraint_1000_General_pred - preds_ht.Core_1000_General_pred\n",
    ")\n",
    "\n",
    "# Group by locus and collect tuples of (pred, n_pred) for each model\n",
    "# Format: ((Constraint_pred, Constraint_n), (Core_pred, Core_n), (Complete_pred, Complete_n))\n",
    "preds_ht = preds_ht.order_by('Ensembl_transcriptid', 'locus', 'alleles')\n",
    "preds_agg = preds_ht.group_by('Ensembl_transcriptid', 'locus').aggregate(\n",
    "    preds = hl.struct(\n",
    "        Constraint = hl.agg.collect(hl.tuple([preds_ht.alleles[1], preds_ht.Constraint_1000_General_pred, preds_ht.Constraint_1000_General_n_pred])),\n",
    "        Core = hl.agg.collect(hl.tuple([preds_ht.alleles[1], preds_ht.Core_1000_General_pred, preds_ht.Core_1000_General_n_pred])),\n",
    "        Complete = hl.agg.collect(hl.tuple([preds_ht.alleles[1], preds_ht.Complete_1000_General_pred, preds_ht.Complete_1000_General_n_pred]))\n",
    "    )\n",
    ")\n",
    "\n",
    "preds_agg = preds_agg.checkpoint(f'{my_bucket}/tmp/preds.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "id": "du9ru4eihwo",
   "source": "# VARIANT CONSEQUENCES (Extract from rgc_scaled.ht and group by locus)\n# Source: /storage/zoghbi/home/u235147/merged_vars/rgc_scaled.ht\n# Field: most_deleterious_consequence_cds\n\n# Define consequence categorization function\ndef categorize_consequence(csq):\n    \"\"\"\n    Map VEP consequence terms to simplified categories for filtering.\n    Categories: plof, missense, synonymous, other\n    \"\"\"\n    plof = {'frameshift_variant', 'stop_gained', 'splice_donor_variant',\n            'splice_acceptor_variant', 'start_lost', 'stop_lost'}\n    missense = {'missense_variant', 'inframe_insertion', 'inframe_deletion',\n                'protein_altering_variant'}\n    synonymous = {'synonymous_variant'}\n    \n    return (hl.case()\n        .when(hl.set(plof).contains(csq), 'plof')\n        .when(hl.set(missense).contains(csq), 'missense')\n        .when(hl.set(synonymous).contains(csq), 'synonymous')\n        .default('other'))\n\n# Load rgc_scaled.ht which contains consequence annotations\nprint(\"Loading rgc_scaled.ht for variant consequences...\")\nrgc_scaled = hl.read_table(f'{my_bucket}/rgc_scaled.ht')\n\n# Select only the fields we need: locus, alleles, region (for transcript), and consequence\n# Filter to coding variants (where consequence is defined)\ncsq_ht = rgc_scaled.filter(hl.is_defined(rgc_scaled.most_deleterious_consequence_cds))\ncsq_ht = csq_ht.select(\n    'locus',\n    'alleles',\n    'region',  # Contains transcript-position info\n    csq_raw = csq_ht.most_deleterious_consequence_cds\n)\n\n# Add categorized consequence\ncsq_ht = csq_ht.annotate(\n    csq_category = categorize_consequence(csq_ht.csq_raw)\n)\n\n# Extract transcript ID from region (format: ENST00000123456-100)\ncsq_ht = csq_ht.annotate(\n    transcript_id = csq_ht.region.split('-')[0]\n)\n\n# Group by locus and transcript, collect tuples of (alt_allele, csq_category)\n# This mirrors the structure of the prediction columns\ncsq_agg = csq_ht.group_by('transcript_id', 'locus').aggregate(\n    variant_consequences = hl.agg.collect(hl.tuple([\n        csq_ht.alleles[1],  # Alternate allele\n        csq_ht.csq_category\n    ]))\n)\n\n# Checkpoint for efficiency\ncsq_agg = csq_agg.checkpoint(f'{my_bucket}/tmp/variant_consequences.ht', overwrite=True)\nprint(f\"Consequence table row count: {csq_agg.count()}\")\n\n# Show distribution of consequence categories\nprint(\"\\nConsequence category distribution:\")\ncsq_ht.group_by('csq_category').aggregate(n=hl.agg.count()).show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f3449",
   "metadata": {},
   "outputs": [],
   "source": "#This is the base table to which everything else is merged\n# Load base table and rekey by (locus, transcript_id)\nbase_ht = hl.read_table('/storage/zoghbi/home/u235147/merged_vars/tmp/constraint_metrics_by_locus_rgc_glaf.ht')\nbase_ht = base_ht.key_by('locus', 'transcript_id')\n\n\ngnomadV4_exomes_coverage = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV4_exomes_coverage_struct.ht')\ngnomadV4_genomes_coverage = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV3_coverage_struct.ht') # Note: Path mentions v3\nall_sites = hl.read_table(f'{my_bucket}/rgc_scaled.ht')\n\nbase_ht = base_ht.annotate(\n    gnomad_exomes_over_20 = gnomadV4_exomes_coverage[base_ht.locus].gnomADV4_coverage.over_20,\n    gnomad_genomes_over_20 = gnomadV4_genomes_coverage[base_ht.locus].gnomADV3_coverage.over_20\n)\n\n# Load preprocessed tables\nclinvar_ht = hl.read_table(f'{my_bucket}/tmp/clinvar_by_locus.ht')\ntrain_ht = hl.read_table(f'{my_bucket}/tmp/train_data.ht')\ndbnsfp_ht = hl.read_table(f'{my_bucket}/tmp/dbnsfp_scores.ht')\ndbnsfp_stacked_ht = hl.read_table(f'{my_bucket}/tmp/dbnsfp_stacked.ht')  # NEW: Stacked dbNSFP scores\ndomain_ht = hl.read_table(f'{my_bucket}/tmp/domains.ht')\npreds_ht = hl.read_table(f'{my_bucket}/tmp/preds.ht')\n\n# Merge ClinVar (by locus)\nclinvar_ht = clinvar_ht.key_by('locus')\nmerged = base_ht.annotate(\n    clinvar = clinvar_ht[base_ht.locus]\n)\n\n# Merge Training Labels (by locus)\ntrain_ht = train_ht.key_by('locus')\nmerged = merged.annotate(\n    training = train_ht[merged.locus]\n)\n\n# Merge dbNSFP scores (by locus, transcript) - max aggregation\ndbnsfp_ht = dbnsfp_ht.key_by('locus', 'Ensembl_transcriptid')\nmerged = merged.annotate(\n    dbnsfp = dbnsfp_ht[merged.locus, merged.transcript_id]\n)\n\n# Merge dbNSFP stacked scores (by locus, transcript) - NEW: variant-level scores\ndbnsfp_stacked_ht = dbnsfp_stacked_ht.key_by('locus', 'Ensembl_transcriptid')\nmerged = merged.annotate(\n    dbnsfp_stacked = dbnsfp_stacked_ht[merged.locus, merged.transcript_id].dbnsfp_stacked\n)\n\n# Expand stacked scores to top level for easier access\nmerged = merged.annotate(\n    AlphaMissense_stacked = merged.dbnsfp_stacked.AlphaMissense,\n    ESM1b_stacked = merged.dbnsfp_stacked.ESM1b\n)\n\n# Merge Domain information (by transcript)\ndomain_ht = domain_ht.key_by('transcript_id')\nmerged = merged.annotate(\n    domains = domain_ht[merged.transcript_id].domains\n)\n\n# Merge Constraint Predictions (by locus)\n# Format: array of ((Constraint_pred, Constraint_n), (Core_pred, Core_n), (Complete_pred, Complete_n))\npreds_ht = preds_ht.key_by('Ensembl_transcriptid', 'locus')\nmerged = merged.annotate(\n    preds = preds_ht[merged.transcript_id, merged.locus].preds\n)\n\nmerged = merged.annotate(\n   **{col: merged.preds[col] for col in list(merged.row.preds)}\n)\n\n# Merge Variant Consequences (by locus, transcript)\n# Format: array of (alt_allele, csq_category) tuples - matches prediction array structure\n# Categories: plof, missense, synonymous, other\ncsq_ht = hl.read_table(f'{my_bucket}/tmp/variant_consequences.ht')\ncsq_ht = csq_ht.key_by('transcript_id', 'locus')\nmerged = merged.annotate(\n    variant_consequences = csq_ht[merged.transcript_id, merged.locus].variant_consequences\n)\n\n# Checkpoint the merged table\nmerged = merged.checkpoint(f'{my_bucket}/tmp/merged_browser_data.ht', overwrite=True)\nprint(f\"Merged table row count: {merged.count()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_cell",
   "metadata": {},
   "outputs": [],
   "source": "# Export merged table to parquet - CHR2 ONLY for faster local iteration\nmerged_ht = hl.read_table(f'{my_bucket}/tmp/merged_browser_data.ht')\n\n# Filter to chr2 only\nCHR_FILTER = 'chr2'  # Set to None for all chromosomes\nif CHR_FILTER:\n    print(f\"Filtering to {CHR_FILTER}...\")\n    merged_ht = merged_ht.filter(merged_ht.locus.contig == CHR_FILTER)\n    print(f\"Rows after filter: {merged_ht.count()}\")\n    output_file = f'{my_bucket}/rgc_browser_data_{CHR_FILTER}.parquet'\nelse:\n    output_file = f'{my_bucket}/rgc_browser_data_merged.parquet'\n\n# Repartition to single partition and export to parquet\nmerged_repartitioned = merged_ht.repartition(1)\n\n# Convert to Spark DataFrame and export\nspark_df = merged_repartitioned.to_spark()\nspark_df.write.mode('overwrite').parquet(output_file)\n\nprint(f\"Exported to: {output_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae8fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'region': str \n",
      "    'locus': locus<GRCh38> \n",
      "    'rgc_any_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_any_obs_exomes_XX_XY': int64 \n",
      "    'rgc_any_count': int64 \n",
      "    'rgc_any_max_af': float64 \n",
      "    'rgc_syn_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_syn_obs_exomes_XX_XY': int64 \n",
      "    'rgc_syn_max_af': float64 \n",
      "    'rgc_syn_count': int64 \n",
      "    'rgc_mis_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_mis_obs_exomes_XX_XY': int64 \n",
      "    'rgc_mis_max_af': float64 \n",
      "    'rgc_mis_count': int64 \n",
      "    'rgc_stop_gained_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_stop_gained_obs_exomes_XX_XY': int64 \n",
      "    'rgc_stop_gained_max_af': float64 \n",
      "    'rgc_stop_gained_count': int64 \n",
      "    'aa_pos': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af0epos00': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af0epos00': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg06': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg06': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg05': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg05': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg04': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg04': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg03': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg03': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg02': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg02': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af0epos00': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg06': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg06': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg06': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg06': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg05': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg05': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg05': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg05': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg04': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg03': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg03': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg03': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg03': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg02': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg02': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg02': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg02': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af0epos00': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg06': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg06': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg06': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg06': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg05': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg05': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg05': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg05': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg04': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg04': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg04': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg04': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg03': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg03': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg03': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg03': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg02': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg02': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg02': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_3bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_3bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_9bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_9bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_e_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_oe_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_e_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_oe_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_e_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_oe_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_e_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_oe_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_e_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_oe_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_e_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_oe_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_3bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_3bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_9bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_9bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_21bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_21bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_45bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_45bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_93bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_93bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_3bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_3bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_9bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_9bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_21bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_21bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_45bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_45bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_93bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_93bp_oe_af0epos00': float64 \n",
      "    'transcript_id': str \n",
      "    'HGNC': str \n",
      "    'chrom': str \n",
      "    'pos': int32 \n",
      "----------------------------------------\n",
      "Key: ['locus', 'region']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_ht = hl.read_table('/storage/zoghbi/home/u235147/merged_vars/tmp/constraint_metrics_by_locus_rgc_glaf.ht')\n",
    "base_ht.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6b7da",
   "metadata": {},
   "outputs": [],
   "source": "!cd {my_bucket}/gosling_mvp && python preprocess_mis_all.py "
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}