{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc342201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fad55a85-607b-423c-a510-b060654a34fc\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"fad55a85-607b-423c-a510-b060654a34fc\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fad55a85-607b-423c-a510-b060654a34fc\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 3.5.4\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.134-952ae203dbbe\n",
      "LOGGING: writing to /storage/zoghbi/home/u235147/merged_vars/hail-20251219-1435-0.2.134-952ae203dbbe.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"e3596359-463e-4091-bd31-b7cc87433636\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"e3596359-463e-4091-bd31-b7cc87433636\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"e3596359-463e-4091-bd31-b7cc87433636\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 14:40:58.940 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2025-12-19 14:41:25.972 Hail: INFO: Coerced sorted dataset\n",
      "2025-12-19 14:41:45.885 Hail: INFO: wrote table with 27778551 rows in 96 partitions to /storage/zoghbi/home/u235147/merged_vars/tmp/preds.ht\n",
      "2025-12-19 14:42:17.503 Hail: INFO: Coerced sorted dataset\n",
      "2025-12-19 14:42:28.445 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2025-12-19 14:43:23.631 Hail: INFO: Coerced sorted dataset\n",
      "2025-12-19 14:43:36.936 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2025-12-19 14:45:12.650 Hail: INFO: wrote table with 33387608 rows in 40 partitions to /storage/zoghbi/home/u235147/merged_vars/tmp/merged_browser_data.ht\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "cpus = 96\n",
    "memory = int(3600*cpus/256)\n",
    "\n",
    "\n",
    "tmpdir = '/local/tmp'\n",
    "\n",
    "config = {\n",
    "    'spark.driver.memory': f'{memory}g',  #Set to total memory\n",
    "    'spark.executor.memory': f'{memory}g',\n",
    "    'spark.local.dir': tmpdir,\n",
    "    'spark.ui.enabled': 'false'\n",
    "}\n",
    "\n",
    "hl.init(spark_conf=config, master=f'local[{cpus}]', tmp_dir=tmpdir, local_tmpdir=tmpdir)\n",
    "\n",
    "hl.plot.output_notebook()\n",
    "%matplotlib inline\n",
    "\n",
    "rg38 = hl.get_reference('GRCh38')\n",
    "\n",
    "#!wget https://storage.googleapis.com/hail-common/references/Homo_sapiens_assembly38.fasta.fai\n",
    "#!wget https://storage.googleapis.com/hail-common/references/Homo_sapiens_assembly38.fasta.gz\n",
    "\n",
    "rg38.add_sequence('Homo_sapiens_assembly38.fasta.gz',\n",
    "                            'Homo_sapiens_assembly38.fasta.fai')\n",
    "\n",
    "#!wget https://storage.googleapis.com/hail-common/references/grch37_to_grch38.over.chain.gz\n",
    "# rg37 = hl.get_reference('GRCh37') \n",
    "# rg37.add_liftover('grch37_to_grch38.over.chain.gz', rg38)\n",
    "\n",
    "my_bucket = '/storage/zoghbi/home/u235147/merged_vars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b9bcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/storage/zoghbi/home/u235147/miniforge3/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "#CLINVAR DATA (Add P/LP missense variants to locus-level data, group by locus, count variants and include clinrevstat metadata as a comma separated list)\n",
    "clinvar_file = 'clinvar_20251013.vcf.gz'\n",
    "#!wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/weekly/{clinvar_file}\n",
    "#!gsutil cp {clinvar_file} {my_bucket}/tmp/\n",
    "recode = {str(i): f\"chr{i}\" for i in list(range(1, 23)) + ['X', 'Y']}\n",
    "recode['MT'] = 'chrM'\n",
    "clinvar_data = hl.import_vcf(f'{my_bucket}/{clinvar_file}', reference_genome='GRCh38', contig_recoding=recode, skip_invalid_loci=True, force_bgz=True).rows()\n",
    "clinvar_data = clinvar_data.key_by('locus', 'alleles')\n",
    "clinvar_data = clinvar_data.select(\n",
    "    clinvar_status = clinvar_data.info.CLNREVSTAT,\n",
    "    clinvar_label = clinvar_data.info.CLNSIG,\n",
    "    clinvar_var_type = clinvar_data.info.MC.map(lambda x: x.split('|')[1])\n",
    ")\n",
    "\n",
    "# Group by locus - aggregate to count variants and combine metadata as comma-separated lists\n",
    "clinvar_by_locus = clinvar_data.group_by('locus').aggregate(\n",
    "    clinvar_count = hl.agg.count(),\n",
    "    clinvar_status_list = hl.agg.collect(hl.delimit(clinvar_data.clinvar_status, ',')),\n",
    "    clinvar_label_list = hl.agg.collect(hl.delimit(clinvar_data.clinvar_label, ',')),\n",
    "    clinvar_var_type_list = hl.agg.collect(hl.delimit(clinvar_data.clinvar_var_type, ','))\n",
    ")\n",
    "\n",
    "clinvar_by_locus = clinvar_by_locus.checkpoint(f'{my_bucket}/tmp/clinvar_by_locus.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70f2ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# TRAINING LABELS: (Include as locus level information, with a count for each label (unlabelled, labelled, unlabelled_high_qual, labelled_high_qual))\n",
    "train_data = hl.import_table('/local/Missense_Predictor_copy/_rgc_train_vars.csv', delimiter=',')\n",
    "train_data = train_data.key_by(\n",
    "    locus = hl.locus('chr' + train_data.ID_38.split('-')[0], hl.int(train_data.ID_38.split('-')[1]), reference_genome='GRCh38')\n",
    ")\n",
    "train_data = train_data.group_by('locus').aggregate(\n",
    "    train_counts = hl.struct(\n",
    "        unlabelled = hl.agg.count_where(train_data.label == 'unlabelled'),\n",
    "        labelled = hl.agg.count_where(train_data.label == 'labelled'),\n",
    "        unlabelled_high_qual = hl.agg.count_where((train_data.label == 'unlabelled') & (train_data.High_Qual == 'True')),\n",
    "        labelled_high_qual = hl.agg.count_where((train_data.label == 'labelled') & (train_data.High_Qual == 'True'))\n",
    "    )\n",
    ")\n",
    "train_data = train_data.checkpoint(f'{my_bucket}/tmp/train_data.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54ee540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/storage/zoghbi/home/u235147/miniforge3/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 2:====================================================>(1174 + 1) / 1175]\r"
     ]
    }
   ],
   "source": [
    "#Locus level data (Collapse to max value for that locus and transcript, rename the columns to reflect this)\n",
    "ht = hl.read_table('/local/Missense_Predictor_copy/Data/dbnsfp/All_missense_with_impute_mane_select_final_with_perc_with_impute_con_perc.ht')\n",
    "\n",
    "# Find OE/VIR exome_perc columns\n",
    "all_cols = list(ht.row)\n",
    "oe_vir_cols = [col for col in all_cols if ('_oe_' in col or '_vir_' in col) and '_exome_perc' in col]\n",
    "\n",
    "# Score columns to include\n",
    "score_cols = ['AlphaMissense_am_pathogenicity', 'RGC_MTR.MTR', 'RGC_MTR.MTRpercentile_exome', \n",
    "              'Non_Neuro_CCR.resid_pctile', 'ESM1b_score', 'AlphaSync.plddt', 'AlphaSync.plddt10',\n",
    "              'AlphaSync.relasa', 'AlphaSync.relasa10', 'AlphaMissense_am_pathogenicity_exome_perc', \n",
    "              'ESM1b_score_exome_perc', 'AlphaSync.plddt_exome_perc', 'AlphaSync.plddt10_exome_perc',\n",
    "              'AlphaSync.relasa_exome_perc', 'AlphaSync.relasa10_exome_perc']\n",
    "\n",
    "# Create locus column\n",
    "ht = ht.annotate(\n",
    "    locus = hl.locus(ht['locus.contig'], ht['locus.position'], reference_genome='GRCh38')\n",
    ")\n",
    "\n",
    "# Build selection dictionary with sanitized column names (replace . with _)\n",
    "select_dict = {}\n",
    "for col in score_cols:\n",
    "    if col in all_cols:\n",
    "        select_dict[col.replace('.', '_')] = ht[col]\n",
    "for col in oe_vir_cols:\n",
    "    select_dict[col] = ht[col]\n",
    "\n",
    "ht_selected = ht.select(\n",
    "    'locus',\n",
    "    'alleles', \n",
    "    'Ensembl_transcriptid',\n",
    "    **select_dict\n",
    ")\n",
    "\n",
    "# Group by locus and transcript, take max of each numeric column\n",
    "agg_dict = {}\n",
    "for col in select_dict.keys():\n",
    "    agg_dict[f'max_{col}'] = hl.agg.max(ht_selected[col])\n",
    "\n",
    "dbnsfp_ht = ht_selected.group_by('locus', 'Ensembl_transcriptid').aggregate(**agg_dict)\n",
    "\n",
    "dbnsfp_ht = dbnsfp_ht.checkpoint(f'{my_bucket}/tmp/dbnsfp_scores.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467326",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_block = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MANE Select Domain Annotation Track Pipeline - Version 2\n",
    "=========================================================\n",
    "\n",
    "Simplified approach using UniProt REST API to fetch InterPro domains.\n",
    "\n",
    "Output schema:\n",
    "- transcript_id_ensembl: MANE Select transcript (ENST...)\n",
    "- protein_id_uniprot: UniProt accession\n",
    "- domain_id_interpro: InterPro ID\n",
    "- domain_name: Domain name\n",
    "- domain_start_aa: Amino acid start (1-based)\n",
    "- domain_end_aa: Amino acid end (1-based)\n",
    "- source_db: Source database (Pfam, etc.)\n",
    "\n",
    "Author: Bioinformatics Pipeline Assistant\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('mane_domain_pipeline_v2.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Pipeline configuration.\"\"\"\n",
    "    \n",
    "    data_dir: Path = field(default_factory=lambda: Path(\"data\"))\n",
    "    raw_dir: Path = field(default_factory=lambda: Path(\"data/raw\"))\n",
    "    cache_dir: Path = field(default_factory=lambda: Path(\"data/cache\"))\n",
    "    output_dir: Path = field(default_factory=lambda: Path(\"output\"))\n",
    "    \n",
    "    # MANE Select\n",
    "    mane_summary_url: str = \"https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/current/MANE.GRCh38.v1.4.summary.txt.gz\"\n",
    "    \n",
    "    # UniProt API\n",
    "    uniprot_batch_size: int = 100\n",
    "    uniprot_api_delay: float = 0.1  # Seconds between API calls\n",
    "    \n",
    "    # Output\n",
    "    output_filename: str = \"mane_domain_track_v2\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        for d in [self.data_dir, self.raw_dir, self.cache_dir, self.output_dir]:\n",
    "            d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Download Utilities\n",
    "# =============================================================================\n",
    "\n",
    "def download_file(url: str, output_path: Path, description: str = None) -> Path:\n",
    "    \"\"\"Download a file using aria2c or requests.\"\"\"\n",
    "    if output_path.exists():\n",
    "        logger.info(f\"File already exists: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    logger.info(f\"Downloading: {url}\")\n",
    "    \n",
    "    aria2c_path = shutil.which('aria2c')\n",
    "    if aria2c_path:\n",
    "        try:\n",
    "            cmd = [\n",
    "                'aria2c', '--max-connection-per-server=8', '--split=8',\n",
    "                '--min-split-size=1M', '--file-allocation=none',\n",
    "                '--continue=true', '--auto-file-renaming=false',\n",
    "                '-d', str(output_path.parent), '-o', output_path.name, url\n",
    "            ]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if result.returncode == 0 and output_path.exists():\n",
    "                logger.info(f\"Downloaded: {output_path}\")\n",
    "                return output_path\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"aria2c failed: {e}, falling back to requests\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        with tqdm(total=total_size, unit='iB', unit_scale=True, desc=description) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MANE Select Parsing\n",
    "# =============================================================================\n",
    "\n",
    "def parse_mane_summary(summary_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse MANE summary to get transcript-protein mappings.\"\"\"\n",
    "    logger.info(f\"Parsing MANE summary: {summary_path}\")\n",
    "    \n",
    "    df = pd.read_csv(summary_path, sep='\\t', compression='gzip')\n",
    "    \n",
    "    # Filter to MANE Select only\n",
    "    if 'MANE_status' in df.columns:\n",
    "        df = df[df['MANE_status'] == 'MANE Select'].copy()\n",
    "    \n",
    "    logger.info(f\"Found {len(df)} MANE Select entries\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UniProt Mapping\n",
    "# =============================================================================\n",
    "\n",
    "def map_ensembl_to_uniprot(\n",
    "    ensembl_protein_ids: List[str],\n",
    "    id_mapping_path: Path\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Map Ensembl protein IDs to UniProt accessions.\"\"\"\n",
    "    logger.info(\"Mapping Ensembl proteins to UniProt\")\n",
    "    \n",
    "    # Build lookup set (strip versions)\n",
    "    ensp_set = set()\n",
    "    for ensp in ensembl_protein_ids:\n",
    "        if pd.notna(ensp):\n",
    "            ensp_set.add(str(ensp))\n",
    "            ensp_set.add(str(ensp).split('.')[0])\n",
    "    \n",
    "    logger.info(f\"Looking up {len(ensp_set)} Ensembl protein IDs\")\n",
    "    \n",
    "    # Parse ID mapping file\n",
    "    ensp_to_uniprot = {}\n",
    "    reviewed_accs = set()\n",
    "    \n",
    "    with gzip.open(id_mapping_path, 'rt') as f:\n",
    "        for line in tqdm(f, desc=\"Parsing ID mapping\"):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "            \n",
    "            uniprot_acc, id_type, id_val = parts\n",
    "            \n",
    "            if id_type == 'Ensembl_PRO':\n",
    "                id_base = id_val.split('.')[0]\n",
    "                if id_val in ensp_set or id_base in ensp_set:\n",
    "                    # Store both versioned and unversioned\n",
    "                    if id_val not in ensp_to_uniprot:\n",
    "                        ensp_to_uniprot[id_val] = []\n",
    "                    ensp_to_uniprot[id_val].append(uniprot_acc)\n",
    "                    \n",
    "                    if id_base not in ensp_to_uniprot:\n",
    "                        ensp_to_uniprot[id_base] = []\n",
    "                    if uniprot_acc not in ensp_to_uniprot[id_base]:\n",
    "                        ensp_to_uniprot[id_base].append(uniprot_acc)\n",
    "            \n",
    "            elif id_type == 'UniProtKB-ID' and '_HUMAN' in id_val:\n",
    "                reviewed_accs.add(uniprot_acc)\n",
    "    \n",
    "    # Select best UniProt accession for each ENSP\n",
    "    final_mapping = {}\n",
    "    for ensp, accs in ensp_to_uniprot.items():\n",
    "        if len(accs) == 1:\n",
    "            final_mapping[ensp] = accs[0]\n",
    "        else:\n",
    "            # Prefer reviewed\n",
    "            reviewed = [a for a in accs if a in reviewed_accs]\n",
    "            if reviewed:\n",
    "                final_mapping[ensp] = reviewed[0]\n",
    "            else:\n",
    "                final_mapping[ensp] = accs[0]\n",
    "    \n",
    "    logger.info(f\"Mapped {len(final_mapping)} Ensembl proteins to UniProt\")\n",
    "    return final_mapping\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UniProt API Domain Fetching\n",
    "# =============================================================================\n",
    "\n",
    "def is_family_domain(domain: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a domain is Family-level (should be filtered out).\n",
    "    \n",
    "    Filters out:\n",
    "    - Domains with domain_type == 'Family' or 'family' (case-insensitive)\n",
    "    \"\"\"\n",
    "    domain_type = str(domain.get('domain_type', '')).lower()\n",
    "    \n",
    "    # Filter Family-level domains (case-insensitive)\n",
    "    if domain_type == 'family':\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def fetch_uniprot_domains_batch(\n",
    "    uniprot_accessions: List[str],\n",
    "    config: PipelineConfig\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Fetch InterPro domain annotations from UniProt REST API.\n",
    "    \n",
    "    Uses the UniProt API to get domain features for each protein.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Fetching domains for {len(uniprot_accessions)} proteins via UniProt API\")\n",
    "    \n",
    "    domains = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(uniprot_accessions), config.uniprot_batch_size), \n",
    "                  desc=\"Fetching from UniProt\"):\n",
    "        batch = uniprot_accessions[i:i + config.uniprot_batch_size]\n",
    "        \n",
    "        # Build query for batch\n",
    "        acc_query = ' OR '.join([f'accession:{acc}' for acc in batch])\n",
    "        \n",
    "        url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "        params = {\n",
    "            'query': acc_query,\n",
    "            'format': 'json',\n",
    "            'fields': 'accession,xref_interpro',\n",
    "            'size': len(batch)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for entry in data.get('results', []):\n",
    "                acc = entry.get('primaryAccession', '')\n",
    "                \n",
    "                # Extract InterPro cross-references\n",
    "                xrefs = entry.get('uniProtKBCrossReferences', [])\n",
    "                interpro_refs = [x for x in xrefs if x.get('database') == 'InterPro']\n",
    "                \n",
    "                if interpro_refs:\n",
    "                    domains[acc] = []\n",
    "                    for ref in interpro_refs:\n",
    "                        ipr_id = ref.get('id', '')\n",
    "                        props = {p.get('key'): p.get('value') for p in ref.get('properties', [])}\n",
    "                        \n",
    "                        domains[acc].append({\n",
    "                            'interpro_id': ipr_id,\n",
    "                            'domain_name': props.get('EntryName', ''),\n",
    "                        })\n",
    "            \n",
    "            time.sleep(config.uniprot_api_delay)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"API error for batch starting at {i}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Found domains for {len(domains)} proteins\")\n",
    "    return domains\n",
    "\n",
    "\n",
    "def fetch_interpro_domains_direct(\n",
    "    uniprot_accessions: List[str],\n",
    "    config: PipelineConfig,\n",
    "    cache_path: Optional[Path] = None\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Fetch InterPro domains using InterPro API directly.\n",
    "    \n",
    "    This gives us the actual domain coordinates.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Fetching domains for {len(uniprot_accessions)} proteins via InterPro API\")\n",
    "    \n",
    "    # Check cache\n",
    "    if cache_path and cache_path.exists():\n",
    "        logger.info(f\"Loading cached domains from {cache_path}\")\n",
    "        with open(cache_path, 'r') as f:\n",
    "            cached = json.load(f)\n",
    "        # Drop Family-level domains (Family type and PANTHER)\n",
    "        cached = {\n",
    "            acc: [d for d in feats if not is_family_domain(d)]\n",
    "            for acc, feats in cached.items()\n",
    "            if any(not is_family_domain(d) for d in feats)\n",
    "        }\n",
    "        logger.info(f\"Filtered out Family-level domains from cache\")\n",
    "        return cached\n",
    "    \n",
    "    domains = {}\n",
    "    \n",
    "    for acc in tqdm(uniprot_accessions, desc=\"Fetching from InterPro\"):\n",
    "        # Use InterPro API to get protein matches\n",
    "        url = f\"https://www.ebi.ac.uk/interpro/api/entry/interpro/protein/uniprot/{acc}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers={'Accept': 'application/json'})\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                domains[acc] = []\n",
    "                for result in data.get('results', []):\n",
    "                    metadata = result.get('metadata', {})\n",
    "                    ipr_id = metadata.get('accession', '')\n",
    "                    ipr_name = metadata.get('name', '')\n",
    "                    ipr_type = metadata.get('type', '')\n",
    "                    source_db = metadata.get('source_database', '')\n",
    "                    \n",
    "                    # Get protein locations\n",
    "                    proteins = result.get('proteins', [])\n",
    "                    for protein in proteins:\n",
    "                        for location in protein.get('entry_protein_locations', []):\n",
    "                            for fragment in location.get('fragments', []):\n",
    "                                domains[acc].append({\n",
    "                                    'interpro_id': ipr_id,\n",
    "                                    'domain_name': ipr_name,\n",
    "                                    'domain_type': ipr_type,\n",
    "                                    'source_db': source_db,\n",
    "                                    'start_aa': fragment.get('start', 0),\n",
    "                                    'end_aa': fragment.get('end', 0)\n",
    "                                })\n",
    "            \n",
    "            time.sleep(config.uniprot_api_delay)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.debug(f\"Error fetching {acc}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Found domains for {len(domains)} proteins\")\n",
    "    \n",
    "    # Filter out Family-level domains (Family type and PANTHER)\n",
    "    filtered_domains = {\n",
    "        acc: [d for d in feats if not is_family_domain(d)]\n",
    "        for acc, feats in domains.items()\n",
    "        if any(not is_family_domain(d) for d in feats)\n",
    "    }\n",
    "    \n",
    "    # Cache results\n",
    "    if cache_path:\n",
    "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump(filtered_domains, f)\n",
    "        logger.info(f\"Cached filtered domains to {cache_path} (Family-level domains excluded)\")\n",
    "    \n",
    "    logger.info(f\"Filtered out Family-level domains (Family type and PANTHER)\")\n",
    "    return filtered_domains\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(config: PipelineConfig):\n",
    "    \"\"\"Run the pipeline.\"\"\"\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"MANE Select Domain Annotation Track Pipeline v2\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Download MANE summary\n",
    "    logger.info(\"\\n[Step 1] Downloading MANE Select data...\")\n",
    "    mane_summary_path = download_file(\n",
    "        config.mane_summary_url,\n",
    "        config.raw_dir / \"MANE.GRCh38.summary.txt.gz\",\n",
    "        \"MANE Summary\"\n",
    "    )\n",
    "    \n",
    "    # Step 2: Parse MANE summary\n",
    "    logger.info(\"\\n[Step 2] Parsing MANE Select...\")\n",
    "    mane_df = parse_mane_summary(mane_summary_path)\n",
    "    \n",
    "    # Step 3: Map to UniProt\n",
    "    logger.info(\"\\n[Step 3] Mapping to UniProt...\")\n",
    "    \n",
    "    # Download ID mapping if needed\n",
    "    id_mapping_url = \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/by_organism/HUMAN_9606_idmapping.dat.gz\"\n",
    "    id_mapping_path = download_file(\n",
    "        id_mapping_url,\n",
    "        config.raw_dir / \"HUMAN_9606_idmapping.dat.gz\",\n",
    "        \"UniProt ID Mapping\"\n",
    "    )\n",
    "    \n",
    "    # Get Ensembl protein IDs from MANE\n",
    "    ensembl_proteins = mane_df['Ensembl_prot'].dropna().tolist()\n",
    "    \n",
    "    # Map to UniProt\n",
    "    ensp_to_uniprot = map_ensembl_to_uniprot(ensembl_proteins, id_mapping_path)\n",
    "    \n",
    "    # Add UniProt accession to MANE dataframe\n",
    "    mane_df['UniProt_acc'] = mane_df['Ensembl_prot'].apply(\n",
    "        lambda x: ensp_to_uniprot.get(str(x), ensp_to_uniprot.get(str(x).split('.')[0], '')) \n",
    "        if pd.notna(x) else ''\n",
    "    )\n",
    "    \n",
    "    # Get unique UniProt accessions\n",
    "    uniprot_accessions = mane_df['UniProt_acc'].dropna()\n",
    "    uniprot_accessions = [a for a in uniprot_accessions if a != '']\n",
    "    uniprot_accessions = list(set(uniprot_accessions))\n",
    "    \n",
    "    logger.info(f\"Found {len(uniprot_accessions)} unique UniProt accessions\")\n",
    "    \n",
    "    # Step 4: Fetch InterPro domains\n",
    "    logger.info(\"\\n[Step 4] Fetching InterPro domain annotations...\")\n",
    "    \n",
    "    cache_path = config.cache_dir / \"interpro_domains_cache.json\"\n",
    "    interpro_domains = fetch_interpro_domains_direct(\n",
    "        uniprot_accessions, config, cache_path\n",
    "    )\n",
    "    \n",
    "    # Step 5: Build output table\n",
    "    logger.info(\"\\n[Step 5] Building output table...\")\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for _, row in tqdm(mane_df.iterrows(), total=len(mane_df), desc=\"Building records\"):\n",
    "        enst = row.get('Ensembl_nuc', '')\n",
    "        ensp = row.get('Ensembl_prot', '')\n",
    "        uniprot = row.get('UniProt_acc', '')\n",
    "        gene_symbol = row.get('symbol', '')\n",
    "        \n",
    "        if not uniprot or uniprot not in interpro_domains:\n",
    "            continue\n",
    "        \n",
    "        for domain in interpro_domains[uniprot]:\n",
    "            records.append({\n",
    "                'transcript_id_ensembl': enst,\n",
    "                'protein_id_ensembl': ensp,\n",
    "                'protein_id_uniprot': uniprot,\n",
    "                'gene_symbol': gene_symbol,\n",
    "                'domain_id_interpro': domain.get('interpro_id', ''),\n",
    "                'domain_name': domain.get('domain_name', ''),\n",
    "                'domain_type': domain.get('domain_type', ''),\n",
    "                'source_db': domain.get('source_db', ''),\n",
    "                'domain_start_aa': domain.get('start_aa', 0),\n",
    "                'domain_end_aa': domain.get('end_aa', 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    logger.info(f\"Created {len(df)} domain annotation records\")\n",
    "    \n",
    "    # Step 6: Write output\n",
    "    logger.info(\"\\n[Step 6] Writing Parquet output...\")\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        # Sort by transcript\n",
    "        df = df.sort_values(['transcript_id_ensembl', 'domain_start_aa'])\n",
    "        \n",
    "        # Write Parquet\n",
    "        output_path = config.output_dir / f\"{config.output_filename}.parquet\"\n",
    "        df.to_parquet(output_path, index=False, compression='snappy')\n",
    "        logger.info(f\"Wrote {len(df)} records to: {output_path}\")\n",
    "        \n",
    "        # Also write TSV for easy inspection\n",
    "        tsv_path = config.output_dir / f\"{config.output_filename}.tsv\"\n",
    "        df.to_csv(tsv_path, sep='\\t', index=False)\n",
    "        logger.info(f\"Wrote TSV to: {tsv_path}\")\n",
    "    else:\n",
    "        logger.warning(\"No domain records to write!\")\n",
    "    \n",
    "    # Summary\n",
    "    logger.info(\"\\n\" + \"=\" * 70)\n",
    "    logger.info(\"Pipeline Complete - Summary\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"MANE Select transcripts: {len(mane_df)}\")\n",
    "    logger.info(f\"Transcripts with UniProt: {len(mane_df[mane_df['UniProt_acc'] != ''])}\")\n",
    "    logger.info(f\"UniProt proteins with domains: {len(interpro_domains)}\")\n",
    "    logger.info(f\"Total domain annotations: {len(df)}\")\n",
    "    if len(df) > 0:\n",
    "        logger.info(f\"Unique InterPro entries: {df['domain_id_interpro'].nunique()}\")\n",
    "        logger.info(f\"Genes with domains: {df['gene_symbol'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"MANE Domain Track Pipeline v2\")\n",
    "    parser.add_argument('--data-dir', type=Path, default=Path('data'))\n",
    "    parser.add_argument('--output-dir', type=Path, default=Path('output'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config = PipelineConfig(\n",
    "        data_dir=args.data_dir,\n",
    "        raw_dir=args.data_dir / 'raw',\n",
    "        cache_dir=args.data_dir / 'cache',\n",
    "        output_dir=args.output_dir\n",
    "    )\n",
    "    \n",
    "    run_pipeline(config)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"mane_domain_pipeline_v2.py\", \"w\") as f:\n",
    "    f.write(code_block)\n",
    "\n",
    "!python mane_domain_pipeline_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafdd21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>domain_type</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;Coiled_coil&quot;</td><td>85</td></tr><tr><td>&quot;Disordered&quot;</td><td>30</td></tr><tr><td>&quot;Homologous_superfamily&quot;</td><td>12747</td></tr><tr><td>&quot;Domain&quot;</td><td>13902</td></tr><tr><td>&quot;Conserved_site&quot;</td><td>1849</td></tr><tr><td>&quot;Repeat&quot;</td><td>901</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 2)\n",
       "\n",
       " domain_type             count \n",
       " ---                     ---   \n",
       " str                     u32   \n",
       "\n",
       " Coiled_coil             85    \n",
       " Disordered              30    \n",
       " Homologous_superfamily  12747 \n",
       " Domain                  13902 \n",
       " Conserved_site          1849  \n",
       " Repeat                  901   \n",
       ""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_parquet('/storage/zoghbi/home/u235147/merged_vars/output/mane_domain_track_v2.parquet')\n",
    "df['domain_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9d7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MANE Select genes: 19338\n",
      "Genes with domains: 7410\n",
      "Genes without domains: 11928\n",
      "\n",
      "First 10 examples of genes without domains:\n",
      "1. A1BG\n",
      "   Transcript: ENST00000263100.8\n",
      "   RefSeq: NM_130786.4\n",
      "\n",
      "2. A1CF\n",
      "   Transcript: ENST00000373997.8\n",
      "   RefSeq: NM_014576.4\n",
      "\n",
      "3. A2ML1\n",
      "   Transcript: ENST00000299698.12\n",
      "   RefSeq: NM_144670.6\n",
      "\n",
      "4. AAAS\n",
      "   Transcript: ENST00000209873.9\n",
      "   RefSeq: NM_015665.6\n",
      "\n",
      "5. AACS\n",
      "   Transcript: ENST00000316519.11\n",
      "   RefSeq: NM_023928.5\n",
      "\n",
      "6. AADACL2\n",
      "   Transcript: ENST00000356517.4\n",
      "   RefSeq: NM_207365.4\n",
      "\n",
      "7. AADAT\n",
      "   Transcript: ENST00000337664.9\n",
      "   RefSeq: NM_016228.4\n",
      "\n",
      "8. AAGAB\n",
      "   Transcript: ENST00000261880.10\n",
      "   RefSeq: NM_024666.5\n",
      "\n",
      "9. AAK1\n",
      "   Transcript: ENST00000409085.9\n",
      "   RefSeq: NM_014911.5\n",
      "\n",
      "10. AAMDC\n",
      "   Transcript: ENST00000393427.7\n",
      "   RefSeq: NM_024684.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load MANE Select data\n",
    "mane_path = Path('data/raw/MANE.GRCh38.summary.txt.gz')\n",
    "df_mane = pd.read_csv(mane_path, sep='\\t', compression='gzip')\n",
    "df_mane = df_mane[df_mane['MANE_status'] == 'MANE Select'].copy()\n",
    "\n",
    "# Load output with domains\n",
    "output_path = Path('output/mane_domain_track_v2.parquet')\n",
    "df_output = pd.read_parquet(output_path)\n",
    "\n",
    "# Get all genes in MANE\n",
    "all_mane_genes = set(df_mane['symbol'].dropna().unique())\n",
    "\n",
    "# Get genes with domains\n",
    "genes_with_domains = set(df_output['gene_symbol'].dropna().unique())\n",
    "\n",
    "# Find genes without domains\n",
    "genes_without_domains = all_mane_genes - genes_with_domains\n",
    "\n",
    "print(f\"Total MANE Select genes: {len(all_mane_genes)}\")\n",
    "print(f\"Genes with domains: {len(genes_with_domains)}\")\n",
    "print(f\"Genes without domains: {len(genes_without_domains)}\")\n",
    "print(\"\\nFirst 10 examples of genes without domains:\")\n",
    "for i, gene in enumerate(sorted(genes_without_domains)[:10], 1):\n",
    "    # Get transcript info for this gene\n",
    "    gene_info = df_mane[df_mane['symbol'] == gene].iloc[0]\n",
    "    print(f\"{i}. {gene}\")\n",
    "    print(f\"   Transcript: {gene_info.get('Ensembl_nuc', 'N/A')}\")\n",
    "    print(f\"   RefSeq: {gene_info.get('RefSeq_nuc', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82c8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "#Domain information (Load from pre-generated parquet and prepare for merging by transcript)\n",
    "# The domain parquet was generated by the MANE Domain Track Pipeline\n",
    "spark_session = hl.utils.java.Env.spark_session()\n",
    "\n",
    "# Read domain parquet\n",
    "domain_df = spark_session.read.parquet(f'{my_bucket}/output/mane_domain_track_v2.parquet')\n",
    "\n",
    "# Convert to Hail table\n",
    "domain_ht = hl.Table.from_spark(domain_df)\n",
    "\n",
    "# Strip version from transcript_id for joining\n",
    "domain_ht = domain_ht.annotate(\n",
    "    transcript_id = domain_ht.transcript_id_ensembl.split('\\\\.')[0]\n",
    ")\n",
    "\n",
    "# Key by transcript_id and aggregate domains into an array (multiple domains per transcript)\n",
    "domain_ht = domain_ht.key_by('transcript_id')\n",
    "domain_agg = domain_ht.group_by('transcript_id').aggregate(\n",
    "    domains = hl.agg.collect(hl.struct(\n",
    "        domain_id = domain_ht.domain_id_interpro,\n",
    "        domain_name = domain_ht.domain_name,\n",
    "        domain_type = domain_ht.domain_type,\n",
    "        source_db = domain_ht.source_db,\n",
    "        start_aa = domain_ht.domain_start_aa,\n",
    "        end_aa = domain_ht.domain_end_aa\n",
    "    ))\n",
    ")\n",
    "\n",
    "domain_agg = domain_agg.checkpoint(f'{my_bucket}/tmp/domains.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d025bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'ID_38': str \n",
      "    'genename': str \n",
      "    'alleles': array<str> \n",
      "    'Ensembl_transcriptid': str \n",
      "    'aapos': int32 \n",
      "    'aaref': str \n",
      "    'Uniprot_acc': str \n",
      "    'aaalt': str \n",
      "    'plddt_lt_60': bool \n",
      "    'Constraint_1000_General_pred': float32 \n",
      "    'Constraint_1000_General_pred_std': float32 \n",
      "    'Constraint_1000_General_label': float32 \n",
      "    'Constraint_1000_General_n_pred': int32 \n",
      "    'Core_1000_General_pred': float32 \n",
      "    'Core_1000_General_pred_std': float32 \n",
      "    'Core_1000_General_label': float32 \n",
      "    'Core_1000_General_n_pred': int32 \n",
      "    'Complete_1000_General_pred': float32 \n",
      "    'Complete_1000_General_pred_std': float32 \n",
      "    'Complete_1000_General_label': float32 \n",
      "    'Complete_1000_General_n_pred': int32 \n",
      "    'Constraint_200_rgc_zero_General_pred': float32 \n",
      "    'Constraint_200_rgc_zero_General_pred_std': float32 \n",
      "    'Constraint_200_rgc_zero_General_label': float32 \n",
      "    'Constraint_200_rgc_zero_General_n_pred': int32 \n",
      "    'Core_200_rgc_zero_General_pred': float32 \n",
      "    'Core_200_rgc_zero_General_pred_std': float32 \n",
      "    'Core_200_rgc_zero_General_label': float32 \n",
      "    'Core_200_rgc_zero_General_n_pred': int32 \n",
      "    'Complete_200_rgc_zero_General_pred': float32 \n",
      "    'Complete_200_rgc_zero_General_pred_std': float32 \n",
      "    'Complete_200_rgc_zero_General_label': float32 \n",
      "    'Complete_200_rgc_zero_General_n_pred': int32 \n",
      "    'Complete_200_obs_weighted_General_pred': float32 \n",
      "    'Complete_200_obs_weighted_General_pred_std': float32 \n",
      "    'Complete_200_obs_weighted_General_label': float32 \n",
      "    'Complete_200_obs_weighted_General_n_pred': int32 \n",
      "    'Constraint_200_obs_weighted_General_pred': float32 \n",
      "    'Constraint_200_obs_weighted_General_pred_std': float32 \n",
      "    'Constraint_200_obs_weighted_General_label': float32 \n",
      "    'Constraint_200_obs_weighted_General_n_pred': int32 \n",
      "    'Complete_200_site_weighted_General_pred': float32 \n",
      "    'Complete_200_site_weighted_General_pred_std': float32 \n",
      "    'Complete_200_site_weighted_General_label': float32 \n",
      "    'Complete_200_site_weighted_General_n_pred': int32 \n",
      "    'Constraint_200_site_weighted_General_pred': float32 \n",
      "    'Constraint_200_site_weighted_General_pred_std': float32 \n",
      "    'Constraint_200_site_weighted_General_label': float32 \n",
      "    'Constraint_200_site_weighted_General_n_pred': int32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_pred': float32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_pred_std': float32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_label': float32 \n",
      "    'Complete_PU_1to10_200_site_weighted_General_n_pred': int32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_pred': float32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_pred_std': float32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_label': float32 \n",
      "    'Constraint_PU_1to10_200_site_weighted_General_n_pred': int32 \n",
      "    'locus': locus<GRCh38> \n",
      "    'REVEL_score': str \n",
      "    'REVEL_rankscore': str \n",
      "    'MPC_score': str \n",
      "    'MPC_rankscore': str \n",
      "    'ESM1b_converted_rankscore': str \n",
      "    'AllofUs_ALL_AF': str \n",
      "    'RegeneronME_ALL_AF': str \n",
      "    'gnomAD4.1_joint_AF': str \n",
      "    'ESM1b_score': float64 \n",
      "    'iGEMME_score': float64 \n",
      "    'CPT.CPT1_score': float64 \n",
      "    'AlphaMissense_am_pathogenicity': float64 \n",
      "    'PopEve_Score': float64 \n",
      "    'PopEve_pop_adjusted_Score': float64 \n",
      "    'PopEve_pop_adjusted_ESM1v_Score': float64 \n",
      "----------------------------------------\n",
      "Key: []\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preds_ht = hl.read_table('/local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht')\n",
    "preds_ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "knphdflxbh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred====> (94 + 2) / 96]\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/storage/zoghbi/home/u235147/miniforge3/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.4.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 4:========================================================>(95 + 1) / 96]\r"
     ]
    }
   ],
   "source": [
    "# CONSTRAINT PREDICTIONS (Load predictions and group by locus into tuples of (pred, n_pred))\n",
    "# Source: /local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht\n",
    "preds_ht = hl.read_table('/local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht')\n",
    "\n",
    "# Create locus from ID_38 (format: chr-pos-ref-alt)\n",
    "preds_ht = preds_ht.annotate(\n",
    "    locus = hl.locus('chr' + preds_ht.ID_38.split('-')[0], hl.int(preds_ht.ID_38.split('-')[1]), reference_genome='GRCh38')\n",
    ")\n",
    "\n",
    "# Select the prediction columns we need\n",
    "preds_ht = preds_ht.select(\n",
    "    'locus',\n",
    "    'alleles',\n",
    "    'Ensembl_transcriptid',\n",
    "    'Constraint_1000_General_pred',\n",
    "    'Constraint_1000_General_n_pred',\n",
    "    'Core_1000_General_pred',\n",
    "    'Core_1000_General_n_pred',\n",
    "    'Complete_1000_General_pred',\n",
    "    'Complete_1000_General_n_pred'\n",
    ")\n",
    "\n",
    "preds_ht = preds_ht.annotate(\n",
    "    Const_Core_diff_1000_General_pred = preds_ht.Constraint_1000_General_pred - preds_ht.Core_1000_General_pred\n",
    ")\n",
    "\n",
    "# Group by locus and collect tuples of (pred, n_pred) for each model\n",
    "# Format: ((Constraint_pred, Constraint_n), (Core_pred, Core_n), (Complete_pred, Complete_n))\n",
    "preds_ht = preds_ht.order_by('Ensembl_transcriptid', 'locus', 'alleles')\n",
    "preds_agg = preds_ht.group_by('Ensembl_transcriptid', 'locus').aggregate(\n",
    "    preds = hl.struct(\n",
    "        Constraint = hl.agg.collect(hl.tuple([preds_ht.alleles[1], preds_ht.Constraint_1000_General_pred, preds_ht.Constraint_1000_General_n_pred])),\n",
    "        Core = hl.agg.collect(hl.tuple([preds_ht.alleles[1], preds_ht.Core_1000_General_pred, preds_ht.Core_1000_General_n_pred])),\n",
    "        Complete = hl.agg.collect(hl.tuple([preds_ht.alleles[1], preds_ht.Complete_1000_General_pred, preds_ht.Complete_1000_General_n_pred]))\n",
    "    )\n",
    ")\n",
    "\n",
    "preds_agg = preds_agg.checkpoint(f'{my_bucket}/tmp/preds.ht', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f3449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:======================================================> (39 + 1) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged table row count: 33387608\n"
     ]
    }
   ],
   "source": [
    "#This is the base table to which everything else is merged\n",
    "# Load base table and rekey by (locus, transcript_id)\n",
    "base_ht = hl.read_table('/storage/zoghbi/home/u235147/merged_vars/tmp/constraint_metrics_by_locus_rgc_glaf.ht')\n",
    "base_ht = base_ht.key_by('locus', 'transcript_id')\n",
    "\n",
    "\n",
    "gnomadV4_exomes_coverage = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV4_exomes_coverage_struct.ht')\n",
    "gnomadV4_genomes_coverage = hl.read_table('/storage/zoghbi/data/sharing/hail_tables/gnomadV3_coverage_struct.ht') # Note: Path mentions v3\n",
    "all_sites = hl.read_table(f'{my_bucket}/rgc_scaled.ht')\n",
    "\n",
    "base_ht = base_ht.annotate(\n",
    "    gnomad_exomes_over_20 = gnomadV4_exomes_coverage[base_ht.locus].gnomADV4_coverage.over_20,\n",
    "    gnomad_genomes_over_20 = gnomadV4_genomes_coverage[base_ht.locus].gnomADV3_coverage.over_20\n",
    ")\n",
    "\n",
    "# Load preprocessed tables\n",
    "clinvar_ht = hl.read_table(f'{my_bucket}/tmp/clinvar_by_locus.ht')\n",
    "train_ht = hl.read_table(f'{my_bucket}/tmp/train_data.ht')\n",
    "dbnsfp_ht = hl.read_table(f'{my_bucket}/tmp/dbnsfp_scores.ht')\n",
    "domain_ht = hl.read_table(f'{my_bucket}/tmp/domains.ht')\n",
    "preds_ht = hl.read_table(f'{my_bucket}/tmp/preds.ht')\n",
    "\n",
    "# Merge ClinVar (by locus)\n",
    "clinvar_ht = clinvar_ht.key_by('locus')\n",
    "merged = base_ht.annotate(\n",
    "    clinvar = clinvar_ht[base_ht.locus]\n",
    ")\n",
    "\n",
    "# Merge Training Labels (by locus)\n",
    "train_ht = train_ht.key_by('locus')\n",
    "merged = merged.annotate(\n",
    "    training = train_ht[merged.locus]\n",
    ")\n",
    "\n",
    "# Merge dbNSFP scores (by locus, transcript)\n",
    "dbnsfp_ht = dbnsfp_ht.key_by('locus', 'Ensembl_transcriptid')\n",
    "merged = merged.annotate(\n",
    "    dbnsfp = dbnsfp_ht[merged.locus, merged.transcript_id]\n",
    ")\n",
    "\n",
    "# Merge Domain information (by transcript)\n",
    "domain_ht = domain_ht.key_by('transcript_id')\n",
    "merged = merged.annotate(\n",
    "    domains = domain_ht[merged.transcript_id].domains\n",
    ")\n",
    "\n",
    "# Merge Constraint Predictions (by locus)\n",
    "# Format: array of ((Constraint_pred, Constraint_n), (Core_pred, Core_n), (Complete_pred, Complete_n))\n",
    "preds_ht = preds_ht.key_by('Ensembl_transcriptid', 'locus')\n",
    "merged = merged.annotate(\n",
    "    preds = preds_ht[merged.transcript_id, merged.locus].preds\n",
    ")\n",
    "\n",
    "merged = merged.annotate(\n",
    "   **{col: merged.preds[col] for col in list(merged.row.preds)}\n",
    ")\n",
    "\n",
    "# Checkpoint the merged table\n",
    "merged = merged.checkpoint(f'{my_bucket}/tmp/merged_browser_data.ht', overwrite=True)\n",
    "print(f\"Merged table row count: {merged.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_cell",
   "metadata": {},
   "outputs": [],
   "source": "# Export merged table to parquet - CHR2 ONLY for faster local iteration\nmerged_ht = hl.read_table(f'{my_bucket}/tmp/merged_browser_data.ht')\n\n# Filter to chr2 only\nCHR_FILTER = 'chr2'  # Set to None for all chromosomes\nif CHR_FILTER:\n    print(f\"Filtering to {CHR_FILTER}...\")\n    merged_ht = merged_ht.filter(merged_ht.locus.contig == CHR_FILTER)\n    print(f\"Rows after filter: {merged_ht.count()}\")\n    output_file = f'{my_bucket}/rgc_browser_data_{CHR_FILTER}.parquet'\nelse:\n    output_file = f'{my_bucket}/rgc_browser_data_merged.parquet'\n\n# Repartition to single partition and export to parquet\nmerged_repartitioned = merged_ht.repartition(1)\n\n# Convert to Spark DataFrame and export\nspark_df = merged_repartitioned.to_spark()\nspark_df.write.mode('overwrite').parquet(output_file)\n\nprint(f\"Exported to: {output_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae8fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'region': str \n",
      "    'locus': locus<GRCh38> \n",
      "    'rgc_any_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_any_obs_exomes_XX_XY': int64 \n",
      "    'rgc_any_count': int64 \n",
      "    'rgc_any_max_af': float64 \n",
      "    'rgc_syn_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_syn_obs_exomes_XX_XY': int64 \n",
      "    'rgc_syn_max_af': float64 \n",
      "    'rgc_syn_count': int64 \n",
      "    'rgc_mis_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_mis_obs_exomes_XX_XY': int64 \n",
      "    'rgc_mis_max_af': float64 \n",
      "    'rgc_mis_count': int64 \n",
      "    'rgc_stop_gained_prob_mu_exomes_XX_XY': float64 \n",
      "    'rgc_stop_gained_obs_exomes_XX_XY': int64 \n",
      "    'rgc_stop_gained_max_af': float64 \n",
      "    'rgc_stop_gained_count': int64 \n",
      "    'aa_pos': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af0epos00': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af0epos00': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg06': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg06': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg05': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg05': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg05': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg04': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg04': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg03': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg03': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg03': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_length_af1eneg02': int32 \n",
      "    'rgc_mis_exomes_XX_XY_vir_mu_exp_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_vir_depth_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_mean_vir_exp_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_depth_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_aa_vir_length_af1eneg02': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af0epos00': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg06': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg06': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg06': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg06': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg05': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg05': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg05': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg05': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg04': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg03': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg03': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg03': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg03': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_length_af1eneg02': int32 \n",
      "    'rgc_syn_exomes_XX_XY_vir_mu_exp_af1eneg02': float64 \n",
      "    'rgc_syn_exomes_XX_XY_vir_depth_af1eneg02': float64 \n",
      "    'rgc_syn_exomes_XX_XY_mean_vir_exp_af1eneg02': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af0epos00': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg06': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg06': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg06': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg06': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg05': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg05': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg05': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg05': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg04': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg04': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg04': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg04': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg03': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg03': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg03': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg03': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_length_af1eneg02': int32 \n",
      "    'rgc_any_exomes_XX_XY_vir_mu_exp_af1eneg02': float64 \n",
      "    'rgc_any_exomes_XX_XY_vir_depth_af1eneg02': float64 \n",
      "    'rgc_any_exomes_XX_XY_mean_vir_exp_af1eneg02': float64 \n",
      "    'rgc_mis_exomes_XX_XY_3bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_3bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_9bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_9bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_e_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_oe_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_e_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_21bp_oe_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_e_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_oe_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_e_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_45bp_oe_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_e_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_oe_af0epos00': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_e_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_oe_af1eneg06': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_e_af1eneg04': float64 \n",
      "    'rgc_mis_exomes_XX_XY_93bp_oe_af1eneg04': float64 \n",
      "    'rgc_syn_exomes_XX_XY_3bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_3bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_9bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_9bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_21bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_21bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_45bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_45bp_oe_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_93bp_e_af0epos00': float64 \n",
      "    'rgc_syn_exomes_XX_XY_93bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_3bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_3bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_9bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_9bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_21bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_21bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_45bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_45bp_oe_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_93bp_e_af0epos00': float64 \n",
      "    'rgc_any_exomes_XX_XY_93bp_oe_af0epos00': float64 \n",
      "    'transcript_id': str \n",
      "    'HGNC': str \n",
      "    'chrom': str \n",
      "    'pos': int32 \n",
      "----------------------------------------\n",
      "Key: ['locus', 'region']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_ht = hl.read_table('/storage/zoghbi/home/u235147/merged_vars/tmp/constraint_metrics_by_locus_rgc_glaf.ht')\n",
    "base_ht.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e6b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPRESSED GENOME VIEWER - DATA PREPROCESSING\n",
      "Generating axis tables for all filter modes\n",
      "============================================================\n",
      "\n",
      "Loading data from ../rgc_browser_data_merged.parquet...\n",
      " Loaded 33,387,608 total positions\n",
      "  Columns: 278\n",
      "\n",
      "Detected columns:\n",
      "  Chromosome: chrom\n",
      "  Position: pos\n",
      "  Gene: HGNC\n",
      "\n",
      "Chromosome distribution (top 10):\n",
      "  chr1: 3,414,807\n",
      "  chr2: 2,477,732\n",
      "  chr19: 2,149,560\n",
      "  chr11: 1,980,678\n",
      "  chr3: 1,921,014\n",
      "  chr17: 1,918,143\n",
      "  chr12: 1,732,842\n",
      "  chr6: 1,690,167\n",
      "  chr7: 1,565,442\n",
      "  chr5: 1,562,490\n",
      "\n",
      "============================================================\n",
      "Processing filter: any_count_gt0\n",
      "  All positions where any variant is possible\n",
      "============================================================\n",
      "\n",
      "Applying filter...\n",
      " Kept 33,387,608 positions\n",
      "Sorting by chromosome and position...\n",
      "Generating compressed coordinates...\n",
      "\n",
      "Column breakdown:\n",
      "  - Core (idx, chrom, pos, gene): 4\n",
      "  - RGC raw metrics: 142\n",
      "  - ClinVar: 4\n",
      "  - Training labels: 4\n",
      "  - dbNSFP scores: 9\n",
      "  - Constraint predictions: 3\n",
      "  - Percentiles: 105\n",
      "  - Domains: 1\n",
      "  Total columns: 273\n",
      "\n",
      "Saving axis table...\n",
      " Saved: data/any_count_gt0.parquet\n",
      "  Size: 15434.78 MB\n",
      "\n",
      "Generating gene index...\n",
      " Saved: data/gene_index_any_count_gt0.parquet\n",
      "  Genes: 19,024\n",
      "\n",
      "Per-chromosome breakdown:\n",
      "  chr1  :  3,414,807 positions\n",
      "  chr2  :  2,477,732 positions\n",
      "  chr3  :  1,921,014 positions\n",
      "  chr4  :  1,352,616 positions\n",
      "  chr5  :  1,562,490 positions\n",
      "  chr6  :  1,690,167 positions\n",
      "  chr7  :  1,565,442 positions\n",
      "  chr8  :  1,143,267 positions\n",
      "  chr9  :  1,359,315 positions\n",
      "  chr10 :  1,295,571 positions\n",
      "  chr11 :  1,980,678 positions\n",
      "  chr12 :  1,732,842 positions\n",
      "  chr13 :    614,844 positions\n",
      "  chr14 :  1,051,530 positions\n",
      "  chr15 :  1,165,698 positions\n",
      "  chr16 :  1,388,352 positions\n",
      "  chr17 :  1,918,143 positions\n",
      "  chr18 :    524,208 positions\n",
      "  chr19 :  2,149,560 positions\n",
      "  chr20 :    787,176 positions\n",
      "  chr21 :    317,811 positions\n",
      "  chr22 :    685,752 positions\n",
      "  chrX  :  1,288,593 positions\n",
      "\n",
      "============================================================\n",
      "Processing filter: mis_count_gt0\n",
      "  Positions where missense variants are possible\n",
      "============================================================\n",
      "\n",
      "Applying filter...\n",
      " Kept 27,720,524 positions\n",
      "Sorting by chromosome and position...\n",
      "Generating compressed coordinates...\n",
      "\n",
      "Column breakdown:\n",
      "  - Core (idx, chrom, pos, gene): 4\n",
      "  - RGC raw metrics: 142\n",
      "  - ClinVar: 4\n",
      "  - Training labels: 4\n",
      "  - dbNSFP scores: 9\n",
      "  - Constraint predictions: 3\n",
      "  - Percentiles: 105\n",
      "  - Domains: 1\n",
      "  Total columns: 273\n",
      "\n",
      "Saving axis table...\n",
      " Saved: data/mis_count_gt0.parquet\n",
      "  Size: 13665.79 MB\n",
      "\n",
      "Generating gene index...\n",
      " Saved: data/gene_index_mis_count_gt0.parquet\n",
      "  Genes: 19,022\n",
      "\n",
      "Per-chromosome breakdown:\n",
      "  chr1  :  2,833,702 positions\n",
      "  chr2  :  2,072,012 positions\n",
      "  chr3  :  1,601,531 positions\n",
      "  chr4  :  1,135,738 positions\n",
      "  chr5  :  1,302,820 positions\n",
      "  chr6  :  1,406,285 positions\n",
      "  chr7  :  1,299,379 positions\n",
      "  chr8  :    951,338 positions\n",
      "  chr9  :  1,127,843 positions\n",
      "  chr10 :  1,080,374 positions\n",
      "  chr11 :  1,632,655 positions\n",
      "  chr12 :  1,441,811 positions\n",
      "  chr13 :    516,092 positions\n",
      "  chr14 :    875,286 positions\n",
      "  chr15 :    973,712 positions\n",
      "  chr16 :  1,139,557 positions\n",
      "  chr17 :  1,581,124 positions\n",
      "  chr18 :    439,570 positions\n",
      "  chr19 :  1,761,940 positions\n",
      "  chr20 :    649,164 positions\n",
      "  chr21 :    263,829 positions\n",
      "  chr22 :    562,496 positions\n",
      "  chrX  :  1,072,266 positions\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "\n",
      "Generated 2 axis tables:\n",
      "\n",
      "  any_count_gt0:\n",
      "    - Positions: 33,387,608\n",
      "    - Columns: 273\n",
      "    - Genes: 19,024\n",
      "\n",
      "  mis_count_gt0:\n",
      "    - Positions: 27,720,524\n",
      "    - Columns: 273\n",
      "    - Genes: 19,022\n",
      "\n",
      "Output directory: /storage/zoghbi/home/u235147/merged_vars/gosling_mvp/data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!cd gosling_mvp && python preprocess_mis_all.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}