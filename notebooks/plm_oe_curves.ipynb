{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PLM Score vs Population Constraint Curves\n\nThis notebook explores the relationship between PLM (protein language model) predicted pathogenicity scores and observed/expected ratios from population data.\n\n**Features:**\n- Supports both **RGC** and **gnomAD** data sources\n- Supports multiple prediction models: **Core**, **Complete**, **Constraint**\n- Generates per-gene curves showing PLM score vs local O/E\n- Overlays ClinVar pathogenic variants for validation\n- **NEW: gnomAD-only dataset (gnomAD variants not observed in RGC) for model validation**\n- **NEW: Generates browser-ready binned data for all 9 model-dataset combinations**\n- **NEW: EDA comparing models and datasets**\n\n**Objectives:**\n1. Order missense variants by PLM score (from selected model)\n2. Compute sliding window local O/E along this ordering\n3. Plot the resulting curve (PLM score on x-axis, local O/E on y-axis)\n4. Overlay ClinVar pathogenic variants\n5. Characterize curve shapes and identify patterns across genes\n6. **Export binned curve data for browser visualization**\n7. **Compare models and datasets to understand model value**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Hail and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CONFIGURATION - Modify these settings to switch data sources and models\n# =============================================================================\n\n# Data source: 'rgc' or 'gnomad'\n# - rgc: RGC (Regeneron Genetics Center) variant data\n# - gnomad: gnomAD v4 variant data\nDATA_SOURCE = 'rgc'\n\n# Prediction model: 'constraint', 'core', or 'complete'\n# - constraint: Constraint-aware model (uses population constraint features)\n# - core: Core features only (sequence-based, no structural features)\n# - complete: Complete feature set (includes structural/AlphaFold features)\nPREDICTION_MODEL = 'constraint'\n\n# =============================================================================\n# Path configuration - Update these paths for your environment\n# =============================================================================\nimport yaml\nimport os\n\nmy_bucket = '/storage/zoghbi/home/u235147/merged_vars'\n\n# Try to load from config file, fall back to defaults\nconfig_path = '../config/paths.yaml'\nif os.path.exists(config_path):\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n    DATA_PATHS = config['data_sources']\n    HAIL_CONFIG = config.get('hail', {})\nelse:\n    # Default paths (update for your environment)\n    DATA_PATHS = {\n        'rgc_scaled': f'{my_bucket}/rgc_scaled.ht',\n        'gnomad_scaled': f'{my_bucket}/gnomadV4_scaled.ht',\n        'predictions': '/local/Missense_Predictor_copy/Results/Inference/Predictions/AOU_RGC_All_preds.ht',\n        'clinvar': '/storage/zoghbi/data/sharing/hail_tables/cv_38_final.ht',\n    }\n    HAIL_CONFIG = {'cpus': 40, 'tmpdir': '/local/tmp'}\n\n# Prediction model column mapping (updated to match actual column names in predictions table)\nPREDICTION_MODELS = {\n    'constraint': 'Constraint_1000_General_aou_observed_neg4_pred',\n    'core': 'Core_1000_General_aou_observed_neg4_pred',\n    'complete': 'Complete_1000_General_aou_observed_neg4_pred'\n}\n\n# =============================================================================\n# ALL COMBINATIONS - For browser data generation\n# =============================================================================\nALL_MODELS = {\n    'Constraint': 'Constraint_1000_General_aou_observed_neg4_pred',\n    'Core': 'Core_1000_General_aou_observed_neg4_pred',\n    'Complete': 'Complete_1000_General_aou_observed_neg4_pred'\n}\n\n# Datasets: RGC, gnomAD, and gnomAD-only (gnomAD variants not observed in RGC)\n# gnomAD-only helps assess model performance on variants unseen in training data\nALL_DATASETS = {\n    'RGC': f'{my_bucket}/rgc_scaled.ht',\n    'gnomAD': f'{my_bucket}/gnomadV4_scaled.ht',\n    'gnomAD-only': f'{my_bucket}/gnomadV4_scaled.ht',  # gnomAD with RGC-observed variants removed\n}\n\n# Browser output directory\nBROWSER_OUTPUT_DIR = f'{my_bucket}/data/oe_curves'\n\n# Validate configuration\nassert DATA_SOURCE in ['rgc', 'gnomad'], f\"Invalid DATA_SOURCE: {DATA_SOURCE}\"\nassert PREDICTION_MODEL in PREDICTION_MODELS, f\"Invalid PREDICTION_MODEL: {PREDICTION_MODEL}\"\n\nprint(f\"Configuration:\")\nprint(f\"  Data Source: {DATA_SOURCE}\")\nprint(f\"  Prediction Model: {PREDICTION_MODEL}\")\nprint(f\"  PLM Score Column: {PREDICTION_MODELS[PREDICTION_MODEL]}\")\nprint(f\"\\nBrowser output will include all 9 combinations (3 models x 3 datasets):\")\nfor model in ALL_MODELS:\n    for dataset in ALL_DATASETS:\n        print(f\"  - {model} + {dataset}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import hail as hl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import spearmanr\nfrom itertools import combinations\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Hail configuration from config file or defaults\ncpus = HAIL_CONFIG.get('cpus', 40)\ntmpdir = HAIL_CONFIG.get('tmpdir', '/local/tmp')\nmemory = int(3600 * cpus / 256)\n\nspark_config = {\n    'spark.driver.memory': f'{memory}g',\n    'spark.executor.memory': f'{memory}g',\n    'spark.local.dir': tmpdir,\n    'spark.ui.enabled': 'false'\n}\n\nhl.init(spark_conf=spark_config, master=f'local[{cpus}]', tmp_dir=tmpdir, local_tmpdir=tmpdir)\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Set paths based on DATA_SOURCE configuration\nif DATA_SOURCE == 'rgc':\n    main_ht_path = DATA_PATHS['rgc_scaled']\nelif DATA_SOURCE == 'gnomad':\n    main_ht_path = DATA_PATHS['gnomad_scaled']\n    \nplm_ht_path = DATA_PATHS['predictions']\nclinvar_path = DATA_PATHS['clinvar']\n\n# Output directory - include data source and model in name\noutput_dir = f'../output/curve_data/{DATA_SOURCE}_{PREDICTION_MODEL}'\n\nprint(f\"\\nPaths configured:\")\nprint(f\"  Main variant table: {main_ht_path}\")\nprint(f\"  Predictions table: {plm_ht_path}\")\nprint(f\"  ClinVar table: {clinvar_path}\")\nprint(f\"  Output directory: {output_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/gene_plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PLM predictions and check structure\n",
    "plm_ht = hl.read_table(plm_ht_path)\n",
    "print(\"PLM predictions structure:\")\n",
    "plm_ht.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main variant table\n",
    "main_ht = hl.read_table(main_ht_path)\n",
    "print(\"Main variant table structure:\")\n",
    "main_ht.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Join Tables and Filter to Missense Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Join PLM scores to main table\n# The PLM table has locus and alleles but is not keyed by default\nprint(\"PLM table fields:\", [f for f in plm_ht.row])\nprint(\"Main table key:\", main_ht.key)\n\n# Key PLM table by locus and alleles for joining\nplm_ht = plm_ht.key_by('locus', 'alleles')\nprint(\"PLM table now keyed by:\", plm_ht.key)\n\n# Get the PLM score column name from configuration\nplm_score_col = PREDICTION_MODELS[PREDICTION_MODEL]\nprint(f\"\\nUsing prediction model: {PREDICTION_MODEL}\")\nprint(f\"PLM score column: {plm_score_col}\")\n\n# Join PLM scores - using the configured prediction model\nmain_ht = main_ht.annotate(\n    plm_score = plm_ht[main_ht.locus, main_ht.alleles][plm_score_col]\n)\n\n# Filter to missense variants with valid data\n# Note: Both RGC and gnomAD tables use the same column names for XX_XY exomes\nmissense_ht = main_ht.filter(\n    (main_ht.most_deleterious_consequence_cds == 'missense_variant') &\n    hl.is_defined(main_ht.plm_score) &\n    hl.is_defined(main_ht.roulette_AR_MR_scaled_exomes_XX_XY) &\n    (main_ht.roulette_AR_MR_scaled_exomes_XX_XY > 0)\n)\n\n# Extract transcript ID from region\nmissense_ht = missense_ht.annotate(\n    transcript = missense_ht.region.split('-')[0]\n)\n\n# Checkpoint for efficiency\nmissense_ht = missense_ht.checkpoint(f'{tmpdir}/eda_step1_missense_{DATA_SOURCE}_{PREDICTION_MODEL}.ht', overwrite=True)\nprint(f\"\\nFiltered missense variants with PLM scores: {missense_ht.count():,}\")\nprint(f\"Data source: {DATA_SOURCE}, Prediction model: {PREDICTION_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract ClinVar Pathogenic Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_origin_sums():\n",
    "    \"\"\"Calculate all valid origin code sums (excluding somatic-only=2).\"\"\"\n",
    "    origin_codes = [0, 1, 2, 4, 8, 16, 32, 256]\n",
    "    all_sums = set()\n",
    "    for r in range(1, len(origin_codes) + 1):\n",
    "        for combo in combinations(origin_codes, r):\n",
    "            all_sums.add(sum(combo))\n",
    "    return sorted([s for s in all_sums if s != 2])\n",
    "\n",
    "def extract_pathogenic_variants(clinvar_path):\n",
    "    \"\"\"\n",
    "    Extract filtered pathogenic missense variants from ClinVar.\n",
    "    \n",
    "    Note: ClinVar fields are nested under 'clinvar_38' struct.\n",
    "    MC field contains molecular consequence - we filter to missense_variant.\n",
    "    \"\"\"\n",
    "    clinvar = hl.read_table(clinvar_path)\n",
    "    \n",
    "    # Check structure\n",
    "    print(\"ClinVar table structure:\")\n",
    "    clinvar.describe()\n",
    "    \n",
    "    valid_origins = get_valid_origin_sums()\n",
    "    valid_origins_set = hl.set([str(i) for i in valid_origins])\n",
    "    \n",
    "    # Access nested fields under clinvar_38\n",
    "    cv = clinvar.clinvar_38\n",
    "\n",
    "    # Filter 1: Pathogenic missense variants\n",
    "    # MC is an array of molecular consequences\n",
    "    path_ht = clinvar.filter(\n",
    "        hl.any(lambda mc: mc.contains('missense_variant'), cv.MC) &\n",
    "        hl.any(lambda sig: sig.matches(\"athogenic\"), cv.CLNSIG)  # matches Pathogenic or Likely_pathogenic\n",
    "    )\n",
    "    print(f\"After pathogenic missense filter: {path_ht.count()}\")\n",
    "\n",
    "    # Filter 2: Valid germline origin (exclude somatic-only)\n",
    "    path_ht = path_ht.filter(\n",
    "        hl.any(lambda origin: valid_origins_set.contains(origin), path_ht.clinvar_38.ORIGIN)\n",
    "    )\n",
    "    print(f\"After origin filter: {path_ht.count()}\")\n",
    "\n",
    "    # Filter 3: Exclude conflicting interpretations\n",
    "    path_ht = path_ht.filter(\n",
    "        ~hl.any(lambda sig: sig.contains(\"Conflicting\"), path_ht.clinvar_38.CLNSIG)\n",
    "    )\n",
    "    print(f\"After excluding conflicting: {path_ht.count()}\")\n",
    "\n",
    "    return path_ht\n",
    "\n",
    "# Extract ClinVar pathogenic variants\n",
    "clinvar_ht = extract_pathogenic_variants(clinvar_path)\n",
    "# Table is already keyed by locus, alleles based on the schema\n",
    "print(f\"ClinVar table key: {clinvar_ht.key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate missense variants with ClinVar status\n",
    "# ClinVar table is already keyed by locus, alleles\n",
    "missense_ht = missense_ht.annotate(\n",
    "    is_clinvar_pathogenic = hl.is_defined(clinvar_ht[missense_ht.locus, missense_ht.alleles])\n",
    ")\n",
    "\n",
    "missense_ht = missense_ht.checkpoint(f'{tmpdir}/eda_step2_with_clinvar.ht', overwrite=True)\n",
    "\n",
    "n_clinvar = missense_ht.aggregate(hl.agg.count_where(missense_ht.is_clinvar_pathogenic))\n",
    "print(f\"ClinVar P/LP missense variants in dataset: {n_clinvar:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Generate Browser Data (All 9 Model-Dataset Combinations)\n\nThis section generates pre-computed O/E curve data for all combinations of:\n- **Models**: Constraint, Core, Complete\n- **Datasets**: RGC, gnomAD, gnomAD-only\n\nThe gnomAD-only dataset contains gnomAD variants NOT observed in RGC, which helps assess model performance on variants from an independent population (unseen during training)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create browser output directory\nimport os\nos.makedirs(BROWSER_OUTPUT_DIR, exist_ok=True)\nprint(f\"Browser output directory: {BROWSER_OUTPUT_DIR}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from datetime import datetime\nimport os\n\ndef log_progress(step, message):\n    \"\"\"Helper for timestamped logging.\"\"\"\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {step}: {message}\")\n\ndef process_model_dataset_combination(model_name, model_col, dataset_name, dataset_path, plm_ht, clinvar_ht, tmpdir, gnomad_ht=None):\n    \"\"\"\n    Process a single model + dataset combination and return curve data.\n    \n    Args:\n        model_name: Name of the model (Constraint, Core, Complete)\n        model_col: Column name in predictions table\n        dataset_name: Name of the dataset (RGC, gnomAD, RGC-only)\n        dataset_path: Path to the dataset\n        plm_ht: PLM predictions table (keyed)\n        clinvar_ht: ClinVar pathogenic variants table (keyed)\n        tmpdir: Temporary directory for checkpoints\n        gnomad_ht: gnomAD table for filtering RGC-only (optional)\n    \n    Returns path to output parquet file.\n    \"\"\"\n    log_progress(\"START\", f\"Processing: {model_name} + {dataset_name}\")\n    print(f\"{'='*60}\")\n    \n    # Load dataset\n    log_progress(\"1/8\", f\"Loading dataset from {dataset_path}...\")\n    main_ht = hl.read_table(dataset_path)\n    \n    # For RGC-only: filter out variants that appear in gnomAD\n    if dataset_name == 'RGC-only' and gnomad_ht is not None:\n        log_progress(\"1b/8\", \"Filtering out gnomAD-observed variants...\")\n        n_before = main_ht.count()\n        main_ht = main_ht.filter(~hl.is_defined(gnomad_ht[main_ht.locus, main_ht.alleles]))\n        n_after = main_ht.count()\n        print(f\"    Before: {n_before:,} -> After: {n_after:,} ({100*n_after/n_before:.1f}% retained)\")\n    \n    # Join PLM scores using the model-specific column\n    log_progress(\"2/8\", f\"Joining PLM scores using column: {model_col}\")\n    main_ht = main_ht.annotate(\n        plm_score = plm_ht[main_ht.locus, main_ht.alleles][model_col]\n    )\n    \n    # Filter to missense variants with valid data\n    log_progress(\"3/8\", \"Filtering to missense variants...\")\n    missense_ht = main_ht.filter(\n        (main_ht.most_deleterious_consequence_cds == 'missense_variant') &\n        hl.is_defined(main_ht.plm_score) &\n        hl.is_defined(main_ht.roulette_AR_MR_scaled_exomes_XX_XY) &\n        (main_ht.roulette_AR_MR_scaled_exomes_XX_XY > 0)\n    )\n    \n    # Extract transcript ID and gene symbol from region\n    missense_ht = missense_ht.annotate(\n        transcript = missense_ht.region.split('-')[0],\n        gene_symbol = missense_ht.region.split('-')[1]\n    )\n    \n    # Annotate with ClinVar status\n    missense_ht = missense_ht.annotate(\n        is_clinvar_pathogenic = hl.is_defined(clinvar_ht[missense_ht.locus, missense_ht.alleles])\n    )\n    \n    # Checkpoint\n    checkpoint_path = f'{tmpdir}/browser_{model_name}_{dataset_name}_missense.ht'\n    missense_ht = missense_ht.checkpoint(checkpoint_path, overwrite=True)\n    n_variants = missense_ht.count()\n    log_progress(\"3b/8\", f\"Checkpointed {n_variants:,} missense variants\")\n    \n    # Order by transcript + PLM score\n    log_progress(\"4/8\", \"Ordering by transcript and PLM score...\")\n    missense_ht = missense_ht.order_by('transcript', 'plm_score')\n    \n    # Add index and detect transcript boundaries\n    missense_ht = missense_ht.annotate(\n        __idx=hl.scan.count(),\n        __prev_transcript=hl.scan.fold(\n            hl.missing(hl.tstr),\n            lambda acc: missense_ht.transcript,\n            lambda a, b: b\n        )\n    )\n    \n    missense_ht = missense_ht.annotate(\n        __transcript_match=hl.if_else(\n            (missense_ht.__prev_transcript == missense_ht.transcript) | hl.is_missing(missense_ht.__prev_transcript),\n            1, 0\n        )\n    )\n    \n    # Build neighbor arrays (MAX_WINDOW on each side)\n    MAX_WINDOW = 150\n    MIN_EXPECTED = 10.0\n    neighbor_struct = hl.tstruct(obs=hl.tint32, exp=hl.tfloat64)\n    empty_neighbors = hl.empty_array(neighbor_struct)\n    \n    # Forward pass (descending): collect \"following\" neighbors\n    log_progress(\"5/8\", \"Building following neighbor arrays (forward pass)...\")\n    missense_ht = missense_ht.order_by(hl.desc(missense_ht.__idx))\n    missense_ht = missense_ht.annotate(\n        __following=hl.scan.fold(\n            empty_neighbors,\n            lambda acc: hl.if_else(\n                missense_ht.__transcript_match == 0,\n                empty_neighbors,\n                hl.array([hl.struct(\n                    obs=missense_ht.binary_presence_exomes_XX_XY,\n                    exp=missense_ht.roulette_AR_MR_scaled_exomes_XX_XY\n                )]).extend(acc)[:MAX_WINDOW]\n            ),\n            lambda a, b: b\n        )\n    )\n    \n    missense_ht = missense_ht.checkpoint(f'{tmpdir}/browser_{model_name}_{dataset_name}_following.ht', overwrite=True)\n    log_progress(\"5b/8\", \"Checkpointed following arrays\")\n    \n    # Backward pass: collect \"previous\" neighbors\n    log_progress(\"6/8\", \"Building previous neighbor arrays (backward pass)...\")\n    missense_ht = missense_ht.order_by(hl.asc(missense_ht.__idx))\n    missense_ht = missense_ht.annotate(\n        __previous=hl.scan.fold(\n            empty_neighbors,\n            lambda acc: hl.if_else(\n                missense_ht.__transcript_match == 0,\n                empty_neighbors,\n                hl.array([hl.struct(\n                    obs=missense_ht.binary_presence_exomes_XX_XY,\n                    exp=missense_ht.roulette_AR_MR_scaled_exomes_XX_XY\n                )]).extend(acc)[:MAX_WINDOW]\n            ),\n            lambda a, b: b\n        )\n    )\n    \n    missense_ht = missense_ht.checkpoint(f'{tmpdir}/browser_{model_name}_{dataset_name}_previous.ht', overwrite=True)\n    log_progress(\"6b/8\", \"Checkpointed previous arrays\")\n    \n    # Compute local O/E\n    log_progress(\"7/8\", \"Computing local O/E...\")\n    missense_ht = missense_ht.annotate(\n        __all_values=missense_ht.__previous[::-1].extend(\n            hl.array([hl.struct(\n                obs=missense_ht.binary_presence_exomes_XX_XY,\n                exp=missense_ht.roulette_AR_MR_scaled_exomes_XX_XY\n            )])\n        ).extend(missense_ht.__following)\n    )\n    \n    missense_ht = missense_ht.annotate(\n        window_obs=hl.sum(missense_ht.__all_values.map(lambda x: x.obs)),\n        window_exp=hl.sum(missense_ht.__all_values.map(lambda x: x.exp)),\n        window_size=hl.len(missense_ht.__all_values)\n    )\n    \n    missense_ht = missense_ht.annotate(\n        local_oe=hl.if_else(\n            missense_ht.window_exp >= MIN_EXPECTED,\n            missense_ht.window_obs / missense_ht.window_exp,\n            hl.missing(hl.tfloat64)\n        )\n    )\n    \n    # Select output columns (key_by() drops __idx key before select)\n    output_ht = missense_ht.key_by().select(\n        'transcript',\n        'gene_symbol',\n        'plm_score',\n        'local_oe',\n        'is_clinvar_pathogenic',\n        'window_obs',\n        'window_exp',\n        'window_size'\n    )\n    \n    # Export to parquet\n    output_path = f'{BROWSER_OUTPUT_DIR}/raw_{model_name}_{dataset_name}.parquet'\n    log_progress(\"8/8\", f\"Writing to {output_path}...\")\n    \n    # Count rows before writing to ensure data exists\n    row_count = output_ht.count()\n    log_progress(\"8a/8\", f\"About to write {row_count:,} rows\")\n    \n    output_ht.to_spark().write.mode('overwrite').parquet(output_path)\n    \n    # Verify the write succeeded\n    if os.path.exists(output_path):\n        log_progress(\"DONE\", f\"SUCCESS: {output_path} exists\")\n    else:\n        log_progress(\"ERROR\", f\"FAILED: {output_path} does NOT exist!\")\n        raise RuntimeError(f\"Parquet write failed - file does not exist: {output_path}\")\n    \n    return output_path",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load PLM predictions table (keyed for joining)\nprint(\"Loading PLM predictions table...\")\nplm_ht_browser = hl.read_table(DATA_PATHS['predictions'])\nplm_ht_browser = plm_ht_browser.key_by('locus', 'alleles')\n\n# Load ClinVar table (already keyed)\nprint(\"Loading ClinVar table...\")\nclinvar_ht_browser = extract_pathogenic_variants(DATA_PATHS['clinvar'])\n\n# Load RGC table for gnomAD-only filtering (need binary_presence to identify observed variants)\n# RGC and gnomAD tables contain the same positions - the difference is which are observed\nprint(\"Loading RGC table for gnomAD-only filtering...\")\nrgc_ht_browser = hl.read_table(ALL_DATASETS['RGC'])\nrgc_ht_browser = rgc_ht_browser.select('binary_presence_exomes_XX_XY')  # Keep observed status for filtering\nprint(f\"RGC table loaded with {rgc_ht_browser.count():,} variants\")\n\nprint(\"\\nReady to process all 9 combinations (3 models x 3 datasets).\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# OPTIMIZATION: Prepare base datasets (one per dataset, reused for all models)\n# =============================================================================\n# This reduces redundant computation: instead of loading/filtering each dataset \n# 3 times (once per model), we do it once and reuse for all 3 models.\n\ndef prepare_dataset_base(dataset_name, dataset_path, clinvar_ht, rgc_ht=None):\n    \"\"\"\n    Prepare base dataset with all shared annotations (done ONCE per dataset).\n    Returns checkpointed table ready for PLM score joining.\n    \n    Args:\n        dataset_name: Name of the dataset (RGC, gnomAD, gnomAD-only)\n        dataset_path: Path to the dataset\n        clinvar_ht: ClinVar pathogenic variants table (keyed)\n        rgc_ht: RGC table for gnomAD-only filtering (contains binary_presence_exomes_XX_XY)\n    \"\"\"\n    log_progress(\"BASE\", f\"Preparing base dataset: {dataset_name}\")\n    print(f\"{'='*60}\")\n\n    # Load dataset\n    log_progress(\"BASE-1\", f\"Loading from {dataset_path}\")\n    main_ht = hl.read_table(dataset_path)\n\n    # For gnomAD-only: keep gnomAD-observed variants that are NOT observed in RGC\n    # IMPORTANT: Both tables have the same positions - the difference is binary_presence\n    if dataset_name == 'gnomAD-only' and rgc_ht is not None:\n        log_progress(\"BASE-2\", \"Filtering to gnomAD-observed but RGC-unobserved variants\")\n        n_before = main_ht.count()\n        \n        # Join with RGC to get RGC observed status\n        main_ht = main_ht.annotate(\n            rgc_observed = rgc_ht[main_ht.locus, main_ht.alleles].binary_presence_exomes_XX_XY\n        )\n        \n        # Keep variants that are:\n        # 1. Observed in gnomAD (binary_presence_exomes_XX_XY == 1)\n        # 2. NOT observed in RGC (rgc_observed == 0 or missing)\n        main_ht = main_ht.filter(\n            (main_ht.binary_presence_exomes_XX_XY == 1) &\n            ((main_ht.rgc_observed == 0) | hl.is_missing(main_ht.rgc_observed))\n        )\n        \n        n_after = main_ht.count()\n        print(f\"    Filtered: {n_before:,} -> {n_after:,} ({100*n_after/n_before:.1f}% retained)\")\n\n    # Filter to missense with valid expected values (PLM filter happens later per-model)\n    log_progress(\"BASE-3\", \"Filtering to missense variants\")\n    missense_ht = main_ht.filter(\n        (main_ht.most_deleterious_consequence_cds == 'missense_variant') &\n        hl.is_defined(main_ht.roulette_AR_MR_scaled_exomes_XX_XY) &\n        (main_ht.roulette_AR_MR_scaled_exomes_XX_XY > 0)\n    )\n\n    # Add shared annotations\n    log_progress(\"BASE-4\", \"Adding transcript, gene_symbol, ClinVar annotations\")\n    missense_ht = missense_ht.annotate(\n        transcript = missense_ht.region.split('-')[0],\n        gene_symbol = missense_ht.region.split('-')[1],\n        is_clinvar_pathogenic = hl.is_defined(clinvar_ht[missense_ht.locus, missense_ht.alleles])\n    )\n\n    # CHECKPOINT - This is reused for all 3 models\n    checkpoint_path = f'{tmpdir}/base_{dataset_name}.ht'\n    missense_ht = missense_ht.checkpoint(checkpoint_path, overwrite=True)\n    n_variants = missense_ht.count()\n    log_progress(\"BASE-DONE\", f\"Checkpointed {n_variants:,} variants to {checkpoint_path}\")\n\n    return missense_ht\n\n# Prepare base datasets (3 datasets, done ONCE each)\nprint(\"=\"*60)\nprint(\"OPTIMIZATION: Preparing base datasets (3 datasets)\")\nprint(\"Each will be reused for all 3 models, saving ~30% runtime\")\nprint(\"=\"*60)\n\nbase_datasets = {}\nfor dataset_name, dataset_path in ALL_DATASETS.items():\n    base_datasets[dataset_name] = prepare_dataset_base(\n        dataset_name=dataset_name,\n        dataset_path=dataset_path,\n        clinvar_ht=clinvar_ht_browser,\n        rgc_ht=rgc_ht_browser if dataset_name == 'gnomAD-only' else None\n    )\n    print()\n\nprint(\"=\"*60)\nprint(f\"Base datasets prepared: {list(base_datasets.keys())}\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# OPTIMIZED: Process model on pre-prepared base dataset\n# =============================================================================\n# This function is much faster because dataset loading, missense filtering,\n# and ClinVar annotation are already done in the base dataset.\n\ndef process_model_on_base(base_ht, model_name, model_col, dataset_name, plm_ht):\n    \"\"\"\n    Process a single model on a pre-prepared base dataset.\n    Much faster since base filtering/annotation is already done.\n    \"\"\"\n    log_progress(\"MODEL\", f\"Processing model: {model_name} on {dataset_name}\")\n    \n    # Join PLM scores (the only model-specific step before windowing)\n    log_progress(\"MODEL-1\", f\"Joining PLM scores using column: {model_col}\")\n    ht = base_ht.annotate(plm_score = plm_ht[base_ht.locus, base_ht.alleles][model_col])\n    \n    # Filter to variants with valid PLM scores\n    log_progress(\"MODEL-2\", \"Filtering to variants with PLM scores\")\n    ht = ht.filter(hl.is_defined(ht.plm_score))\n    \n    # Repartition to 40 for efficient downstream processing\n    log_progress(\"MODEL-2a\", \"Repartitioning to 40 partitions\")\n    ht = ht.repartition(40)\n    \n    # Checkpoint after PLM join\n    checkpoint_path = f'{tmpdir}/opt_{model_name}_{dataset_name}_plm.ht'\n    ht = ht.checkpoint(checkpoint_path, overwrite=True)\n    n_variants = ht.count()\n    log_progress(\"MODEL-2b\", f\"Checkpointed {n_variants:,} variants with PLM scores\")\n    \n    # Order by transcript + PLM score\n    log_progress(\"MODEL-3\", \"Ordering by transcript and PLM score\")\n    ht = ht.order_by('transcript', 'plm_score')\n    \n    # Add index and detect transcript boundaries\n    ht = ht.annotate(\n        __idx=hl.scan.count(),\n        __prev_transcript=hl.scan.fold(\n            hl.missing(hl.tstr),\n            lambda acc: ht.transcript,\n            lambda a, b: b\n        )\n    )\n    \n    ht = ht.annotate(\n        __transcript_match=hl.if_else(\n            (ht.__prev_transcript == ht.transcript) | hl.is_missing(ht.__prev_transcript),\n            1, 0\n        )\n    )\n    \n    # Build neighbor arrays\n    MAX_WINDOW = 150\n    MIN_EXPECTED = 10.0\n    neighbor_struct = hl.tstruct(obs=hl.tint32, exp=hl.tfloat64)\n    empty_neighbors = hl.empty_array(neighbor_struct)\n    \n    # Forward pass\n    log_progress(\"MODEL-4\", \"Building following neighbor arrays\")\n    ht = ht.order_by(hl.desc(ht.__idx))\n    ht = ht.annotate(\n        __following=hl.scan.fold(\n            empty_neighbors,\n            lambda acc: hl.if_else(\n                ht.__transcript_match == 0,\n                empty_neighbors,\n                hl.array([hl.struct(\n                    obs=ht.binary_presence_exomes_XX_XY,\n                    exp=ht.roulette_AR_MR_scaled_exomes_XX_XY\n                )]).extend(acc)[:MAX_WINDOW]\n            ),\n            lambda a, b: b\n        )\n    )\n    ht = ht.checkpoint(f'{tmpdir}/opt_{model_name}_{dataset_name}_following.ht', overwrite=True)\n    \n    # Backward pass\n    log_progress(\"MODEL-5\", \"Building previous neighbor arrays\")\n    ht = ht.order_by(hl.asc(ht.__idx))\n    ht = ht.annotate(\n        __previous=hl.scan.fold(\n            empty_neighbors,\n            lambda acc: hl.if_else(\n                ht.__transcript_match == 0,\n                empty_neighbors,\n                hl.array([hl.struct(\n                    obs=ht.binary_presence_exomes_XX_XY,\n                    exp=ht.roulette_AR_MR_scaled_exomes_XX_XY\n                )]).extend(acc)[:MAX_WINDOW]\n            ),\n            lambda a, b: b\n        )\n    )\n    ht = ht.checkpoint(f'{tmpdir}/opt_{model_name}_{dataset_name}_previous.ht', overwrite=True)\n    \n    # Compute local O/E\n    log_progress(\"MODEL-6\", \"Computing local O/E\")\n    ht = ht.annotate(\n        __all_values=ht.__previous[::-1].extend(\n            hl.array([hl.struct(\n                obs=ht.binary_presence_exomes_XX_XY,\n                exp=ht.roulette_AR_MR_scaled_exomes_XX_XY\n            )])\n        ).extend(ht.__following)\n    )\n    \n    ht = ht.annotate(\n        window_obs=hl.sum(ht.__all_values.map(lambda x: x.obs)),\n        window_exp=hl.sum(ht.__all_values.map(lambda x: x.exp)),\n        window_size=hl.len(ht.__all_values)\n    )\n    \n    ht = ht.annotate(\n        local_oe=hl.if_else(\n            ht.window_exp >= MIN_EXPECTED,\n            ht.window_obs / ht.window_exp,\n            hl.missing(hl.tfloat64)\n        )\n    )\n    \n    # Select output columns (key_by() drops __idx key before select)\n    output_ht = ht.key_by().select(\n        'transcript',\n        'gene_symbol',\n        'plm_score',\n        'local_oe',\n        'is_clinvar_pathogenic',\n        'window_obs',\n        'window_exp',\n        'window_size'\n    )\n    \n    # Export to parquet\n    output_path = f'{BROWSER_OUTPUT_DIR}/raw_{model_name}_{dataset_name}.parquet'\n    log_progress(\"MODEL-7\", f\"Writing to {output_path}\")\n    \n    row_count = output_ht.count()\n    log_progress(\"MODEL-7b\", f\"Writing {row_count:,} rows\")\n    \n    output_ht.to_spark().write.mode('overwrite').parquet(output_path)\n    \n    # Verify\n    if os.path.exists(output_path):\n        log_progress(\"MODEL-DONE\", f\"SUCCESS: {output_path} exists\")\n    else:\n        raise RuntimeError(f\"Parquet write failed: {output_path}\")\n    \n    return output_path\n\nprint(\"Optimized processing function defined: process_model_on_base()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# OPTIMIZED LOOP: Process all 9 combinations using pre-prepared base datasets\n# =============================================================================\n# Run this INSTEAD of the original loop (next cell) for ~30% faster execution.\n# Requires base_datasets to be prepared (run cell above first).\n\nimport traceback\n\nraw_parquet_paths = {}\ntotal_models = len(ALL_MODELS)\ntotal_datasets = len(base_datasets)\ntotal_combos = total_models * total_datasets\n\nprint(\"=\"*60)\nprint(f\"OPTIMIZED: Processing {total_combos} combinations\")\nprint(f\"  Using {total_datasets} pre-prepared base datasets\")\nprint(f\"  Each dataset used for {total_models} models\")\nprint(\"=\"*60)\n\ncombo_num = 0\nfor dataset_name, base_ht in base_datasets.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Dataset: {dataset_name}\")\n    print(f\"{'='*60}\")\n    \n    for model_name, model_col in ALL_MODELS.items():\n        combo_num += 1\n        key = f\"{model_name}_{dataset_name}\"\n        print(f\"\\n[{combo_num}/{total_combos}] {key}\")\n        \n        try:\n            path = process_model_on_base(\n                base_ht=base_ht,\n                model_name=model_name,\n                model_col=model_col,\n                dataset_name=dataset_name,\n                plm_ht=plm_ht_browser\n            )\n            raw_parquet_paths[key] = path\n            print(f\"[{combo_num}/{total_combos}] SUCCESS: {key}\")\n        except Exception as e:\n            print(f\"[{combo_num}/{total_combos}] FAILED: {key}\")\n            print(f\"ERROR: {e}\")\n            traceback.print_exc()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"OPTIMIZED LOOP COMPLETE\")\nprint(f\"  Successful: {len(raw_parquet_paths)}/{total_combos}\")\nprint(f\"  Failed: {total_combos - len(raw_parquet_paths)}/{total_combos}\")\nprint(f\"\\nOutput paths:\")\nfor key, path in raw_parquet_paths.items():\n    print(f\"  {key}: {path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Process all 9 model-dataset combinations (3 models x 3 datasets)\nfrom itertools import product\nimport traceback\n\nraw_parquet_paths = {}\ncombinations = list(product(ALL_MODELS.items(), ALL_DATASETS.items()))\ntotal = len(combinations)\n\nprint(f\"Processing {total} combinations (3 models x 3 datasets)\")\nprint(f\"Output directory: {BROWSER_OUTPUT_DIR}\")\nprint(\"=\"*60)\n\nfor i, ((model_name, model_col), (dataset_name, dataset_path)) in enumerate(combinations, 1):\n    key = f\"{model_name}_{dataset_name}\"\n    print(f\"\\n[{i}/{total}] Starting: {key}\")\n    \n    try:\n        path = process_model_dataset_combination(\n            model_name=model_name,\n            model_col=model_col,\n            dataset_name=dataset_name,\n            dataset_path=dataset_path,\n            plm_ht=plm_ht_browser,\n            clinvar_ht=clinvar_ht_browser,\n            tmpdir=tmpdir,\n            gnomad_ht=gnomad_ht_browser  # Pass gnomAD for RGC-only filtering\n        )\n        raw_parquet_paths[key] = path\n        print(f\"[{i}/{total}] SUCCESS: {key} -> {path}\")\n    except Exception as e:\n        print(f\"[{i}/{total}] FAILED: {key}\")\n        print(f\"ERROR: {e}\")\n        print(\"Full traceback:\")\n        traceback.print_exc()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Completed processing. Results:\")\nprint(f\"  Successful: {len(raw_parquet_paths)}/{total}\")\nprint(f\"  Failed: {total - len(raw_parquet_paths)}/{total}\")\nprint(f\"\\nOutput paths:\")\nfor key, path in raw_parquet_paths.items():\n    print(f\"  {key}: {path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Bin Curve Data for Browser\n\nAggregate variant-level curve data into PLM score bins for efficient browser loading.\nEach bin contains mean O/E, standard deviation, and variant count."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def bin_curve_data(raw_parquet_path, model_name, dataset_name, bin_width=0.02, use_percentile=False):\n    \"\"\"\n    Bin raw curve data by PLM score or within-gene percentile for browser visualization.\n    \n    Args:\n        raw_parquet_path: Path to raw parquet directory\n        model_name: Model name for output\n        dataset_name: Dataset name for output\n        bin_width: Width of PLM score bins (default 0.02 = 50 bins)\n        use_percentile: If True, bin by within-gene PLM percentile instead of raw score\n    \n    Returns:\n        Polars DataFrame with binned data\n    \"\"\"\n    import polars as pl\n    import numpy as np\n    \n    # Read raw data\n    df = pl.read_parquet(f'{raw_parquet_path}/**/*.parquet')\n    \n    # Filter to valid O/E values\n    df = df.filter(pl.col('local_oe').is_not_null())\n    \n    if use_percentile:\n        # Compute within-gene PLM percentile: rank(plm_score) / count() within each transcript\n        df = df.with_columns([\n            ((pl.col('plm_score').rank(method='average').over('transcript')) /\n             (pl.col('plm_score').count().over('transcript'))).alias('plm_gn_perc')\n        ])\n        # Create percentile bins (0.00, 0.02, 0.04, ..., 1.00)\n        df = df.with_columns([\n            ((pl.col('plm_gn_perc') / bin_width).floor() * bin_width).alias('plm_bin')\n        ])\n        x_axis_type = 'percentile'\n    else:\n        # Create PLM bins (0.00, 0.02, 0.04, ..., 1.00) - existing behavior\n        df = df.with_columns([\n            ((pl.col('plm_score') / bin_width).floor() * bin_width).alias('plm_bin')\n        ])\n        x_axis_type = 'raw'\n    \n    # Aggregate by gene + bin\n    binned = df.group_by(['transcript', 'gene_symbol', 'plm_bin']).agg([\n        pl.col('local_oe').mean().alias('mean_oe'),\n        pl.col('local_oe').std().alias('std_oe'),\n        pl.count().alias('n_variants'),\n        pl.col('is_clinvar_pathogenic').sum().alias('n_clinvar')\n    ])\n    \n    # Add model, dataset, and x_axis_type columns\n    binned = binned.with_columns([\n        pl.lit(model_name).alias('model'),\n        pl.lit(dataset_name).alias('dataset'),\n        pl.lit(x_axis_type).alias('x_axis_type')\n    ])\n    \n    return binned.sort(['transcript', 'plm_bin'])\n\nprint('Binning function defined (supports both raw PLM score and within-gene percentile)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Bin all 9 combinations for BOTH raw PLM score and within-gene percentile (total 18)\nimport polars as pl\n\nall_binned = []\n\nfor key, path in raw_parquet_paths.items():\n    model_name, dataset_name = key.split('_', 1)\n    \n    # Bin by raw PLM score (existing behavior)\n    print(f'Binning {key} by raw PLM score...')\n    try:\n        binned_raw = bin_curve_data(path, model_name, dataset_name, use_percentile=False)\n        all_binned.append(binned_raw)\n        print(f'  -> {binned_raw.height:,} bins (raw)')\n    except Exception as e:\n        print(f'  ERROR (raw): {e}')\n    \n    # Bin by within-gene PLM percentile (NEW)\n    print(f'Binning {key} by within-gene percentile...')\n    try:\n        binned_perc = bin_curve_data(path, model_name, dataset_name, use_percentile=True)\n        all_binned.append(binned_perc)\n        print(f'  -> {binned_perc.height:,} bins (percentile)')\n    except Exception as e:\n        print(f'  ERROR (percentile): {e}')\n\n# Combine all binned data (now 18 combinations: 9 raw + 9 percentile)\ncombined_binned = pl.concat(all_binned)\nprint(f'\\nTotal binned records: {combined_binned.height:,}')\nprint(f'Unique genes: {combined_binned.select(\"transcript\").unique().height:,}')\nprint(f'X-axis types: {combined_binned.select(\"x_axis_type\").unique().to_series().to_list()}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Export to final parquet file for browser\n",
    "output_file = f'{BROWSER_OUTPUT_DIR}/oe_curves_binned.parquet'\n",
    "combined_binned.write_parquet(output_file)\n",
    "print(f'Saved binned curves to: {output_file}')\n",
    "\n",
    "# Also create gene summary table\n",
    "gene_summary = combined_binned.group_by(['transcript', 'gene_symbol', 'model', 'dataset']).agg([\n",
    "    pl.col('n_variants').sum().alias('total_variants'),\n",
    "    pl.col('n_clinvar').sum().alias('total_clinvar'),\n",
    "    pl.col('mean_oe').mean().alias('avg_oe'),\n",
    "    pl.col('plm_bin').min().alias('min_plm'),\n",
    "    pl.col('plm_bin').max().alias('max_plm'),\n",
    "])\n",
    "\n",
    "summary_file = f'{BROWSER_OUTPUT_DIR}/oe_curves_gene_summary.parquet'\n",
    "gene_summary.write_parquet(summary_file)\n",
    "print(f'Saved gene summary to: {summary_file}')\n",
    "\n",
    "print(f'\\nOutput files:')\n",
    "print(f'  {output_file}')\n",
    "print(f'  {summary_file}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: EDA - Compare Models and Datasets\n\nExploratory analysis to understand:\n1. How do curves differ between models (Constraint, Core, Complete)?\n2. How do curves differ between datasets (RGC, gnomAD, RGC-only)?\n3. What is the value of the RGC-only dataset for model validation?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load the binned data for EDA\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\neda_df = pl.read_parquet(f'{BROWSER_OUTPUT_DIR}/oe_curves_binned.parquet')\nprint(f'Loaded {eda_df.height:,} binned records')\nprint(f'Models: {eda_df.select(\"model\").unique().to_series().to_list()}')\nprint(f'Datasets: {eda_df.select(\"dataset\").unique().to_series().to_list()}')\nprint(f'X-axis types: {eda_df.select(\"x_axis_type\").unique().to_series().to_list()}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Compare aggregate O/E curves across models (average over all genes)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ndatasets = ['RGC', 'gnomAD', 'gnomAD-only']\nmodels = ['Constraint', 'Core', 'Complete']\ncolors = {'Constraint': '#e94560', 'Core': '#3498db', 'Complete': '#2ecc71'}\n\nfor i, dataset in enumerate(datasets):\n    ax = axes[i]\n    \n    for model in models:\n        # Filter to this model+dataset combination\n        subset = eda_df.filter(\n            (pl.col('model') == model) & (pl.col('dataset') == dataset)\n        )\n        \n        if subset.height == 0:\n            continue\n        \n        # Average across all genes per bin\n        avg_by_bin = subset.group_by('plm_bin').agg([\n            pl.col('mean_oe').mean().alias('avg_oe'),\n            pl.col('mean_oe').std().alias('std_oe'),\n            pl.col('n_variants').sum().alias('total_n')\n        ]).sort('plm_bin')\n        \n        plm_bins = avg_by_bin['plm_bin'].to_numpy()\n        avg_oe = avg_by_bin['avg_oe'].to_numpy()\n        \n        ax.plot(plm_bins, avg_oe, '-o', markersize=3, label=model, color=colors[model])\n    \n    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n    ax.set_xlabel('PLM Score')\n    ax.set_ylabel('Mean O/E')\n    ax.set_title(f'Dataset: {dataset}')\n    ax.legend()\n    ax.set_ylim(0, 2)\n\nplt.suptitle('Average O/E Curves by Model (aggregated across all genes)', fontsize=14)\nplt.tight_layout()\nplt.savefig(f'{BROWSER_OUTPUT_DIR}/eda_models_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Compare datasets within each model\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ndataset_colors = {'RGC': '#e94560', 'gnomAD': '#3498db', 'gnomAD-only': '#f39c12'}\n\nfor i, model in enumerate(models):\n    ax = axes[i]\n    \n    for dataset in datasets:\n        subset = eda_df.filter(\n            (pl.col('model') == model) & (pl.col('dataset') == dataset)\n        )\n        \n        if subset.height == 0:\n            continue\n        \n        avg_by_bin = subset.group_by('plm_bin').agg([\n            pl.col('mean_oe').mean().alias('avg_oe')\n        ]).sort('plm_bin')\n        \n        ax.plot(avg_by_bin['plm_bin'].to_numpy(), avg_by_bin['avg_oe'].to_numpy(),\n                '-o', markersize=3, label=dataset, color=dataset_colors[dataset])\n    \n    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n    ax.set_xlabel('PLM Score')\n    ax.set_ylabel('Mean O/E')\n    ax.set_title(f'Model: {model}')\n    ax.legend()\n    ax.set_ylim(0, 2)\n\nplt.suptitle('Average O/E Curves by Dataset (aggregated across all genes)', fontsize=14)\nplt.tight_layout()\nplt.savefig(f'{BROWSER_OUTPUT_DIR}/eda_datasets_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# gnomAD vs gnomAD-only comparison (model validation)\n# This shows how the model performs on gnomAD variants not seen in RGC training data\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, model in enumerate(models):\n    ax = axes[i]\n    \n    for dataset in ['gnomAD', 'gnomAD-only']:\n        subset = eda_df.filter(\n            (pl.col('model') == model) & (pl.col('dataset') == dataset)\n        )\n        \n        avg_by_bin = subset.group_by('plm_bin').agg([\n            pl.col('mean_oe').mean().alias('avg_oe'),\n            pl.col('mean_oe').std().alias('std_oe')\n        ]).sort('plm_bin')\n        \n        plm = avg_by_bin['plm_bin'].to_numpy()\n        oe = avg_by_bin['avg_oe'].to_numpy()\n        std = avg_by_bin['std_oe'].to_numpy()\n        \n        color = '#3498db' if dataset == 'gnomAD' else '#f39c12'\n        ax.fill_between(plm, oe - std, oe + std, alpha=0.2, color=color)\n        ax.plot(plm, oe, '-', linewidth=2, label=dataset, color=color)\n    \n    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n    ax.set_xlabel('PLM Score')\n    ax.set_ylabel('Mean O/E')\n    ax.set_title(f'{model} Model')\n    ax.legend()\n    ax.set_ylim(0, 2)\n\nplt.suptitle('gnomAD vs gnomAD-only: Model Validation on Unseen Variants', fontsize=14)\nplt.tight_layout()\nplt.savefig(f'{BROWSER_OUTPUT_DIR}/eda_gnomad_only_validation.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('\\ngnomAD-only contains gnomAD variants NOT observed in RGC (i.e., unseen during model training).')\nprint('Similar curves between gnomAD and gnomAD-only indicate good model generalization.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Gene-level O/E Stratification Analysis\n# =============================================================================\n# Stratify genes by their overall missense O/E (computed from RGC data)\n# This reveals how PLM model performance varies by gene constraint level\n\n# Calculate gene-level O/E from the binned data (use raw PLM data only for consistency)\n# We use weighted mean of bin O/E values as gene-level O/E\ngene_oe = eda_df.filter(\n    (pl.col('dataset') == 'RGC') & \n    (pl.col('model') == 'Constraint') &\n    (pl.col('x_axis_type') == 'raw')  # Use raw PLM binning for gene O/E calculation\n).group_by(['transcript', 'gene_symbol']).agg([\n    (pl.col('mean_oe') * pl.col('n_variants')).sum().alias('weighted_oe_sum'),\n    pl.col('n_variants').sum().alias('total_variants'),\n]).with_columns([\n    (pl.col('weighted_oe_sum') / pl.col('total_variants')).alias('gene_oe')\n]).filter(\n    (pl.col('total_variants') >= 1) & (pl.col('gene_oe') > 0)  # At least 1 observed variant\n)\n\nprint(f\"Genes with O/E data: {gene_oe.height:,}\")\n\n# Compute gene-level O/E PERCENTILE (rank within all genes)\n# This gives us both absolute O/E and percentile O/E for stratification\ngene_oe = gene_oe.with_columns([\n    # Percentile rank of gene O/E (0-1 scale)\n    (pl.col('gene_oe').rank(method='average') / pl.col('gene_oe').count()).alias('gene_oe_percentile')\n])\n\n# Create bins for BOTH absolute O/E and percentile O/E\n# Absolute O/E bins (terciles based on actual O/E values)\ngene_oe = gene_oe.with_columns([\n    pl.col('gene_oe').qcut(3, labels=['Low O/E\\n(Constrained)', 'Medium O/E', 'High O/E\\n(Tolerant)']).alias('oe_bin_absolute')\n])\n\n# Percentile O/E bins (terciles based on percentile rank)\ngene_oe = gene_oe.with_columns([\n    pl.col('gene_oe_percentile').qcut(3, labels=['Low O/E\\n(Constrained)', 'Medium O/E', 'High O/E\\n(Tolerant)']).alias('oe_bin_percentile')\n])\n\n# Show distribution for both binning methods\nprint(\"\\nGene O/E bin distribution (Absolute):\")\nprint(gene_oe.group_by('oe_bin_absolute').agg([\n    pl.count().alias('n_genes'),\n    pl.col('gene_oe').mean().alias('mean_oe'),\n    pl.col('gene_oe').min().alias('min_oe'),\n    pl.col('gene_oe').max().alias('max_oe'),\n]))\n\nprint(\"\\nGene O/E bin distribution (Percentile):\")\nprint(gene_oe.group_by('oe_bin_percentile').agg([\n    pl.count().alias('n_genes'),\n    pl.col('gene_oe_percentile').mean().alias('mean_percentile'),\n    pl.col('gene_oe_percentile').min().alias('min_percentile'),\n    pl.col('gene_oe_percentile').max().alias('max_percentile'),\n]))\n\n# Join gene O/E bins back to main EDA data (includes both raw and percentile binned data)\neda_df_stratified = eda_df.join(\n    gene_oe.select(['transcript', 'oe_bin_absolute', 'oe_bin_percentile', 'gene_oe', 'gene_oe_percentile']),\n    on='transcript',\n    how='inner'\n)\nprint(f\"\\nStratified data rows: {eda_df_stratified.height:,}\")\nprint(f\"X-axis types in stratified data: {eda_df_stratified.select('x_axis_type').unique().to_series().to_list()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Figure 4: 3x3 Stratified Figure using Raw PLM Score (x-axis)\n# =============================================================================\n# Create 3x3 grid: 3 datasets (rows) x 3 O/E bins (columns)\n# This shows how model performance varies by gene constraint level AND dataset\n\n# Filter to raw PLM data only for this figure\neda_df_raw = eda_df_stratified.filter(pl.col('x_axis_type') == 'raw')\n\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\n\ndatasets = ['RGC', 'gnomAD', 'gnomAD-only']\noe_bins = ['Low O/E\\n(Constrained)', 'Medium O/E', 'High O/E\\n(Tolerant)']\nmodels = ['Constraint', 'Core', 'Complete']\ncolors = {'Constraint': '#e94560', 'Core': '#3498db', 'Complete': '#2ecc71'}\n\nfor row, dataset in enumerate(datasets):\n    for col, oe_bin in enumerate(oe_bins):\n        ax = axes[row, col]\n        \n        for model in models:\n            subset = eda_df_raw.filter(\n                (pl.col('model') == model) &\n                (pl.col('dataset') == dataset) &\n                (pl.col('oe_bin_absolute') == oe_bin)  # Fixed: was 'oe_bin'\n            )\n            \n            if subset.height == 0:\n                continue\n            \n            avg_by_bin = subset.group_by('plm_bin').agg([\n                pl.col('mean_oe').mean().alias('avg_oe')\n            ]).sort('plm_bin')\n            \n            ax.plot(avg_by_bin['plm_bin'].to_numpy(), avg_by_bin['avg_oe'].to_numpy(),\n                    '-o', markersize=2, label=model, color=colors[model], alpha=0.8)\n        \n        ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n        ax.set_ylim(0, 2.5)\n        \n        # Labels\n        if row == 0:\n            ax.set_title(oe_bin, fontsize=11)\n        if col == 0:\n            ax.set_ylabel(f'{dataset}\\nMean O/E', fontsize=10)\n        if row == 2:\n            ax.set_xlabel('PLM Score', fontsize=10)\n        if row == 0 and col == 2:\n            ax.legend(loc='upper right', fontsize=8)\n\nplt.suptitle('O/E Curves Stratified by Gene-Level Constraint (Raw PLM Score)\\n(3 Datasets x 3 Constraint Bins)', fontsize=14)\nplt.tight_layout()\nplt.savefig(f'{BROWSER_OUTPUT_DIR}/eda_stratified_by_gene_oe.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('\\nInterpretation:')\nprint('- Low O/E genes (constrained): PLM should show stronger O/E signal (steeper curves)')\nprint('- High O/E genes (tolerant): May show weaker signal (flatter curves)')\nprint('- gnomAD-only validates generalization to unseen variants')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Figure 5: 3x3 Stratified Figure using Within-Gene PLM Percentile (x-axis)\n# =============================================================================\n# Same structure as Figure 4, but x-axis is within-gene PLM percentile\n# This normalizes for gene-specific PLM score distributions\n\n# Filter to percentile-binned data only\neda_df_perc = eda_df_stratified.filter(pl.col('x_axis_type') == 'percentile')\n\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\n\ndatasets = ['RGC', 'gnomAD', 'gnomAD-only']\noe_bins = ['Low O/E\\n(Constrained)', 'Medium O/E', 'High O/E\\n(Tolerant)']\nmodels = ['Constraint', 'Core', 'Complete']\ncolors = {'Constraint': '#e94560', 'Core': '#3498db', 'Complete': '#2ecc71'}\n\nfor row, dataset in enumerate(datasets):\n    for col, oe_bin in enumerate(oe_bins):\n        ax = axes[row, col]\n        \n        for model in models:\n            subset = eda_df_perc.filter(\n                (pl.col('model') == model) &\n                (pl.col('dataset') == dataset) &\n                (pl.col('oe_bin_absolute') == oe_bin)\n            )\n            \n            if subset.height == 0:\n                continue\n            \n            avg_by_bin = subset.group_by('plm_bin').agg([\n                pl.col('mean_oe').mean().alias('avg_oe')\n            ]).sort('plm_bin')\n            \n            ax.plot(avg_by_bin['plm_bin'].to_numpy(), avg_by_bin['avg_oe'].to_numpy(),\n                    '-o', markersize=2, label=model, color=colors[model], alpha=0.8)\n        \n        ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n        ax.set_ylim(0, 2.5)\n        ax.set_xlim(0, 1)  # Percentile is always 0-1\n        \n        # Labels\n        if row == 0:\n            ax.set_title(oe_bin, fontsize=11)\n        if col == 0:\n            ax.set_ylabel(f'{dataset}\\nMean O/E', fontsize=10)\n        if row == 2:\n            ax.set_xlabel('Within-Gene PLM Percentile', fontsize=10)\n        if row == 0 and col == 2:\n            ax.legend(loc='upper right', fontsize=8)\n\nplt.suptitle('O/E Curves by Within-Gene PLM Percentile\\n(Stratified by Gene-Level Constraint)', fontsize=14)\nplt.tight_layout()\nplt.savefig(f'{BROWSER_OUTPUT_DIR}/eda_stratified_by_gene_oe_percentile.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('\\nInterpretation:')\nprint('- X-axis is now within-gene PLM percentile (0 = lowest PLM in gene, 1 = highest)')\nprint('- This normalizes for gene-specific PLM score distributions')\nprint('- Curves should be more comparable across genes with different PLM ranges')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summary statistics table\n",
    "summary_stats = eda_df.group_by(['model', 'dataset']).agg([\n",
    "    pl.col('n_variants').sum().alias('total_variants'),\n",
    "    pl.col('n_clinvar').sum().alias('total_clinvar'),\n",
    "    pl.col('mean_oe').mean().alias('overall_mean_oe'),\n",
    "    pl.col('transcript').n_unique().alias('n_genes')\n",
    "]).sort(['model', 'dataset'])\n",
    "\n",
    "print('\\n=== Summary Statistics by Model and Dataset ===')\n",
    "print(summary_stats)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nBrowser data generation complete! Output files:\n- `oe_curves_binned.parquet`: Binned O/E curve data for all **18** model-dataset-x_axis combinations (9 raw + 9 percentile)\n- `oe_curves_gene_summary.parquet`: Gene-level summary statistics\n\n**New Feature: Within-Gene PLM Percentiles**\n- Figure 4: 3x3 grid using raw PLM score as x-axis (9 panels)\n- Figure 5: 3x3 grid using within-gene PLM percentile as x-axis (9 panels)\n- Total: **18 panels** for comprehensive model evaluation\n\nKey findings from EDA:\n- [Model comparison insights will appear after running]\n- [Dataset comparison insights will appear after running]\n- [gnomAD-only validation insights will appear after running]\n- [Within-gene percentile comparison insights will appear after running]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Browser data generation complete! Output files:\n",
    "- `oe_curves_binned.parquet`: Binned O/E curve data for all 9 model-dataset combinations\n",
    "- `oe_curves_gene_summary.parquet`: Gene-level summary statistics\n",
    "\n",
    "Key findings from EDA:\n",
    "- [Model comparison insights will appear after running]\n",
    "- [Dataset comparison insights will appear after running]\n",
    "- [RGC-only validation insights will appear after running]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}